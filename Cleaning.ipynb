{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85be4264",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d089bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efc292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('arxiv.parquet', engine='pyarrow')\n",
    "category_df = pd.read_parquet('categories.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7862e",
   "metadata": {},
   "source": [
    "## Drop unneccessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7233821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.drop(columns=['submitter', 'authors', 'update_date', 'comments',\n",
    "                              'journal-ref', # many msising values, handled via journals added via API in feature engineering notebook\n",
    "                              'report-no', 'license'] # mainly empty values\n",
    "                              ).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c094a6",
   "metadata": {},
   "source": [
    "## Dropping records without doi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f97ac",
   "metadata": {},
   "source": [
    "Records without DOIs were removed to filter out unpublished research and erroneous or incomplete submissions that have not been formally published. This approach was taken to ensure that only credible, peer-reviewed sources remain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae66209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with None in 'versions'...\n",
      "Dropped 1467846 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"Dropping rows with None in 'versions'...\")\n",
    "initial_rows = len(df_cleaned)\n",
    "\n",
    "df_cleaned.dropna(subset=['doi'], inplace=True)\n",
    "print(f\"Dropped {initial_rows - len(df_cleaned)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76842a49",
   "metadata": {},
   "source": [
    "## Dropping duplicate records (with the same doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472668c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping duplicate records...\n",
      "Dropped 2244 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"Dropping duplicate records...\")\n",
    "initial_rows = len(df_cleaned)\n",
    "\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset='doi', keep='first') \n",
    "print(f\"Dropped {initial_rows - len(df_cleaned)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db91b72",
   "metadata": {},
   "source": [
    "## Filter to Papers created from 2015 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f76edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pandarallel...\n",
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n",
      "Pandarallel initialized.\n",
      "Applying function using parallel_apply...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b978efcf3f4bf98c3dfe9c6829fee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=205024), Label(value='0 / 205024')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel_apply finished.\n",
      "Converting to consistent datetime format...\n",
      "Applying final filter...\n"
     ]
    }
   ],
   "source": [
    "def get_first_version_date(version_data):\n",
    "    \"\"\"\n",
    "    Extracts the 'created' date from the FIRST dictionary in a list OR NumPy array,\n",
    "    parses it using a specific format, and returns a timezone-aware datetime object (UTC).\n",
    "    Returns NaT if input is invalid, empty, first item isn't dict,\n",
    "    'created' key is missing, or date is unparseable with the given format.\n",
    "    \"\"\"\n",
    "    # Imports needed within the function for parallel workers\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    is_list = isinstance(version_data, list)\n",
    "    is_ndarray = isinstance(version_data, np.ndarray)\n",
    "\n",
    "    # Check if it's a list/array and if it's NOT empty\n",
    "    if not (is_list or is_ndarray) or len(version_data) == 0:\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        # --- Access the FIRST element ---\n",
    "        first_version = version_data[0]\n",
    "        # --------------------------------\n",
    "\n",
    "        if not isinstance(first_version, dict):\n",
    "            return pd.NaT # First item isn't a dictionary\n",
    "\n",
    "        date_str = first_version.get('created')\n",
    "        if date_str and isinstance(date_str, str):\n",
    "            date_format = \"%a, %d %b %Y %H:%M:%S %Z\"\n",
    "            # Parse using the specific format [[6]]\n",
    "            dt = pd.to_datetime(date_str, format=date_format, errors='coerce', utc=True)\n",
    "            return dt\n",
    "        else:\n",
    "            # 'created' key missing or value is not a string\n",
    "            return pd.NaT\n",
    "    except (IndexError, TypeError):\n",
    "         # IndexError could happen if version_data[0] fails unexpectedly (though len check should prevent)\n",
    "         # TypeError could happen if first_version is not subscriptable or .get fails\n",
    "        return pd.NaT\n",
    "\n",
    "# --- Pandarallel Initialization ---\n",
    "print(\"Initializing Pandarallel...\")\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "print(\"Pandarallel initialized.\")\n",
    "\n",
    "# --- Apply using Pandarallel ---\n",
    "print(\"Applying function using parallel_apply...\")\n",
    "\n",
    "# Apply the NEW function using parallel_apply\n",
    "df_cleaned['first_date_parsed'] = df_cleaned['versions'].parallel_apply(get_first_version_date)\n",
    "\n",
    "print(\"parallel_apply finished.\")\n",
    "\n",
    "# --- Ensure Consistent Datetime Type and Timezone ---\n",
    "print(\"Converting to consistent datetime format...\")\n",
    "df_cleaned['first_date'] = pd.to_datetime(df_cleaned['first_date_parsed'], errors='coerce', utc=True)\n",
    "\n",
    "# --- Final Filtering ---\n",
    "print(\"Applying final filter...\")\n",
    "cutoff_date = pd.Timestamp('2015-01-01', tz='UTC')\n",
    "# Use the 'first_date' column for comparison\n",
    "mask = df_cleaned['first_date'] >= cutoff_date\n",
    "df_cleaned = df_cleaned[mask].copy()\n",
    "\n",
    "# Drop the temporary column\n",
    "df_cleaned = df_cleaned.drop(columns=['first_date_parsed'])\n",
    "\n",
    "# Drop the original 'versions' column\n",
    "df_cleaned = df_cleaned.drop(columns=['versions'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a3272",
   "metadata": {},
   "source": [
    "## Handling Category Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7968f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonyms dictionary\n",
    "replacements = {\n",
    "    \"math.NA\": \"cs.NA\",\n",
    "    \"eess.SY\": \"cs.SY\",\n",
    "    \"math.IT\": \"cs.IT\",\n",
    "    \"math.MP\": \"math-ph\",\n",
    "    \"q-fin.EC\": \"econ.GN\",\n",
    "    \"math.ST\": \"stat.TH\"\n",
    "}\n",
    "\n",
    "# --- Count replacements ---\n",
    "\n",
    "categories_to_count = df_cleaned['categories'].fillna('').astype(str)\n",
    "\n",
    "total_replacements_count = 0\n",
    "\n",
    "for old_cat, new_cat in replacements.items():\n",
    "    # Escape the old category name for regex\n",
    "    escaped_old_cat = re.escape(old_cat)\n",
    "    # Create the regex pattern with word boundaries\n",
    "    pattern = fr'\\b{escaped_old_cat}\\b'\n",
    "\n",
    "    # Count occurrences of the pattern in the Series\n",
    "    count_for_this_cat = categories_to_count.str.count(pattern).sum()\n",
    "\n",
    "    # Add the count for this specific category to the total\n",
    "    total_replacements_count += count_for_this_cat\n",
    "\n",
    "for old_cat, new_cat in replacements.items():\n",
    "    # Escape the old category name to handle special regex characters (like '.')\n",
    "    escaped_old_cat = re.escape(old_cat)\n",
    "    # Use regex with word boundaries (\\b) to ensure only the exact category is replaced\n",
    "    # fr'\\b{escaped_old_cat}\\b' creates a regex like r'\\bmath\\.NA\\b'\n",
    "    df_cleaned['categories'] = df_cleaned['categories'].str.replace(\n",
    "        fr'\\b{escaped_old_cat}\\b',\n",
    "        new_cat,\n",
    "        regex=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d76310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_in_string(category_string):\n",
    "    # Handle potential empty strings or NaN remnants if fillna wasn't perfect\n",
    "    if not isinstance(category_string, str) or not category_string.strip():\n",
    "        return \"\" # Return empty string for empty/invalid input\n",
    "\n",
    "    # Split the string into a list of categories\n",
    "    categories = category_string.split()\n",
    "\n",
    "    # Use dict.fromkeys to get unique categories while preserving order\n",
    "    # Filter out any potential empty strings that might result from multiple spaces\n",
    "    unique_categories = list(dict.fromkeys(cat for cat in categories if cat))\n",
    "\n",
    "    # Join the unique categories back into a space-separated string\n",
    "    return ' '.join(unique_categories)\n",
    "\n",
    "# Apply the function to the 'categories' column\n",
    "df_cleaned['categories'] = df_cleaned['categories'].apply(remove_duplicates_in_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b27228",
   "metadata": {},
   "source": [
    "## Add Domain, Area and SubArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516d5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lookup_df = category_df.set_index('Code')\n",
    "\n",
    "# Convert relevant columns to a dictionary {Code: {col: value, ...}}\n",
    "category_dict = category_lookup_df[['Domain', 'Area', 'SubArea']].to_dict('index')\n",
    "\n",
    "# Define the function to get and aggregate info\n",
    "def get_aggregated_info(category_string):\n",
    "    if not isinstance(category_string, str) or not category_string.strip():\n",
    "        return pd.Series({'Joined_Domain': '', 'Joined_Area': '', 'Joined_SubArea': ''})\n",
    "\n",
    "    codes = category_string.split()\n",
    "    domains = []\n",
    "    areas = []\n",
    "    subareas = []\n",
    "\n",
    "    for code in codes:\n",
    "        if code in category_dict:\n",
    "            info = category_dict[code]\n",
    "            domains.append(info['Domain'])\n",
    "            areas.append(info['Area'])\n",
    "            subareas.append(info['SubArea'])\n",
    "        # else: handle codes not found in category_df if necessary (e.g., log warning)\n",
    "\n",
    "    # Get unique values while preserving order and join with '; '\n",
    "    joined_domain = '; '.join(list(dict.fromkeys(domains)))\n",
    "    joined_area = '; '.join(list(dict.fromkeys(areas)))\n",
    "    joined_subarea = '; '.join(list(dict.fromkeys(subareas)))\n",
    "\n",
    "    return pd.Series({\n",
    "        'Joined_Domain': joined_domain,\n",
    "        'Joined_Area': joined_area,\n",
    "        'Joined_SubArea': joined_subarea\n",
    "    })\n",
    "\n",
    "new_columns = df_cleaned['categories'].apply(get_aggregated_info)\n",
    "df_cleaned = df_cleaned.join(new_columns) \n",
    "\n",
    "df_cleaned = df_cleaned.rename(columns={\"Joined_Domain\": \"Domain\", \"Joined_Area\": \"Area\", \"Joined_SubArea\": \"SubArea\"})\n",
    "\n",
    "# Drop the original 'categories' column\n",
    "df_cleaned = df_cleaned.drop(columns=['categories'])\n",
    "\n",
    "# Drop Area (Only differs from SubArea for Physics, no broader Areas needed for Physics)\n",
    "df_cleaned = df_cleaned.drop(columns=['Area'])\n",
    "df_cleaned = df_cleaned.rename(columns={\"SubArea\": \"Area\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7446c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_parquet(\"arxiv_cleaned.parquet\", engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b95b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
