{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170aaf13",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd6973",
   "metadata": {},
   "source": [
    "## Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import scipy.sparse\n",
    "from typing import List, Dict, Tuple, Any, Set, Optional\n",
    "import inflect\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import hdbscan\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File Paths ---\n",
    "DATA_FILE = \"checkpoint_with_keywords.parquet\"\n",
    "VARIABLES_FILE = \"checkpoint_variables.pkl\"\n",
    "\n",
    "# --- Pipeline Parameters ---\n",
    "# PCA Configuration\n",
    "PCA_N_COMPONENTS = 150\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# UMAP Configuration (for clustering)\n",
    "UMAP_CLUSTERING_PARAMS = {\n",
    "    'n_neighbors': 15,\n",
    "    'n_components': 15,\n",
    "    'min_dist': 0.0,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "# UMAP Configuration (for 2D visualization)\n",
    "UMAP_VISUALIZATION_PARAMS = {\n",
    "    'n_neighbors': 30,\n",
    "    'n_components': 2,\n",
    "    'min_dist': 0.1,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "# HDBSCAN Configuration\n",
    "HDBSCAN_PARAMS = {\n",
    "    'min_cluster_size': 300,\n",
    "    'min_samples': 50,\n",
    "    'metric': 'euclidean',\n",
    "    'cluster_selection_method': 'eom',\n",
    "    'gen_min_span_tree': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading data from '{DATA_FILE}'...\")\n",
    "df = pd.read_parquet(DATA_FILE, engine=\"pyarrow\")\n",
    "print(f\"DataFrame loaded. Shape: {df.shape}\")\n",
    "\n",
    "# Load the helper variables (column lists)\n",
    "print(f\"Loading feature lists from '{VARIABLES_FILE}'...\")\n",
    "with open(VARIABLES_FILE, \"rb\") as f:\n",
    "    loaded_variables = pickle.load(f)\n",
    "metadata_features = loaded_variables[\"metadata_features\"]\n",
    "unique_domains = np.array(loaded_variables[\"unique_domains\"])\n",
    "keyword_cols_created = loaded_variables[\"keyword_cols_created\"]\n",
    "print(\"Helper variables loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the full feature set\n",
    "print(\"Assembling the final feature set for clustering...\")\n",
    "\n",
    "# Ensure all feature columns exist in the DataFrame\n",
    "domain_features = [col for col in unique_domains if col in df.columns]\n",
    "meta_features = [col for col in metadata_features if col in df.columns]\n",
    "keyword_features = [col for col in keyword_cols_created if col in df.columns]\n",
    "\n",
    "# Combine all feature lists\n",
    "all_features = meta_features + domain_features + keyword_features\n",
    "all_features = sorted(list(set(all_features)))  # Get unique sorted list\n",
    "print(f\"Total number of features: {len(all_features)}\")\n",
    "print(f\" - Metadata & Area features: {len(meta_features)}\")\n",
    "print(f\" - Domain features: {len(domain_features)}\")\n",
    "print(f\" - Keyword features: {len(keyword_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ed75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrix X\n",
    "X = df[all_features]\n",
    "\n",
    "# Handle Missing Values\n",
    "# Impute with median for numeric columns. This is a robust strategy.\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"NaNs found. Imputing with column medians...\")\n",
    "    X = X.fillna(X.median())\n",
    "else:\n",
    "    print(\"No NaNs found in the feature matrix.\")\n",
    "\n",
    "# Convert to NumPy array for scikit-learn\n",
    "print(\"Converting feature matrix to NumPy array...\")\n",
    "X_np = X.to_numpy(dtype=np.float32)\n",
    "print(f\"Final feature matrix 'X_np' created with shape: {X_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e924e6e",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Clustering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d085a",
   "metadata": {},
   "source": [
    "\n",
    "1. Scale (StandardScaler): Standardizes all features to have a mean of 0 and a standard deviation of 1. This is crucial because PCA and UMAP are sensitive to the scale of the data, and this step ensures that all features (e.g., number of authors, binary keywords) contribute equally.\n",
    "\n",
    "2. PCA (Principal Component Analysis): Performs an initial, fast dimensionality reduction. It takes the thousands of scaled features and reduces them to a smaller, more manageable set by capturing the main linear patterns, which helps reduce noise and speeds up the next step significantly.\n",
    "\n",
    "3. UMAP (Uniform Manifold Approximation and Projection): Takes the PCA-reduced data and performs a second, more sophisticated dimensionality reduction. UMAP is excellent at finding the complex, non-linear structure in the data, creating a low-dimensional representation (e.g., 15 components) that best preserves the true relationships between data points.\n",
    "\n",
    "4. HDBSCAN (Hierarchical Density-Based Clustering): Performs the final clustering on the low-dimensional, structure-rich UMAP output. It identifies clusters based on density, allowing it to find groups of varying shapes and sizes and automatically identify noise points, without requiring to specify the number of clusters in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf99a43",
   "metadata": {},
   "source": [
    "### Step 1: Scaling\n",
    "\n",
    "Advantages: \n",
    "Denoising: Removes a massive amount of noise, giving UMAP a cleaner signal to work with.\n",
    "Speed: Running UMAP on 150 components is dramatically faster than running it on thousands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 1: Scaling data with StandardScaler ---\")\n",
    "start_time = time.time()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_np)\n",
    "end_time = time.time()\n",
    "print(f\"Scaling completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620538b",
   "metadata": {},
   "source": [
    "### Step 2: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ede85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Step 2: Applying PCA to reduce to {PCA_N_COMPONENTS} components ---\")\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=PCA_N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"PCA completed in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Shape after PCA: {X_pca.shape}\")\n",
    "print(f\"Total explained variance by {pca.n_components_} components: {np.sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c5759",
   "metadata": {},
   "source": [
    "### Step 3: UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Step 3: Applying UMAP to reduce to {UMAP_CLUSTERING_PARAMS['n_components']} dimensions for clustering ---\")\n",
    "start_time = time.time()\n",
    "umap_reducer_clustering = umap.UMAP(**UMAP_CLUSTERING_PARAMS)\n",
    "X_umap_clustering = umap_reducer_clustering.fit_transform(X_pca)\n",
    "end_time = time.time()\n",
    "print(f\"UMAP for clustering completed in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Shape after UMAP: {X_umap_clustering.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b428613",
   "metadata": {},
   "source": [
    "### Step 4: HDBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4: Applying HDBSCAN to find clusters ---\")\n",
    "start_time = time.time()\n",
    "clusterer = hdbscan.HDBSCAN(**HDBSCAN_PARAMS)\n",
    "cluster_labels = clusterer.fit_predict(X_umap_clustering)\n",
    "end_time = time.time()\n",
    "print(f\"HDBSCAN completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a36cea",
   "metadata": {},
   "source": [
    "### Add cluster labels to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_label'] = cluster_labels\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = np.sum(cluster_labels == -1)\n",
    "print(\"\\n--- Clustering Results ---\")\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise} ({n_noise / len(df) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a769727a",
   "metadata": {},
   "source": [
    "### Saving Results for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581dce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define file paths for saving ---\n",
    "RESULTS_DF_FILE = \"results_df_with_clusters.parquet\"\n",
    "RESULTS_VARS_FILE = \"results_variables.pkl\"\n",
    "\n",
    "try:\n",
    "    df.to_parquet(RESULTS_DF_FILE, engine=\"pyarrow\")\n",
    "    print(f\"Successfully saved DataFrame to '{RESULTS_DF_FILE}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving DataFrame: {e}\")\n",
    "\n",
    "results_variables = {\n",
    "    'n_clusters': n_clusters,\n",
    "    'n_noise': n_noise,\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'RANDOM_STATE': RANDOM_STATE\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open(RESULTS_VARS_FILE, \"wb\") as f:\n",
    "        pickle.dump(results_variables, f)\n",
    "    print(f\"Successfully saved variables to '{RESULTS_VARS_FILE}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving variables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fe6a1",
   "metadata": {},
   "source": [
    "### Reloading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bc9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import scipy.sparse\n",
    "from typing import List, Dict, Tuple, Any, Set, Optional\n",
    "import inflect\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "import hdbscan\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4086c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DF_FILE = \"results_df_with_clusters.parquet\"\n",
    "RESULTS_VARS_FILE = \"results_variables.pkl\"\n",
    "\n",
    "\n",
    "print(\"--- Loading pre-computed results from disk ---\")\n",
    "\n",
    "\n",
    "# 1. Load the DataFrame\n",
    "try:\n",
    "    df = pd.read_parquet(RESULTS_DF_FILE, engine=\"pyarrow\")\n",
    "    print(f\"Successfully loaded DataFrame from '{RESULTS_DF_FILE}'. Shape: {df.shape}\")\n",
    "    # Display a sample to verify that cluster and UMAP columns are present\n",
    "    display(df[['id', 'title', 'cluster_label', 'umap_x', 'umap_y']].head(3))\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: DataFrame file not found at '{RESULTS_DF_FILE}'. Please run the full pipeline first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DataFrame: {e}\")\n",
    "\n",
    "\n",
    "# 2. Load the variables\n",
    "try:\n",
    "    with open(RESULTS_VARS_FILE, \"rb\") as f:\n",
    "        loaded_results = pickle.load(f)\n",
    "\n",
    "\n",
    "    # Re-assign to global variables so subsequent cells work correctly\n",
    "    n_clusters = loaded_results['n_clusters']\n",
    "    n_noise = loaded_results['n_noise']\n",
    "    cluster_labels = loaded_results['cluster_labels']\n",
    "\n",
    "\n",
    "    print(f\"Successfully loaded variables from '{RESULTS_VARS_FILE}'\")\n",
    "    print(f\"  - n_clusters: {n_clusters}\")\n",
    "    print(f\"  - n_noise: {n_noise}\")\n",
    "    print(f\"  - cluster_labels array shape: {cluster_labels.shape}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Variables file not found at '{RESULTS_VARS_FILE}'. Please run the full pipeline first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading variables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80a99f",
   "metadata": {},
   "source": [
    "## Visualization of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba49e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create 2D UMAP embedding for visualization ---\n",
    "print(\"\\n--- Creating 2D UMAP embedding for visualization ---\")\n",
    "start_time = time.time()\n",
    "umap_reducer_viz = umap.UMAP(**UMAP_VISUALIZATION_PARAMS)\n",
    "X_umap_viz = umap_reducer_viz.fit_transform(X_pca)\n",
    "end_time = time.time()\n",
    "print(f\"2D UMAP for visualization completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Add 2D coordinates to DataFrame for plotting\n",
    "df['umap_x'] = X_umap_viz[:, 0]\n",
    "df['umap_y'] = X_umap_viz[:, 1]\n",
    "\n",
    "# %%\n",
    "print(\"--- Generating interactive cluster plot ---\")\n",
    "\n",
    "# Prepare data for Plotly\n",
    "plot_df = df.copy()\n",
    "plot_df['cluster_label_str'] = plot_df['cluster_label'].astype(str)\n",
    "plot_df.loc[plot_df['cluster_label'] == -1, 'cluster_label_str'] = 'Noise'\n",
    "\n",
    "# Get cluster sizes for the legend\n",
    "cluster_sizes = plot_df['cluster_label_str'].value_counts().reset_index()\n",
    "cluster_sizes.columns = ['cluster_label_str', 'count']\n",
    "plot_df = pd.merge(plot_df, cluster_sizes, on='cluster_label_str')\n",
    "plot_df['legend_entry'] = plot_df['cluster_label_str'] + ' (' + plot_df['count'].astype(str) + ')'\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edaa5f8",
   "metadata": {},
   "source": [
    "### Saving UMAP Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VIZ_DF_FILE = \"df_with_2D_viz_coords.parquet\"\n",
    "\n",
    "print(f\"--- Saving DataFrame with visualization coordinates to '{VIZ_DF_FILE}' ---\")\n",
    "\n",
    "try:\n",
    "    # The 'df' DataFrame now contains everything: original data, cluster labels, and 2D coordinates.\n",
    "    df.to_parquet(VIZ_DF_FILE, engine=\"pyarrow\")\n",
    "    print(\"Successfully saved the DataFrame.\")\n",
    "    print(\"You can now use the 'Load Visualization Data' block in future sessions to skip the UMAP steps.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the DataFrame: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"df_columns.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for col in df.columns:\n",
    "        f.write(f\"{col}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc5c3e",
   "metadata": {},
   "source": [
    "### Loading UMAP Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96deb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIZ_DF_FILE = \"df_with_2D_viz_coords.parquet\"\n",
    "\n",
    "columns_to_load = [\n",
    "    'id', \n",
    "    'title', \n",
    "    'cluster_label', \n",
    "    'umap_x', \n",
    "    'umap_y'\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"--- Loading pre-computed visualization data from '{VIZ_DF_FILE}' ---\")\n",
    "print(f\"Loading only the following columns: {columns_to_load}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Load ONLY the specified columns from the Parquet file\n",
    "    df = pd.read_parquet(VIZ_DF_FILE, columns=columns_to_load, engine=\"pyarrow\")\n",
    "    print(f\"Successfully loaded DataFrame. Shape: {df.shape}\")\n",
    "    \n",
    "    # --- Recreate the 'plot_df' needed for the interactive plot ---\n",
    "    # This step is very fast and ensures the plotting variables are ready.\n",
    "    print(\"\\n--- Preparing data for Plotly ---\")\n",
    "    plot_df = df.copy()\n",
    "    plot_df['cluster_label_str'] = plot_df['cluster_label'].astype(str)\n",
    "    plot_df.loc[plot_df['cluster_label'] == -1, 'cluster_label_str'] = 'Noise'\n",
    "\n",
    "\n",
    "    # Get cluster sizes for the legend\n",
    "    cluster_sizes = plot_df['cluster_label_str'].value_counts().reset_index()\n",
    "    cluster_sizes.columns = ['cluster_label_str', 'count']\n",
    "    plot_df = pd.merge(plot_df, cluster_sizes, on='cluster_label_str')\n",
    "    plot_df['legend_entry'] = plot_df['cluster_label_str'] + ' (' + plot_df['count'].astype(str) + ')'\n",
    "    \n",
    "    print(\"Plotting data is ready.\")\n",
    "    \n",
    "    # --- Initialize the figure object ---\n",
    "    # You are now ready to add traces and show the figure in the subsequent cells.\n",
    "    fig = go.Figure()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at '{VIZ_DF_FILE}'.\")\n",
    "    print(\"Please ensure you have run the full pipeline and saved the results at least once.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or preparing the data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f89107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clustered points first\n",
    "clustered_data = plot_df[plot_df['cluster_label'] != -1].sort_values('cluster_label')\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=clustered_data['umap_x'],\n",
    "    y=clustered_data['umap_y'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=clustered_data['cluster_label'],\n",
    "        colorscale='Viridis',  # A nice colorscale for clusters\n",
    "        showscale=False,\n",
    "        size=3,\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    customdata=clustered_data[['id', 'title', 'cluster_label']],\n",
    "    hovertemplate='<b>Title:</b> %{customdata[1]}<br>' +\n",
    "                  '<b>ID:</b> %{customdata[0]}<br>' +\n",
    "                  '<b>Cluster:</b> %{customdata[2]}<br>' +\n",
    "                  'UMAP-X: %{x:.3f}<br>UMAP-Y: %{y:.3f}<extra></extra>',\n",
    "    name='Clusters'\n",
    "))\n",
    "\n",
    "# Plot noise points on top, in grey\n",
    "noise_data = plot_df[plot_df['cluster_label'] == -1]\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=noise_data['umap_x'],\n",
    "    y=noise_data['umap_y'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color='lightgrey',\n",
    "        size=2,\n",
    "        opacity=0.4\n",
    "    ),\n",
    "    customdata=noise_data[['id', 'title']],\n",
    "    hovertemplate='<b>Title:</b> %{customdata[1]}<br>' +\n",
    "                  '<b>ID:</b> %{customdata[0]}<br>' +\n",
    "                  '<b>Cluster:</b> Noise<br>' +\n",
    "                  'UMAP-X: %{x:.3f}<br>UMAP-Y: %{y:.3f}<extra></extra>',\n",
    "    name=f\"Noise ({len(noise_data)})\"\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f'Global Clustering Results: {n_clusters} Clusters Found',\n",
    "    xaxis_title='UMAP Dimension 1',\n",
    "    yaxis_title='UMAP Dimension 2',\n",
    "    height=800,\n",
    "    legend_title_text='Cluster Labels',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "cluster_counts = df[df['cluster_label'] != -1]['cluster_label'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')\n",
    "plt.title('Number of Records per Cluster (excluding noise)', fontsize=16)\n",
    "plt.xlabel('Cluster ID', fontsize=12)\n",
    "plt.ylabel('Number of Records', fontsize=12)\n",
    "\n",
    "if len(cluster_counts) > 50:\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    # Show every 5th tick label to avoid clutter\n",
    "    for index, label in enumerate(plt.gca().get_xticklabels()):\n",
    "        if index % 5 != 0:\n",
    "            label.set_visible(False)\n",
    "else:\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c304bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Get the counts of records for each cluster, excluding noise points (-1)\n",
    "cluster_sizes = df[df['cluster_label'] != -1]['cluster_label'].value_counts()\n",
    "\n",
    "# --- Identify and filter out the largest cluster ---\n",
    "if not cluster_sizes.empty:\n",
    "    # Find the ID and size of the largest cluster\n",
    "    largest_cluster_id = cluster_sizes.idxmax()\n",
    "    largest_cluster_size = cluster_sizes.max()\n",
    "\n",
    "    # Create a new Series for plotting that excludes the largest cluster\n",
    "    sizes_for_plotting = cluster_sizes  # Not dropping largest cluster as per your edit\n",
    "    \n",
    "    print(f\"Identified and excluded the largest cluster (ID: {largest_cluster_id}) with {largest_cluster_size} records to improve visualization.\")\n",
    "else:\n",
    "    print(\"No clusters to plot.\")\n",
    "    sizes_for_plotting = pd.Series()\n",
    "\n",
    "# --- Create the Histogram for the remaining clusters ---\n",
    "if not sizes_for_plotting.empty:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Use stat='percent' to get percentage on the y-axis\n",
    "    sns.histplot(data=sizes_for_plotting, binwidth=500, kde=False, color='royalblue', stat='percent')\n",
    "\n",
    "    # Update the title to reflect the data\n",
    "    plt.title(f'Distribution of Cluster Sizes', fontsize=16)\n",
    "    plt.xlabel('Cluster Size (Number of Records)', fontsize=12)\n",
    "    plt.ylabel('Percentage of Clusters (%)', fontsize=12)  # Updated y-label\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f763cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Get the counts of records for each cluster, excluding noise points (-1)\n",
    "cluster_sizes = df[df['cluster_label'] != -1]['cluster_label'].value_counts()\n",
    "\n",
    "# --- Identify and filter out the largest cluster ---\n",
    "if not cluster_sizes.empty:\n",
    "    # Find the ID and size of the largest cluster\n",
    "    largest_cluster_id = cluster_sizes.idxmax()\n",
    "    largest_cluster_size = cluster_sizes.max()\n",
    "\n",
    "    # Create a new Series for plotting that excludes the largest cluster\n",
    "    sizes_for_plotting = cluster_sizes  # Not dropping largest cluster as per your edit\n",
    "    \n",
    "    print(f\"Identified and excluded the largest cluster (ID: {largest_cluster_id}) with {largest_cluster_size} records to improve visualization.\")\n",
    "    \n",
    "    # Calculate percentage of clusters with between 300 and 500 records\n",
    "    target_range_clusters = sizes_for_plotting[(sizes_for_plotting >= 300) & (sizes_for_plotting <= 800)]\n",
    "    percentage_in_range = (len(target_range_clusters) / len(sizes_for_plotting)) * 100\n",
    "    print(f\"Percentage of clusters with between 300 and 500 records: {percentage_in_range:.2f}%\")\n",
    "    print(f\"Count of clusters with between 300 and 500 records: {len(target_range_clusters)} out of {len(sizes_for_plotting)} total clusters\")\n",
    "else:\n",
    "    print(\"No clusters to plot.\")\n",
    "    sizes_for_plotting = pd.Series()\n",
    "\n",
    "# --- Create the Histogram for the remaining clusters ---\n",
    "if not sizes_for_plotting.empty:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Use stat='percent' to get percentage on the y-axis\n",
    "    sns.histplot(data=sizes_for_plotting, binwidth=500, kde=False, color='royalblue', stat='percent')\n",
    "\n",
    "    # Update the title to reflect the data\n",
    "    plt.title(f'Distribution of Cluster Sizes', fontsize=16)\n",
    "    plt.xlabel('Cluster Size (Number of Records)', fontsize=12)\n",
    "    plt.ylabel('Percentage of Clusters (%)', fontsize=12)  # Updated y-label\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    \n",
    "    plt.axvspan(300, 800, alpha=0.2, color='red')\n",
    "    plt.axvline(x=300, linestyle='--', color='red', alpha=0.7)\n",
    "    plt.axvline(x=800, linestyle='--', color='red', alpha=0.7)\n",
    "    \n",
    "    # Add annotation about percentage in this range\n",
    "    plt.annotate(f'{percentage_in_range:.2f}% of clusters\\nhave between 300-800 records', \n",
    "                xy=(400, plt.ylim()[1]*0.9),\n",
    "                xytext=(400, plt.ylim()[1]*0.9),\n",
    "                ha='center',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"red\", alpha=0.7))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe57a3",
   "metadata": {},
   "source": [
    "## Cluster Analysis and Characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb69154",
   "metadata": {},
   "source": [
    "1.  **High-Level Summary**: Summary table that provides a quick overview of every cluster, including its size, top keywords, most representative domains, and median publication date. This helps in getting a first impression of the landscape.\n",
    "2.  **Identifying Emerging Topics**: Temporal trends of each cluster to identify which topics are gaining popularity over time (\"emerging topics\").\n",
    "3.  **Deep Dive Analysis**: Specific clusters of interest: More detailed analysis of its internal keyword trends over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fbde2",
   "metadata": {},
   "source": [
    "### High-Level Cluster Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES_FILE = \"checkpoint_variables.pkl\"\n",
    "\n",
    "with open(VARIABLES_FILE, \"rb\") as f:\n",
    "            loaded_vars = pickle.load(f)\n",
    "        \n",
    "keyword_features = loaded_vars.get('keyword_cols_created', [])\n",
    "domain_features = loaded_vars.get('unique_domains', [])\n",
    "area_features = loaded_vars.get('unique_areas', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_emerging_cluster_ids(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_column: str = 'cluster_label',  # Default column for cluster labels\n",
    "    date_column: str = 'first_date',\n",
    "    recent_months_window: int = 12,\n",
    "    min_papers_recent_period: int = 5,\n",
    "    emerging_ratio_threshold: float = 1.5,\n",
    "    emerging_diff_threshold: float = 0.001,\n",
    "    newly_active_min_recent_prop: float = 0.0005\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Identifies emerging cluster IDs from the entire dataset by comparing their\n",
    "    proportion of publications in a recent period versus a baseline period.\n",
    "    This version operates globally, without filtering for a specific domain.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Identifying Emerging Cluster IDs (Global) ---\")\n",
    "    emerging_cluster_ids: List[int] = []\n",
    "\n",
    "\n",
    "    # --- 1. Input Validation and Data Preparation ---\n",
    "    # Filter out noise points and operate on a copy\n",
    "    df_analysis = df[df[cluster_column] != -1].copy()\n",
    "\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_analysis[date_column]):\n",
    "        df_analysis[date_column] = pd.to_datetime(df_analysis[date_column])\n",
    "\n",
    "\n",
    "    if df_analysis.empty:\n",
    "        print(\"  No valid clustered data found after filtering noise/NA.\")\n",
    "        return emerging_cluster_ids\n",
    "\n",
    "\n",
    "    # --- 2. Define Time Windows ---\n",
    "    max_date = df_analysis[date_column].max()\n",
    "    recent_period_start_date = max_date - DateOffset(months=recent_months_window)\n",
    "\n",
    "\n",
    "    print(f\"  Recent period starts: {recent_period_start_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "\n",
    "    df_recent = df_analysis[df_analysis[date_column] >= recent_period_start_date]\n",
    "    df_baseline = df_analysis[df_analysis[date_column] < recent_period_start_date]\n",
    "\n",
    "\n",
    "    if df_recent.empty or df_baseline.empty:\n",
    "        print(\"  Not enough data in both recent and baseline periods to assess emergence.\")\n",
    "        return emerging_cluster_ids\n",
    "\n",
    "\n",
    "    # The total is now across the entire dataset for the period\n",
    "    total_papers_recent = len(df_recent)\n",
    "    total_papers_baseline = len(df_baseline)\n",
    "\n",
    "\n",
    "    unique_clusters = sorted(df_analysis[cluster_column].unique())\n",
    "\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        papers_cluster_recent = (df_recent[cluster_column] == cluster_id).sum()\n",
    "        if papers_cluster_recent < min_papers_recent_period:\n",
    "            continue\n",
    "\n",
    "\n",
    "        papers_cluster_baseline = (df_baseline[cluster_column] == cluster_id).sum()\n",
    "\n",
    "\n",
    "        prop_recent = papers_cluster_recent / total_papers_recent\n",
    "        prop_baseline = papers_cluster_baseline / total_papers_baseline\n",
    "        \n",
    "        is_emerging = False\n",
    "        if prop_baseline == 0:\n",
    "            if prop_recent >= newly_active_min_recent_prop:\n",
    "                print(f\"    Cluster {cluster_id}: Newly Active (Recent Prop: {prop_recent:.4f})\")\n",
    "                is_emerging = True\n",
    "        else:\n",
    "            emergence_ratio = prop_recent / prop_baseline\n",
    "            emergence_difference = prop_recent - prop_baseline\n",
    "            if emergence_ratio >= emerging_ratio_threshold and emergence_difference >= emerging_diff_threshold:\n",
    "                print(f\"    Cluster {cluster_id}: Emerging (Ratio: {emergence_ratio:.2f}, Diff: {emergence_difference:.4f})\")\n",
    "                is_emerging = True\n",
    "        \n",
    "        if is_emerging:\n",
    "            emerging_cluster_ids.append(cluster_id)\n",
    "\n",
    "\n",
    "    print(f\"\\n  Identified {len(emerging_cluster_ids)} emerging/newly active cluster IDs: {emerging_cluster_ids}\")\n",
    "    return sorted(list(set(emerging_cluster_ids)))\n",
    "\n",
    "\n",
    "def plot_combined_trends(\n",
    "    proportions_df: pd.DataFrame,\n",
    "    emerging_ids: List[int],\n",
    "    top_keywords_map: Dict[int, List[str]],\n",
    "    title: str,\n",
    "    default_color: str = 'grey'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots smoothed proportion trends for all clusters, ensuring distinct colors\n",
    "    for the highlighted emerging ones by using a colormap.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Combined Temporal Trends with Distinct Colors ---\")\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "\n",
    "    # Plot non-emerging clusters first in grey\n",
    "    for cluster_id in proportions_df.columns:\n",
    "        if cluster_id != -1 and cluster_id not in emerging_ids:\n",
    "            ax.plot(proportions_df.index, proportions_df[cluster_id] * 100, color=default_color, lw=1.0, alpha=0.5)\n",
    "\n",
    "\n",
    "    # --- Generate a set of distinct colors for the emerging clusters ---\n",
    "    n_emerging = len(emerging_ids)\n",
    "    # Use a vibrant colormap like 'jet' or 'viridis' to get many distinct colors.\n",
    "    # np.linspace creates an evenly spaced sequence of numbers, and cm.jet maps\n",
    "    # each of these numbers to a unique color.\n",
    "    colors = cm.jet(np.linspace(0, 1, n_emerging))\n",
    "\n",
    "\n",
    "    # Plot emerging clusters on top, each with a unique color from our generated list\n",
    "    for i, cluster_id in enumerate(emerging_ids):\n",
    "        if cluster_id in proportions_df.columns:\n",
    "            #label_text = f\"Cluster {cluster_id}: {top_keywords_map.get(cluster_id, ['N/A'])[0]}\"\n",
    "            label_text = f\"Cluster {cluster_id}\"\n",
    "            # The key change is here: assign a unique color from our list\n",
    "            ax.plot(\n",
    "                proportions_df.index, \n",
    "                proportions_df[cluster_id] * 100, \n",
    "                lw=2.5, \n",
    "                label=label_text,\n",
    "                color=colors[i] \n",
    "            )\n",
    "\n",
    "\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Share of Monthly Publications (%)')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=100.0, decimals=1))\n",
    "    \n",
    "    if emerging_ids:\n",
    "        # Adjust legend font size if there are many items to prevent overlap\n",
    "        legend_fontsize = 'small' if n_emerging > 20 else 'medium'\n",
    "        ax.legend(\n",
    "            title='Emerging Clusters', \n",
    "            bbox_to_anchor=(1.02, 1), \n",
    "            loc='upper left',\n",
    "            fontsize=legend_fontsize\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
    "    plt.show()\n",
    "\n",
    "def plot_emerging_trends_only(\n",
    "    proportions_df: pd.DataFrame,\n",
    "    emerging_ids: List[int],\n",
    "    top_keywords_map: Dict[int, List[str]],\n",
    "    title: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots smoothed proportion trends for ONLY the emerging clusters, ensuring\n",
    "    each has a distinct color for clear visualization. Omits all non-emerging\n",
    "    (grey) lines.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Trends for Emerging Clusters Only ---\")\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "\n",
    "    # --- Generate a set of distinct colors for the emerging clusters ---\n",
    "    n_emerging = len(emerging_ids)\n",
    "    if n_emerging == 0:\n",
    "        print(\"  No emerging clusters to plot.\")\n",
    "        ax.text(0.5, 0.5, 'No emerging clusters found.', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "    colors = cm.jet(np.linspace(0, 1, n_emerging))\n",
    "\n",
    "\n",
    "    # --- Plot ONLY the emerging clusters ---\n",
    "    for i, cluster_id in enumerate(emerging_ids):\n",
    "        if cluster_id in proportions_df.columns:\n",
    "            #label_text = f\"Cluster {cluster_id}: {top_keywords_map.get(cluster_id, ['N/A'])[0]}\"\n",
    "            label_text = f\"Cluster {cluster_id}\"\n",
    "            ax.plot(\n",
    "                proportions_df.index, \n",
    "                proportions_df[cluster_id] * 100, \n",
    "                lw=2.5, \n",
    "                label=label_text,\n",
    "                color=colors[i] \n",
    "            )\n",
    "\n",
    "\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Share of Monthly Publications (%)')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=100.0, decimals=1))\n",
    "    \n",
    "    # Adjust legend font size if there are many items to prevent overlap\n",
    "    legend_fontsize = 'small' if n_emerging > 20 else 'medium'\n",
    "    ax.legend(\n",
    "        title='Emerging Clusters', \n",
    "        bbox_to_anchor=(1.02, 1), \n",
    "        loc='upper left',\n",
    "        fontsize=legend_fontsize\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_domain_trends_plotly(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_id: int,\n",
    "    domain_features: List[str],\n",
    "    num_domains_to_plot: int = 5,\n",
    "    date_column: str = 'first_date',\n",
    "    cluster_column: str = 'cluster_label',\n",
    "    rolling_window: int = 12\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the temporal trend of the top N domains/areas for a specific cluster using Plotly.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Domain Trends for Cluster: {cluster_id} ---\")\n",
    "\n",
    "\n",
    "    # Filter the DataFrame for the selected cluster\n",
    "    df_filtered = df[df[cluster_column] == cluster_id].copy()\n",
    "    if df_filtered.empty:\n",
    "        print(f\"  No data found for cluster '{cluster_id}'.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Identify the top N domains for THIS specific cluster ---\n",
    "    # This is calculated on the fly from the filtered data\n",
    "    top_domains = df_filtered[domain_features].mean().sort_values(ascending=False).head(num_domains_to_plot)\n",
    "    domains_to_plot = top_domains.index.tolist()\n",
    "    print(f\"  Top {len(domains_to_plot)} domains: {domains_to_plot}\")\n",
    "\n",
    "\n",
    "    # Add a YearMonth column for grouping\n",
    "    df_filtered.loc[:, 'YearMonth'] = pd.to_datetime(df_filtered[date_column]).dt.to_period('M')\n",
    "\n",
    "\n",
    "    # Prepare a dictionary to store monthly proportions for each domain\n",
    "    domain_monthly_proportions_dict = {}\n",
    "    grouped_by_month = df_filtered.groupby('YearMonth')\n",
    "\n",
    "\n",
    "    for domain in domains_to_plot:\n",
    "        # The domain name is the column name, so no transformation is needed\n",
    "        monthly_proportions = grouped_by_month[domain].mean()\n",
    "        domain_monthly_proportions_dict[domain] = monthly_proportions\n",
    "\n",
    "\n",
    "    if not domain_monthly_proportions_dict:\n",
    "        print(\"  No domain data could be processed for plotting.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Create a DataFrame for the proportions\n",
    "    proportions_df = pd.DataFrame(domain_monthly_proportions_dict)\n",
    "\n",
    "\n",
    "    # Reindex to fill missing months\n",
    "    if not proportions_df.empty:\n",
    "        full_date_range = pd.period_range(\n",
    "            start=proportions_df.index.min().to_timestamp(),\n",
    "            end=proportions_df.index.max().to_timestamp(),\n",
    "            freq='M'\n",
    "        )\n",
    "        proportions_df = proportions_df.reindex(full_date_range, fill_value=0)\n",
    "\n",
    "\n",
    "    # Smooth the proportions using a rolling window\n",
    "    smoothed_proportions_df = proportions_df.rolling(window=rolling_window, center=True, min_periods=1).mean()\n",
    "    smoothed_proportions_df.index = smoothed_proportions_df.index.to_timestamp()\n",
    "\n",
    "\n",
    "    # Plot the trends using Plotly\n",
    "    fig = go.Figure()\n",
    "    for domain in smoothed_proportions_df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=smoothed_proportions_df.index,\n",
    "            y=smoothed_proportions_df[domain] * 100,\n",
    "            mode='lines',\n",
    "            name=domain,\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Internal Domain/Area Trends for Cluster {cluster_id}',\n",
    "        title_x=0.5,\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Share of Papers in Cluster (%)',\n",
    "        yaxis_tickformat='.1f',\n",
    "        yaxis_ticksuffix='%',\n",
    "        legend_title_text='Domains / Areas',\n",
    "        height=600,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def plot_cluster_area_trends_plotly(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_id: int,\n",
    "    area_features: List[str],\n",
    "    num_areas_to_plot: int = 5,\n",
    "    date_column: str = 'first_date',\n",
    "    cluster_column: str = 'cluster_label',\n",
    "    rolling_window: int = 12\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the temporal trend of the top N areas for a specific cluster using Plotly.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Area Trends (Plotly) for Cluster: {cluster_id} ---\")\n",
    "\n",
    "\n",
    "    # Filter the DataFrame for the selected cluster\n",
    "    df_filtered = df[df[cluster_column] == cluster_id].copy()\n",
    "    if df_filtered.empty:\n",
    "        print(f\"  No data found for cluster '{cluster_id}'.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Identify the top N areas for THIS specific cluster ---\n",
    "    top_areas = df_filtered[area_features].mean().sort_values(ascending=False).head(num_areas_to_plot)\n",
    "    areas_to_plot = top_areas.index.tolist()\n",
    "    print(f\"  Top {len(areas_to_plot)} areas for this cluster: {areas_to_plot}\")\n",
    "\n",
    "\n",
    "    # Add a YearMonth column for grouping\n",
    "    df_filtered.loc[:, 'YearMonth'] = pd.to_datetime(df_filtered[date_column]).dt.to_period('M')\n",
    "\n",
    "\n",
    "    # Prepare a dictionary to store monthly proportions for each area\n",
    "    area_monthly_proportions_dict = {}\n",
    "    grouped_by_month = df_filtered.groupby('YearMonth')\n",
    "\n",
    "\n",
    "    for area in areas_to_plot:\n",
    "        monthly_proportions = grouped_by_month[area].mean()\n",
    "        area_monthly_proportions_dict[area] = monthly_proportions\n",
    "\n",
    "\n",
    "    if not area_monthly_proportions_dict:\n",
    "        print(\"  No area data could be processed for plotting.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Create a DataFrame for the proportions\n",
    "    proportions_df = pd.DataFrame(area_monthly_proportions_dict)\n",
    "\n",
    "\n",
    "    # Reindex to fill missing months\n",
    "    if not proportions_df.empty:\n",
    "        full_date_range = pd.period_range(\n",
    "            start=proportions_df.index.min().to_timestamp(),\n",
    "            end=proportions_df.index.max().to_timestamp(),\n",
    "            freq='M'\n",
    "        )\n",
    "        proportions_df = proportions_df.reindex(full_date_range, fill_value=0)\n",
    "\n",
    "\n",
    "    # Smooth the proportions using a rolling window\n",
    "    smoothed_proportions_df = proportions_df.rolling(window=rolling_window, center=True, min_periods=1).mean()\n",
    "    smoothed_proportions_df.index = smoothed_proportions_df.index.to_timestamp()\n",
    "\n",
    "\n",
    "    # Plot the trends using Plotly\n",
    "    fig = go.Figure()\n",
    "    for area in smoothed_proportions_df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=smoothed_proportions_df.index,\n",
    "            y=smoothed_proportions_df[area] * 100,\n",
    "            mode='lines',\n",
    "            name=area,\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Internal Area Trends for Cluster {cluster_id}<br>({rolling_window}-Month Rolling Average)',\n",
    "        title_x=0.5,\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Share of Papers in Cluster (%)',\n",
    "        yaxis_tickformat='.1f',\n",
    "        yaxis_ticksuffix='%',\n",
    "        legend_title_text='Areas',\n",
    "        height=600,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## (Corrected) Load Data for Full Analysis (Memory Optimized)\n",
    "# \n",
    "# **This is the definitive block for loading data after the pipeline has been run.**\n",
    "# \n",
    "# It robustly combines the computed results (clusters, coordinates) with the original features needed for analysis, without ever loading the entire combined dataset into memory at once.\n",
    "\n",
    "# %%\n",
    "# --- Define file paths ---\n",
    "# The file with the final UMAP coordinates and cluster labels\n",
    "RESULTS_FILE = \"df_with_2D_viz_coords.parquet\" \n",
    "# The original data file with all metadata and keyword features\n",
    "ORIGINAL_DATA_FILE = \"checkpoint_with_keywords.parquet\" \n",
    "\n",
    "print(\"--- Loading results and features separately and merging them ---\")\n",
    "all_features\n",
    "try:\n",
    "    # --- Step 1: Load ONLY the computed results ---\n",
    "    # These are the columns that took hours to generate.\n",
    "    results_cols = ['id', 'cluster_label', 'umap_x', 'umap_y']\n",
    "    print(f\"Loading computed results from '{RESULTS_FILE}'...\")\n",
    "    df_results = pd.read_parquet(RESULTS_FILE, columns=results_cols)\n",
    "    print(f\"Loaded results. Shape: {df_results.shape}\")\n",
    "\n",
    "    # --- Step 2: Load the original data features needed for analysis ---\n",
    "    # We load from the original file, which is guaranteed to have all columns.\n",
    "    # We exclude columns already in df_results (except the 'id' key) to save memory.\n",
    "    print(f\"Loading original features from '{ORIGINAL_DATA_FILE}'...\")\n",
    "    df_features = pd.read_parquet(ORIGINAL_DATA_FILE)\n",
    "    \n",
    "    # Define which original columns to keep for the merge\n",
    "    # We need everything EXCEPT the results columns we already loaded\n",
    "    cols_to_keep = [col for col in df_features.columns if col not in ['cluster_label', 'umap_x', 'umap_y']]\n",
    "    df_features = df_features[cols_to_keep]\n",
    "    print(f\"Loaded features. Shape: {df_features.shape}\")\n",
    "\n",
    "    # --- Step 3: Merge the two DataFrames on the 'id' column ---\n",
    "    # This joins the results with the features needed to analyze them.\n",
    "    print(\"\\nMerging results and features...\")\n",
    "    df = pd.merge(df_results, df_features, on='id', how='left')\n",
    "    print(f\"Successfully created final merged DataFrame. Shape: {df.shape}\")\n",
    "\n",
    "    # Clean up to free up memory\n",
    "    del df_results\n",
    "    del df_features\n",
    "    \n",
    "    # --- Step 4: Prepare the 'plot_df' for visualization ---\n",
    "    # This part is fast and uses the newly created full 'df'\n",
    "    print(\"\\n--- Preparing data for Plotly ---\")\n",
    "    plot_df = df.copy()\n",
    "    plot_df['cluster_label_str'] = plot_df['cluster_label'].astype(str)\n",
    "    plot_df.loc[plot_df['cluster_label'] == -1, 'cluster_label_str'] = 'Noise'\n",
    "\n",
    "    # Calculate cluster sizes for the legend\n",
    "    cluster_sizes = plot_df['cluster_label_str'].value_counts().reset_index()\n",
    "    cluster_sizes.columns = ['cluster_label_str', 'count']\n",
    "    plot_df = pd.merge(plot_df, cluster_sizes, on='cluster_label_str')\n",
    "    plot_df['legend_entry'] = plot_df['cluster_label_str'] + ' (' + plot_df['count'].astype(str) + ')'\n",
    "    \n",
    "    print(\"Plotting data is ready.\")\n",
    "    \n",
    "    # --- Initialize the figure object ---\n",
    "    fig = go.Figure()\n",
    "    print(\"Plotly figure object initialized. You can now proceed with plotting and analysis.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: A required file was not found: {e}\")\n",
    "    print(\"Please ensure both 'df_with_2D_viz_coords.parquet' and 'checkpoint_with_keywords.parquet' are present.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or preparing the data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa836540",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_features\n",
    "    meta_features\n",
    "    domain_features\n",
    "    keyword_features\n",
    "except NameError:\n",
    "    print(\"Error: Feature lists (all_features, etc.) not found. Please re-run the feature preparation cells.\")\n",
    "    # As a fallback, try to reconstruct them\n",
    "    domain_features = [col for col in unique_domains if col in df.columns]\n",
    "    meta_features = [col for col in metadata_features if col in df.columns]\n",
    "    keyword_features = [col for col in keyword_cols_created if col in df.columns]\n",
    "\n",
    "# We are interested in the actual clusters, so we exclude noise points (label -1)\n",
    "df_clustered = df[df['cluster_label'] != -1].copy()\n",
    "\n",
    "if df_clustered.empty:\n",
    "    print(\"No clustered data available to analyze (all points might be noise).\")\n",
    "else:\n",
    "    # Group by the new global cluster label\n",
    "    grouped = df_clustered.groupby('cluster_label')\n",
    "\n",
    "# --- Define a helper function to get top features ---\n",
    "def get_top_n_features(group, feature_list, n=5):\n",
    "    \"\"\"Calculates the mean for features and returns the top N.\"\"\"\n",
    "    # Ensure features exist in the group's columns\n",
    "    valid_features = [f for f in feature_list if f in group.columns]\n",
    "    if not valid_features:\n",
    "        return []\n",
    "    \n",
    "    # Calculate mean, sort, and get top N feature names\n",
    "    top_features = group[valid_features].mean().sort_values(ascending=False).head(n)\n",
    "    return top_features.index.tolist()\n",
    "\n",
    "# --- Calculate summary statistics for each cluster ---\n",
    "print(\"Calculating summary statistics for each cluster...\")\n",
    "summary_list = []\n",
    "\n",
    "# Get a list of all cluster labels to iterate over\n",
    "all_cluster_ids = sorted(df_clustered['cluster_label'].unique())\n",
    "\n",
    "for cluster_id in all_cluster_ids:\n",
    "    group = grouped.get_group(cluster_id)\n",
    "    \n",
    "    # Basic stats\n",
    "    size = len(group)\n",
    "    median_date = group['first_date'].median().strftime('%Y-%m')\n",
    "    avg_authors = group['number_of_authors'].mean()\n",
    "    \n",
    "    # Top keywords (remove 'kw_' prefix for readability)\n",
    "    top_keywords_raw = get_top_n_features(group, keyword_features, n=10)\n",
    "    top_keywords = [kw.replace('kw_', '').replace(' ', '_') for kw in top_keywords_raw]\n",
    "\n",
    "    # Top domains\n",
    "    top_domains = get_top_n_features(group, domain_features, n=3)\n",
    "    \n",
    "    summary_list.append({\n",
    "        'Cluster ID': cluster_id,\n",
    "        'Size': size,\n",
    "        'Median Date': median_date,\n",
    "        'Avg Authors': f\"{avg_authors:.2f}\",\n",
    "        'Top 5 Keywords': ', '.join(top_keywords[:5]),\n",
    "        'Top Domains': ', '.join(top_domains),\n",
    "        '_full_keyword_list': top_keywords  # Store for later use\n",
    "    })\n",
    "\n",
    "# Create the summary DataFrame\n",
    "cluster_summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# Create a dictionary mapping cluster ID to its full list of top keywords for later use\n",
    "top_keywords_per_cluster_map = cluster_summary_df.set_index('Cluster ID')['_full_keyword_list'].to_dict()\n",
    "\n",
    "# Drop the helper column before displaying\n",
    "cluster_summary_df = cluster_summary_df.drop(columns=['_full_keyword_list'])\n",
    "\n",
    "# Sort by size for a more organized view\n",
    "cluster_summary_df = cluster_summary_df.sort_values('Size', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n--- Cluster Summary Table ---\")\n",
    "# Display the full DataFrame\n",
    "pd.set_option('display.max_rows', len(cluster_summary_df) + 5)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "display(cluster_summary_df)\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eae4f9",
   "metadata": {},
   "source": [
    "### Identifying Emerging Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_temporal_proportions(df, cluster_column='cluster_label', date_column='first_date', rolling_window=6):\n",
    "    \"\"\"Calculates the monthly proportion of all papers belonging to each cluster.\"\"\"\n",
    "    print(\"\\n--- Calculating Temporal Proportions for All Clusters ---\")\n",
    "    # Ensure date column is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_column]):\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    # Create a YearMonth period\n",
    "    df_temp = df.copy()\n",
    "    df_temp['YearMonth'] = df_temp[date_column].dt.to_period('M')\n",
    "\n",
    "    # Count papers per YearMonth and cluster (including noise)\n",
    "    monthly_counts = df_temp.groupby(['YearMonth', cluster_column]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Total papers per YearMonth (overall)\n",
    "    total_monthly_counts = monthly_counts.sum(axis=1)\n",
    "\n",
    "    # Calculate proportion, avoiding division by zero\n",
    "    proportions_df = monthly_counts.divide(total_monthly_counts, axis=0).fillna(0)\n",
    "\n",
    "    # Smooth the proportions\n",
    "    smoothed_proportions_df = proportions_df.rolling(window=rolling_window, center=True, min_periods=1).mean()\n",
    "\n",
    "    # Convert index to timestamp for plotting\n",
    "    smoothed_proportions_df.index = smoothed_proportions_df.index.to_timestamp()\n",
    "\n",
    "    print(f\"Temporal proportions calculated. Shape: {smoothed_proportions_df.shape}\")\n",
    "    return smoothed_proportions_df\n",
    "\n",
    "# --- Execute the Emergence Analysis ---\n",
    "# 1. Calculate proportions for all clusters over time\n",
    "temporal_proportions = calculate_temporal_proportions(df, rolling_window=12)\n",
    "\n",
    "# 2. Identify emerging clusters\n",
    "emerging_cluster_ids = identify_emerging_cluster_ids(\n",
    "    df,\n",
    "    recent_months_window=12,\n",
    "    min_papers_recent_period=100,\n",
    "    emerging_ratio_threshold=2.0,\n",
    "    emerging_diff_threshold=0.001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123eb605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_trends(\n",
    "    proportions_df=temporal_proportions,\n",
    "    emerging_ids=emerging_cluster_ids,\n",
    "    top_keywords_map=top_keywords_per_cluster_map,\n",
    "    title='Temporal Trends of All Clusters'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c788d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emerging_trends_only(\n",
    "    proportions_df=temporal_proportions,\n",
    "    emerging_ids=emerging_cluster_ids,\n",
    "    top_keywords_map=top_keywords_per_cluster_map,\n",
    "    title='Temporal Trends of Emerging Topics'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Generating UMAP plot for {len(emerging_cluster_ids)} emerging clusters ---\")\n",
    "print(f\"Using colors consistent with the time series plots.\")\n",
    "\n",
    "# 1. Filter the DataFrame to get only the data for emerging clusters\n",
    "emerging_df = df[df['cluster_label'].isin(emerging_cluster_ids)].copy()\n",
    "\n",
    "# 2. Generate the exact same color sequence as the time series plot\n",
    "n_emerging = len(emerging_cluster_ids)\n",
    "# This line is identical to the one in your plot_emerging_trends_only function [[11]]\n",
    "colors_rgba = cm.jet(np.linspace(0, 1, n_emerging))\n",
    "\n",
    "# Convert matplotlib's RGBA (0-1 scale) to Plotly's 'rgb(r,g,b)' string (0-255 scale)\n",
    "colors_rgb_str = [f'rgb({int(r*255)}, {int(g*255)}, {int(b*255)})' for r, g, b, a in colors_rgba]\n",
    "\n",
    "# 3. Create a dictionary that maps each cluster ID to its specific color\n",
    "# Since emerging_cluster_ids is sorted, this assigns colors in the same order as the plot\n",
    "color_map = dict(zip(emerging_cluster_ids, colors_rgb_str))\n",
    "\n",
    "# 4. Create the Plotly figure\n",
    "fig_emerging_colored = go.Figure()\n",
    "\n",
    "# Add a separate trace for each emerging cluster. This is the best way to\n",
    "# assign specific colors and create a clean legend.\n",
    "for cluster_id in emerging_cluster_ids:\n",
    "    cluster_data = emerging_df[emerging_df['cluster_label'] == cluster_id]\n",
    "    \n",
    "    fig_emerging_colored.add_trace(go.Scattergl(\n",
    "        x=cluster_data['umap_x'],\n",
    "        y=cluster_data['umap_y'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=color_map[cluster_id],  # Use the specific color from our map\n",
    "            size=5,\n",
    "            opacity=0.9\n",
    "        ),\n",
    "        # Use the same hovertemplate as your other plots for consistency\n",
    "        customdata=cluster_data[['id', 'title', 'cluster_label']],\n",
    "        hovertemplate='<b>Title:</b> %{customdata[1]}<br>' +\n",
    "                      '<b>ID:</b> %{customdata[0]}<br>' +\n",
    "                      '<b>Cluster:</b> %{customdata[2]}<br>' +\n",
    "                      '<extra></extra>',\n",
    "        name=f'Cluster {cluster_id}'  # This creates the legend entry\n",
    "    ))\n",
    "\n",
    "# 5. Update the layout\n",
    "fig_emerging_colored.update_layout(\n",
    "    title='UMAP Visualization of Emerging Clusters (with Time Series Colors)',\n",
    "    xaxis_title='UMAP Dimension 1',\n",
    "    yaxis_title='UMAP Dimension 2',\n",
    "    height=800,\n",
    "    legend_title_text='Emerging Cluster ID',\n",
    "    # Optional: If you prefer a dark theme like your music taste might suggest\n",
    "    # template='plotly_dark' \n",
    ")\n",
    "\n",
    "fig_emerging_colored.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_overall_emerging_summary_table(\n",
    "    df: pd.DataFrame,\n",
    "    emerging_ids: List[int],\n",
    "    keyword_features: List[str],\n",
    "    domain_features: List[str],\n",
    "    area_features: List[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates and displays a summary table for all emerging clusters,\n",
    "    showing the top items and their prevalence as a percentage.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Overall Summary Table with Percentages for All Emerging Clusters ---\")\n",
    "\n",
    "    if not emerging_ids:\n",
    "        print(\"No emerging clusters to summarize.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Aggregate all emerging clusters into a single DataFrame ---\n",
    "    df_emerging = df[df['cluster_label'].isin(emerging_ids)]\n",
    "    \n",
    "    if df_emerging.empty:\n",
    "        print(\"No documents found for the given emerging cluster IDs.\")\n",
    "        return\n",
    "\n",
    "    # Helper function to get top features with their scores (mean values).\n",
    "    def get_top_n_features_with_scores(group, feature_list, n=5):\n",
    "        valid_features = [f for f in feature_list if f in group.columns]\n",
    "        if not valid_features:\n",
    "            return pd.Series(dtype=float)\n",
    "        top_features_series = group[valid_features].mean().sort_values(ascending=False).head(n)\n",
    "        return top_features_series\n",
    "\n",
    "    # Helper function to format the Series for table display\n",
    "    def format_series_for_table(series, prefix_to_remove=''):\n",
    "        if series.empty:\n",
    "            return \"\", \"\"\n",
    "        names = [str(name).replace(prefix_to_remove, '') for name in series.index]\n",
    "        scores = [f\"{score * 100:.1f}\" for score in series.values]\n",
    "        \n",
    "        names_str = '<br>'.join(names)\n",
    "        scores_str = '<br>'.join(scores)\n",
    "        return names_str, scores_str\n",
    "\n",
    "    # --- 2. Calculate top features and their scores ---\n",
    "    top_keywords_series = get_top_n_features_with_scores(df_emerging, keyword_features, n=10)\n",
    "    top_domains_series = get_top_n_features_with_scores(df_emerging, domain_features, n=5)\n",
    "    top_areas_series = get_top_n_features_with_scores(df_emerging, area_features, n=10)\n",
    "\n",
    "    # --- 3. Format the results for the table ---\n",
    "    top_keywords_str, keyword_scores_str = format_series_for_table(top_keywords_series, prefix_to_remove='kw_')\n",
    "    top_domains_str, domain_scores_str = format_series_for_table(top_domains_series)\n",
    "    top_areas_str, area_scores_str = format_series_for_table(top_areas_series)\n",
    "\n",
    "    # --- 4. Create the Plotly Table ---\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        # --- KEY CHANGE: Set the relative widths of the columns ---\n",
    "        # These numbers are ratios. Here, the second column is given much more space.\n",
    "        columnwidth = [30, 50, 20],\n",
    "        header=dict(\n",
    "            values=['<b>Characteristic</b>', '<b>Top Items</b>', '<b>Prevalence [%]</b>'],\n",
    "            line_color='darkslategray',\n",
    "            fill_color='#9E9E9E',\n",
    "            align=['left', 'left', 'center'],\n",
    "            font=dict(color='black', size=14)\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                ['<b>Top 10 Keywords</b>', '<b>Top 5 Domains</b>', '<b>Top 10 Areas</b>'],\n",
    "                [top_keywords_str, top_domains_str, top_areas_str],\n",
    "                [keyword_scores_str, domain_scores_str, area_scores_str]\n",
    "            ],\n",
    "            line_color='darkslategray',\n",
    "            fill_color='white',\n",
    "            align=['left', 'left', 'center'],\n",
    "            font=dict(color='black', size=12),\n",
    "            height=40\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"<b>Overall Prevalence of Features in Emerging Research</b>\",\n",
    "        title_x=0.5,\n",
    "        height=600,\n",
    "        width=900, # Increased width slightly to better fit the wider column\n",
    "        margin=dict(l=20, r=20, t=50, b=20),\n",
    "        plot_bgcolor=\"#FFFFFF\",\n",
    "        paper_bgcolor=\"#FFFFFF\",\n",
    "        font_color='black'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "display_overall_emerging_summary_table(\n",
    "    df=df,\n",
    "    emerging_ids=emerging_cluster_ids,\n",
    "    keyword_features=keyword_features,\n",
    "    domain_features=domain_features,\n",
    "    area_features=area_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_emerging_cluster_summary_tables(\n",
    "    df: pd.DataFrame,\n",
    "    emerging_ids: List[int],\n",
    "    keyword_features: List[str],\n",
    "    domain_features: List[str],\n",
    "    area_features: List[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a dynamically sized summary table with prevalence percentages for each\n",
    "    individual emerging cluster, removing extra whitespace.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Individual Summary Tables for {len(emerging_ids)} Emerging Clusters ---\")\n",
    "\n",
    "    if not emerging_ids:\n",
    "        print(\"No emerging clusters to display.\")\n",
    "        return\n",
    "\n",
    "    # Helper functions remain the same\n",
    "    def get_top_n_features_with_scores(group, feature_list, n=5):\n",
    "        valid_features = [f for f in feature_list if f in group.columns]\n",
    "        if not valid_features:\n",
    "            return pd.Series(dtype=float)\n",
    "        top_features_series = group[valid_features].mean().sort_values(ascending=False).head(n)\n",
    "        return top_features_series\n",
    "\n",
    "    def format_series_for_table(series, prefix_to_remove=''):\n",
    "        if series.empty:\n",
    "            return \"\", \"\"\n",
    "        names = [str(name).replace(prefix_to_remove, '') for name in series.index]\n",
    "        scores = [f\"{score * 100:.1f}\" for score in series.values]\n",
    "        names_str = '<br>'.join(names)\n",
    "        scores_str = '<br>'.join(scores)\n",
    "        return names_str, scores_str\n",
    "\n",
    "    # Loop through each emerging cluster ID\n",
    "    for cluster_id in emerging_ids:\n",
    "        group = df[df['cluster_label'] == cluster_id]\n",
    "        if group.empty:\n",
    "            print(f\"Skipping Cluster {cluster_id}: No documents found.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate top features\n",
    "        top_keywords_series = get_top_n_features_with_scores(group, keyword_features, n=5)\n",
    "        top_domains_series = get_top_n_features_with_scores(group, domain_features, n=3)\n",
    "        top_areas_series = get_top_n_features_with_scores(group, area_features, n=5)\n",
    "\n",
    "        # Format results for the table\n",
    "        num_records = len(group)\n",
    "        top_keywords_str, keyword_scores_str = format_series_for_table(top_keywords_series, prefix_to_remove='kw_')\n",
    "        top_domains_str, domain_scores_str = format_series_for_table(top_domains_series)\n",
    "        top_areas_str, area_scores_str = format_series_for_table(top_areas_series)\n",
    "\n",
    "        # --- KEY CHANGE 1: Dynamically calculate figure height ---\n",
    "        # Count the total number of lines that will be displayed in the table.\n",
    "        num_lines = (\n",
    "            1  # For \"Number of Records\"\n",
    "            + len(top_keywords_series)\n",
    "            + len(top_domains_series)\n",
    "            + len(top_areas_series)\n",
    "        )\n",
    "        \n",
    "        # Define base heights and calculate the total needed height\n",
    "        header_height = 40\n",
    "        row_height = 28 \n",
    "        margin_height = 80 # For title and bottom margin\n",
    "        dynamic_height = (num_lines * row_height) + header_height + margin_height\n",
    "\n",
    "        # --- Create the Plotly Table ---\n",
    "        fig = go.Figure(data=[go.Table(\n",
    "            columnwidth=[30, 50, 25],\n",
    "            header=dict(\n",
    "                values=['<b>Characteristic</b>', '<b>Top Items</b>', '<b>Prevalence [%]</b>'],\n",
    "                line_color='darkslategray',\n",
    "                fill_color=\"#9E9E9E\",\n",
    "                align=['left', 'left', 'center'],\n",
    "                font=dict(color='black', size=14),\n",
    "                height=header_height\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    ['Number of Records', 'Top 5 Keywords', 'Top 3 Domains', 'Top 5 Areas'],\n",
    "                    [num_records, top_keywords_str, top_domains_str, top_areas_str],\n",
    "                    # --- KEY CHANGE 2: Replace '100.0%' with '-' ---\n",
    "                    ['-', keyword_scores_str, domain_scores_str, area_scores_str]\n",
    "                ],\n",
    "                line_color='darkslategray',\n",
    "                fill_color='white',\n",
    "                align=['left', 'left', 'center'],\n",
    "                font=dict(color='black', size=12),\n",
    "                height=row_height \n",
    "            )\n",
    "        )])\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"<b>Summary for Emerging Cluster {cluster_id}</b>\",\n",
    "            title_x=0.5,\n",
    "            height=dynamic_height, # Use the calculated dynamic height\n",
    "            width=800,\n",
    "            margin=dict(l=20, r=20, t=50, b=20),\n",
    "            plot_bgcolor=\"#FFFFFF\",\n",
    "            paper_bgcolor=\"#FFFFFF\",\n",
    "            font_color='black'\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display_emerging_cluster_summary_tables(\n",
    "    df=df,\n",
    "    emerging_ids=emerging_cluster_ids,\n",
    "    keyword_features=keyword_features,\n",
    "    domain_features=domain_features,\n",
    "    area_features=area_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf8bf7",
   "metadata": {},
   "source": [
    "### Deep Dive into Specific Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_keyword_trends_plotly(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_id: int,\n",
    "    all_cluster_keywords: List[str],\n",
    "    num_keywords_to_plot: int = 10,\n",
    "    date_column: str = 'first_date',\n",
    "    text_column: str = 'abstract',\n",
    "    cluster_column: str = 'cluster_label',\n",
    "    rolling_window: int = 12\n",
    "):\n",
    "    \"\"\"Plots the temporal trend of the top N keywords for a specific cluster using Plotly.\"\"\"\n",
    "    print(f\"\\n--- Plotting Keyword Trends (Plotly) for Cluster: {cluster_id} ---\")\n",
    "\n",
    "    # Filter the DataFrame for the selected cluster\n",
    "    df_filtered = df[df[cluster_column] == cluster_id].copy()\n",
    "    if df_filtered.empty:\n",
    "        print(f\"  No data found for cluster '{cluster_id}'.\")\n",
    "        return\n",
    "\n",
    "    # Add a YearMonth column for grouping\n",
    "    df_filtered.loc[:, 'YearMonth'] = pd.to_datetime(df_filtered[date_column]).dt.to_period('M')\n",
    "\n",
    "    # Select the top keywords for the cluster\n",
    "    keywords_to_plot = all_cluster_keywords[:num_keywords_to_plot]\n",
    "\n",
    "    # Prepare a dictionary to store monthly proportions for each keyword\n",
    "    keyword_monthly_proportions_dict = {}\n",
    "    grouped_by_month = df_filtered.groupby('YearMonth')\n",
    "\n",
    "    for keyword in keywords_to_plot:\n",
    "        # The keyword from the summary is already cleaned, but we search for the original form\n",
    "        keyword_col_name = 'kw_' + keyword.replace('_', ' ')\n",
    "        if keyword_col_name not in df.columns:\n",
    "            print(f\"  Warning: Keyword column '{keyword_col_name}' not found. Skipping keyword '{keyword}'.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate proportion based on the binary feature column\n",
    "        monthly_proportions = grouped_by_month[keyword_col_name].mean()\n",
    "        keyword_monthly_proportions_dict[keyword] = monthly_proportions\n",
    "\n",
    "    if not keyword_monthly_proportions_dict:\n",
    "        print(\"  No keyword data could be processed for plotting.\")\n",
    "        return\n",
    "\n",
    "    # Create a DataFrame for the proportions\n",
    "    proportions_df = pd.DataFrame(keyword_monthly_proportions_dict)\n",
    "\n",
    "    # Reindex to fill missing months\n",
    "    if not proportions_df.empty:\n",
    "        full_date_range = pd.period_range(\n",
    "            start=proportions_df.index.min().to_timestamp(),\n",
    "            end=proportions_df.index.max().to_timestamp(),\n",
    "            freq='M'\n",
    "        )\n",
    "        proportions_df = proportions_df.reindex(full_date_range, fill_value=0)\n",
    "\n",
    "    # Smooth the proportions using a rolling window\n",
    "    smoothed_proportions_df = proportions_df.rolling(window=rolling_window, center=True, min_periods=1).mean()\n",
    "    smoothed_proportions_df.index = smoothed_proportions_df.index.to_timestamp()\n",
    "\n",
    "    # Plot the trends using Plotly\n",
    "    fig = go.Figure()\n",
    "    for keyword in smoothed_proportions_df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=smoothed_proportions_df.index,\n",
    "            y=smoothed_proportions_df[keyword] * 100,\n",
    "            mode='lines',\n",
    "            name=keyword,\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Internal Keyword Trends for Cluster {cluster_id}<br>({rolling_window}-Month Rolling Average)',\n",
    "        title_x=0.5,\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Share of Papers in Cluster (%)',\n",
    "        yaxis_tickformat='.1f',\n",
    "        yaxis_ticksuffix='%',\n",
    "        legend_title_text='Keywords',\n",
    "        height=600,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# %%\n",
    "# --- Execute the Deep Dive ---\n",
    "# Let's pick the first emerging cluster to analyze.\n",
    "# If no clusters were identified as emerging, we can pick the largest one from the summary table.\n",
    "if emerging_cluster_ids:\n",
    "    cluster_to_analyze = emerging_cluster_ids[0]\n",
    "    print(f\"Analyzing the first emerging cluster: {cluster_to_analyze}\")\n",
    "elif not cluster_summary_df.empty:\n",
    "    cluster_to_analyze = cluster_summary_df.iloc[0]['Cluster ID']\n",
    "    print(f\"No emerging clusters found. Analyzing the largest cluster: {cluster_to_analyze}\")\n",
    "else:\n",
    "    cluster_to_analyze = None\n",
    "    print(\"No clusters available for deep dive analysis.\")\n",
    "\n",
    "if cluster_to_analyze is not None:\n",
    "    # Get the top keywords for this cluster from our map\n",
    "    keywords_for_cluster = top_keywords_per_cluster_map.get(cluster_to_analyze, [])\n",
    "\n",
    "    if keywords_for_cluster:\n",
    "        plot_cluster_keyword_trends_plotly(\n",
    "            df=df,\n",
    "            cluster_id=cluster_to_analyze,\n",
    "            all_cluster_keywords=keywords_for_cluster,\n",
    "            num_keywords_to_plot=10,\n",
    "            rolling_window=12\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Could not find top keywords for cluster {cluster_to_analyze}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_analyze = emerging_cluster_ids[0]\n",
    "\n",
    "plot_cluster_domain_trends_plotly(\n",
    "            df=df,\n",
    "            cluster_id=cluster_to_analyze,\n",
    "            domain_features=domain_features,\n",
    "            num_domains_to_plot=5, # Plotting top 5 is usually clear\n",
    "            rolling_window=12\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_area_trends_plotly(\n",
    "                df=df,\n",
    "                cluster_id=cluster_to_analyze,\n",
    "                area_features=area_features,\n",
    "                num_areas_to_plot=5,\n",
    "                rolling_window=12\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c761322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43faf79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
