{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17515d1",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72e445",
   "metadata": {},
   "source": [
    "## Imports and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21274fed-c6c7-4e7e-b413-837634163a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import time\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "from plotly.graph_objects import Figure\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efee45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('arxiv_cleaned.parquet', engine='pyarrow')\n",
    "category_df = pd.read_parquet('categories.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ed86a",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_categories = df['categories'].str.split().explode()\n",
    "# category_classes = []\n",
    "# subcategory_data = []\n",
    "\n",
    "# for category in all_categories:\n",
    "#     if '.' in category:\n",
    "#         # Split into category class and subcategory\n",
    "#         category_class, subcategory = category.split('.', 1)\n",
    "#     else:\n",
    "#         # If no dot, the entire category is the category class\n",
    "#         category_class, subcategory = category, None\n",
    "    \n",
    "#     category_classes.append(category_class)\n",
    "#     subcategory_data.append({'category_class': category_class, 'subcategory': subcategory})\n",
    "\n",
    "\n",
    "# subcategory_df = pd.DataFrame(subcategory_data)\n",
    "\n",
    "# #  Plot for category class counts\n",
    "# category_class_counts = pd.Series(category_classes).value_counts()\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x=category_class_counts.index, y=category_class_counts.values, palette=\"viridis\")\n",
    "# plt.title(\"Counts of Category Classes\")\n",
    "# plt.xlabel(\"Category Class\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()\n",
    "\n",
    "# # Plots for Subcategories\n",
    "# # Count the number of records per subcategory within each category class\n",
    "# subcategory_counts = subcategory_df.groupby(['category_class', 'subcategory']).size().reset_index(name='count')\n",
    "\n",
    "# # Create facet-wrapped bar plots\n",
    "# g = sns.FacetGrid(subcategory_counts, col=\"category_class\", col_wrap=4, sharex=False, sharey=False, height=4)\n",
    "# g.map(sns.barplot, \"subcategory\", \"count\", order=None, palette=\"viridis\")\n",
    "# g.set_titles(\"{col_name}\")\n",
    "# g.set_xticklabels(rotation=45)\n",
    "# g.set_axis_labels(\"Subcategory\", \"Count\")\n",
    "# plt.subplots_adjust(top=0.9)\n",
    "# g.fig.suptitle(\"Counts of Records per Subcategory\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly Category Plot\n",
    "# --- 1. Flatten Data: Extract Timestamps and Category Classes ---\n",
    "all_timestamps = []\n",
    "all_category_classes = []\n",
    "errors_parsing_versions = 0\n",
    "\n",
    "print(\"\\nStarting data flattening and extraction...\")\n",
    "# Use itertuples for performance\n",
    "for row in df.itertuples(index=False, name='Record'):\n",
    "    category_str = getattr(row, 'categories', None)\n",
    "    versions_data = getattr(row, 'versions', None)\n",
    "    versions_list = None # Reset for each row\n",
    "\n",
    "    # --- MODIFIED SECTION TO HANDLE NUMPY ARRAY ---\n",
    "    if isinstance(versions_data, str) and versions_data.startswith('[') and versions_data.endswith(']'):\n",
    "        # Safely parse 'versions' if it's a string representation of a list\n",
    "        try:\n",
    "            versions_list = ast.literal_eval(versions_data)\n",
    "            if not isinstance(versions_list, list):\n",
    "                 versions_list = None\n",
    "                 errors_parsing_versions += 1\n",
    "        except (ValueError, SyntaxError, MemoryError):\n",
    "            versions_list = None\n",
    "            errors_parsing_versions += 1\n",
    "    elif isinstance(versions_data, list):\n",
    "        # It's already a Python list\n",
    "        versions_list = versions_data\n",
    "    elif isinstance(versions_data, np.ndarray): # <--- ADDED CHECK FOR NUMPY ARRAY\n",
    "        # Convert NumPy array of objects (dicts) to a Python list\n",
    "        versions_list = versions_data.tolist()\n",
    "    # --- END MODIFIED SECTION ---\n",
    "\n",
    "\n",
    "    # Process if versions and categories are valid\n",
    "    if versions_list and isinstance(category_str, str) and category_str.strip():\n",
    "        categories = category_str.split()\n",
    "        for category in categories:\n",
    "            category = category.strip()\n",
    "            if not category: continue\n",
    "\n",
    "            # Extract main category class (before first '.')\n",
    "            category_class = category.split('.', 1)[0] if '.' in category else category\n",
    "\n",
    "            # Extract timestamps for this category class\n",
    "            for version_info in versions_list:\n",
    "                # Check if version_info is a dict (it should be now)\n",
    "                if isinstance(version_info, dict) and 'created' in version_info:\n",
    "                    timestamp_str = version_info.get('created')\n",
    "                    if isinstance(timestamp_str, str) and timestamp_str:\n",
    "                        all_timestamps.append(timestamp_str)\n",
    "                        all_category_classes.append(category_class)\n",
    "\n",
    "if errors_parsing_versions > 0:\n",
    "    print(f\"Note: Encountered {errors_parsing_versions} errors parsing 'versions' column strings during flattening.\")\n",
    "\n",
    "# --- Create the initial flattened DataFrame ---\n",
    "if not all_timestamps:\n",
    "    # This error should not happen now, but keep check just in case\n",
    "    print(\"\\nERROR: No valid version timestamps combined with category classes found after flattening.\")\n",
    "    exit()\n",
    "\n",
    "exploded_df = pd.DataFrame({\n",
    "    'category_class': all_category_classes,\n",
    "    'timestamp_str': all_timestamps\n",
    "})\n",
    "print(f\"Flattened DataFrame shape: {exploded_df.shape}\") # Should have rows now\n",
    "del all_timestamps, all_category_classes\n",
    "\n",
    "# --- The rest of the script (Steps 2-5) remains the same ---\n",
    "# (Paste the code for steps 2-5 from the cleaned-up version here)\n",
    "\n",
    "# --- 2. Convert Timestamps and Clean ---\n",
    "print(\"Converting timestamps and cleaning...\")\n",
    "exploded_df['timestamp'] = pd.to_datetime(exploded_df['timestamp_str'], errors='coerce')\n",
    "exploded_df.drop(columns=['timestamp_str'], inplace=True) # Drop the original string column\n",
    "exploded_df.dropna(subset=['timestamp'], inplace=True) # Remove rows where conversion failed\n",
    "\n",
    "# Store the processed flattened DataFrame\n",
    "exploded_df_processed = exploded_df\n",
    "print(f\"Processed flattened DataFrame shape: {exploded_df_processed.shape}\")\n",
    "\n",
    "if exploded_df_processed.empty:\n",
    "    print(\"ERROR: DataFrame is empty after timestamp conversion. Cannot proceed.\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Aggregate Counts per Time Period ---\n",
    "time_freq = 'Y' # Or 'Q', 'Y', 'W'\n",
    "print(f\"Aggregating counts by frequency: {time_freq}...\")\n",
    "# Set index temporarily for Grouper\n",
    "category_counts_over_time = exploded_df_processed.set_index('timestamp').groupby([\n",
    "    pd.Grouper(freq=time_freq),\n",
    "    'category_class'\n",
    "]).size()\n",
    "del exploded_df_processed # Free up memory\n",
    "\n",
    "# --- 4. Calculate Percentages ---\n",
    "print(\"Calculating percentages...\")\n",
    "total_counts_over_time = category_counts_over_time.groupby(level=0).sum()\n",
    "percentage_over_time = category_counts_over_time.div(total_counts_over_time, level=0, fill_value=0) * 100\n",
    "percentage_df = percentage_over_time.reset_index(name='percentage')\n",
    "del category_counts_over_time, total_counts_over_time # Free up memory\n",
    "\n",
    "# --- 5. Plot Top N Categories with Plotly ---\n",
    "print(\"Generating Plotly figure...\")\n",
    "if not percentage_df.empty:\n",
    "    top_n = 30 # Number of top categories to display\n",
    "    # Calculate average percentage per category to find the top ones\n",
    "    avg_percentage = percentage_df.groupby('category_class')['percentage'].mean()\n",
    "    top_categories = avg_percentage.nlargest(top_n).index.tolist()\n",
    "    # Filter the percentage DataFrame for plotting\n",
    "    plot_df = percentage_df[percentage_df['category_class'].isin(top_categories)]\n",
    "\n",
    "    if not plot_df.empty:\n",
    "        print(f\"Plotting top {len(top_categories)} categories...\")\n",
    "        fig = px.line(plot_df,\n",
    "                      x='timestamp',\n",
    "                      y='percentage',\n",
    "                      color='category_class',\n",
    "                      title=f'Top {len(top_categories)} Category Class Distribution Over Time (%)',\n",
    "                      labels={'timestamp': f'Time Period ({time_freq})', 'percentage': 'Percentage of Records (%)'},\n",
    "                      markers=False) # Set markers=True to show points\n",
    "\n",
    "        fig.update_layout(yaxis_tickformat='.1f%') # Format Y axis as percentage\n",
    "        fig.update_traces(hovertemplate='<b>%{fullData.name}</b><br>Period: %{x}<br>Percentage: %{y:.2f}%<extra></extra>')\n",
    "\n",
    "        fig.show() # Display the plot\n",
    "    else:\n",
    "        print(f\"Warning: No data remained after filtering for top {top_n} categories.\")\n",
    "else:\n",
    "    print(\"Warning: Percentage data was empty. No plot generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24696f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Flatten Data: Extract Timestamps and Category Classes ---\n",
    "all_timestamps = []\n",
    "all_category_classes = []\n",
    "errors_parsing_versions = 0\n",
    "\n",
    "# Prepare Category Lookup\n",
    "category_lookup = defaultdict(lambda: None)\n",
    "for _, row in category_df.iterrows():\n",
    "    area_val = str(row['Area']) if pd.notna(row['Area']) else None\n",
    "    subarea_val = str(row['SubArea']) if pd.notna(row['SubArea']) else None\n",
    "    category_lookup[row['Code']] = {\n",
    "        'Domain': row['Domain'],\n",
    "        'Area': area_val,\n",
    "        'SubArea': subarea_val\n",
    "    }\n",
    "\n",
    "print(\"\\nStarting data flattening and extraction...\")\n",
    "# Use itertuples for performance\n",
    "for row in df.itertuples(index=False, name='Record'):\n",
    "    category_str = getattr(row, 'categories', None)\n",
    "    versions_data = getattr(row, 'versions', None)\n",
    "    versions_list = None  # Reset for each row\n",
    "\n",
    "    # --- Modified Section to Handle NumPy Array ---\n",
    "    if isinstance(versions_data, str) and versions_data.startswith('[') and versions_data.endswith(']'):\n",
    "        # Safely parse 'versions' if it's a string representation of a list\n",
    "        try:\n",
    "            versions_list = ast.literal_eval(versions_data)\n",
    "            if not isinstance(versions_list, list):\n",
    "                versions_list = None\n",
    "                errors_parsing_versions += 1\n",
    "        except (ValueError, SyntaxError, MemoryError):\n",
    "            versions_list = None\n",
    "            errors_parsing_versions += 1\n",
    "    elif isinstance(versions_data, list):\n",
    "        # It's already a Python list\n",
    "        versions_list = versions_data\n",
    "    elif isinstance(versions_data, np.ndarray):  # <--- Added Check for NumPy Array\n",
    "        # Convert NumPy array of objects (dicts) to a Python list\n",
    "        versions_list = versions_data.tolist()\n",
    "    # --- End Modified Section ---\n",
    "\n",
    "    # Process if versions and categories are valid\n",
    "    if versions_list and isinstance(category_str, str) and category_str.strip():\n",
    "        categories = category_str.split()\n",
    "        for code in categories:\n",
    "            code = code.strip()\n",
    "            if not code:\n",
    "                continue\n",
    "\n",
    "            # Map category code to hierarchical name (e.g., Area or SubArea)\n",
    "            cat_info = category_lookup[code]\n",
    "            if cat_info:\n",
    "                # Use Area or SubArea as the category class name\n",
    "                category_class = cat_info.get('Area') or cat_info.get('SubArea') or code\n",
    "            else:\n",
    "                # Fallback to the code itself if no mapping is found\n",
    "                category_class = code\n",
    "\n",
    "            # Extract timestamps for this category class\n",
    "            for version_info in versions_list:\n",
    "                # Check if version_info is a dict (it should be now)\n",
    "                if isinstance(version_info, dict) and 'created' in version_info:\n",
    "                    timestamp_str = version_info.get('created')\n",
    "                    if isinstance(timestamp_str, str) and timestamp_str:\n",
    "                        all_timestamps.append(timestamp_str)\n",
    "                        all_category_classes.append(category_class)\n",
    "\n",
    "if errors_parsing_versions > 0:\n",
    "    print(f\"Note: Encountered {errors_parsing_versions} errors parsing 'versions' column strings during flattening.\")\n",
    "\n",
    "# --- Create the initial flattened DataFrame ---\n",
    "if not all_timestamps:\n",
    "    # This error should not happen now, but keep check just in case\n",
    "    print(\"\\nERROR: No valid version timestamps combined with category classes found after flattening.\")\n",
    "    exit()\n",
    "\n",
    "exploded_df = pd.DataFrame({\n",
    "    'category_class': all_category_classes,\n",
    "    'timestamp_str': all_timestamps\n",
    "})\n",
    "print(f\"Flattened DataFrame shape: {exploded_df.shape}\")  # Should have rows now\n",
    "del all_timestamps, all_category_classes\n",
    "\n",
    "# --- 2. Convert Timestamps and Clean ---\n",
    "print(\"Converting timestamps and cleaning...\")\n",
    "exploded_df['timestamp'] = pd.to_datetime(exploded_df['timestamp_str'], errors='coerce')\n",
    "exploded_df.drop(columns=['timestamp_str'], inplace=True)  # Drop the original string column\n",
    "exploded_df.dropna(subset=['timestamp'], inplace=True)  # Remove rows where conversion failed\n",
    "\n",
    "# Store the processed flattened DataFrame\n",
    "exploded_df_processed = exploded_df\n",
    "print(f\"Processed flattened DataFrame shape: {exploded_df_processed.shape}\")\n",
    "\n",
    "if exploded_df_processed.empty:\n",
    "    print(\"ERROR: DataFrame is empty after timestamp conversion. Cannot proceed.\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Aggregate Counts per Time Period ---\n",
    "time_freq = 'Y'  # Or 'Q', 'Y', 'W'\n",
    "print(f\"Aggregating counts by frequency: {time_freq}...\")\n",
    "# Set index temporarily for Grouper\n",
    "category_counts_over_time = exploded_df_processed.set_index('timestamp').groupby([\n",
    "    pd.Grouper(freq=time_freq),\n",
    "    'category_class'\n",
    "]).size()\n",
    "del exploded_df_processed  # Free up memory\n",
    "\n",
    "# --- 4. Calculate Percentages ---\n",
    "print(\"Calculating percentages...\")\n",
    "total_counts_over_time = category_counts_over_time.groupby(level=0).sum()\n",
    "percentage_over_time = category_counts_over_time.div(total_counts_over_time, level=0, fill_value=0) * 100\n",
    "percentage_df = percentage_over_time.reset_index(name='percentage')\n",
    "del category_counts_over_time, total_counts_over_time  # Free up memory\n",
    "\n",
    "# --- 5. Plot Top N Categories with Plotly ---\n",
    "print(\"Generating Plotly figure...\")\n",
    "if not percentage_df.empty:\n",
    "    top_n = 30  # Number of top categories to display\n",
    "    # Calculate average percentage per category to find the top ones\n",
    "    avg_percentage = percentage_df.groupby('category_class')['percentage'].mean()\n",
    "    top_categories = avg_percentage.nlargest(top_n).index.tolist()\n",
    "    # Filter the percentage DataFrame for plotting\n",
    "    plot_df = percentage_df[percentage_df['category_class'].isin(top_categories)]\n",
    "\n",
    "    if not plot_df.empty:\n",
    "        print(f\"Plotting top {len(top_categories)} categories...\")\n",
    "        fig = px.line(plot_df,\n",
    "                      x='timestamp',\n",
    "                      y='percentage',\n",
    "                      color='category_class',\n",
    "                      title=f'Top {len(top_categories)} Category Class Distribution Over Time (%)',\n",
    "                      labels={'timestamp': f'Time Period ({time_freq})', 'percentage': 'Percentage of Records (%)'},\n",
    "                      markers=False)  # Set markers=True to show points\n",
    "\n",
    "        fig.update_layout(yaxis_tickformat='.1f%')  # Format Y axis as percentage\n",
    "        fig.update_traces(hovertemplate='<b>%{fullData.name}</b><br>Period: %{x}<br>Percentage: %{y:.2f}%<extra></extra>')\n",
    "\n",
    "        fig.show()  # Display the plot\n",
    "    else:\n",
    "        print(f\"Warning: No data remained after filtering for top {top_n} categories.\")\n",
    "else:\n",
    "    print(\"Warning: Percentage data was empty. No plot generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b802d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category_distribution(df, category_df,\n",
    "                               level_to_plot,\n",
    "                               filter_domain,\n",
    "                               filter_area=None,\n",
    "                               time_freq='Y',\n",
    "                               top_n=10,\n",
    "                               start_year: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Generates a Plotly time-series line chart showing the percentage distribution\n",
    "    of specified category levels within a domain/area over time.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'categories' and 'versions' columns.\n",
    "        category_df (pd.DataFrame): DataFrame with category hierarchy\n",
    "                                     (columns: Domain, Area, SubArea, Code, Description).\n",
    "        level_to_plot (str): The hierarchical level to plot lines for ('Area' or 'SubArea').\n",
    "        filter_domain (str): The top-level Domain to filter data by (e.g., \"Physics\", \"Computer Science\").\n",
    "        filter_area (str, optional): If level_to_plot is 'SubArea', specify the Area within\n",
    "                                     the Domain to filter by (e.g., \"Astrophysics\"). Defaults to None.\n",
    "        time_freq (str, optional): Time frequency for aggregation ('M', 'Q', 'Y', 'W'). Defaults to 'M'.\n",
    "        top_n (int, optional): Number of top categories at the specified level to display. Defaults to 10.\n",
    "        start_year (int, optional): The earliest year to include in the plot (inclusive).\n",
    "                                    Defaults to None (no start year filter).\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The generated Plotly figure object, or None if plotting is not possible.\n",
    "    \"\"\"\n",
    "\n",
    "    grouping_col = level_to_plot.lower() # 'area' or 'subarea'\n",
    "\n",
    "    # --- 1. Prepare Category Lookup ---\n",
    "    category_lookup = defaultdict(lambda: None)\n",
    "    for _, row in category_df.iterrows():\n",
    "        area_val = str(row['Area']) if pd.notna(row['Area']) else None\n",
    "        subarea_val = str(row['SubArea']) if pd.notna(row['SubArea']) else None\n",
    "        category_lookup[row['Code']] = {\n",
    "            'Domain': row['Domain'],\n",
    "            'Area': area_val,\n",
    "            'SubArea': subarea_val\n",
    "        }\n",
    "\n",
    "    # --- 2. Flatten Data & Filter ---\n",
    "    all_timestamps = []\n",
    "    grouping_values = [] # 'Area' or 'SubArea' names/codes\n",
    "    errors_parsing_versions = 0\n",
    "\n",
    "    for row in df.itertuples(index=False, name='Record'):\n",
    "        category_str = getattr(row, 'categories', None)\n",
    "        versions_data = getattr(row, 'versions', None)\n",
    "        versions_list = None\n",
    "\n",
    "        # Safely parse/convert 'versions'\n",
    "        if isinstance(versions_data, str) and versions_data.startswith('[') and versions_data.endswith(']'):\n",
    "            try:\n",
    "                versions_list = ast.literal_eval(versions_data)\n",
    "                if not isinstance(versions_list, list): versions_list = None; errors_parsing_versions += 1\n",
    "            except: versions_list = None; errors_parsing_versions += 1\n",
    "        elif isinstance(versions_data, list): versions_list = versions_data\n",
    "        elif isinstance(versions_data, np.ndarray): versions_list = versions_data.tolist()\n",
    "\n",
    "        if versions_list and isinstance(category_str, str) and category_str.strip():\n",
    "            categories = category_str.split()\n",
    "            for code in categories:\n",
    "                code = code.strip()\n",
    "                if not code: continue\n",
    "                cat_info = category_lookup[code]\n",
    "\n",
    "                # Filtering Logic\n",
    "                if cat_info and cat_info['Domain'] == filter_domain:\n",
    "                    # Apply Area filter only if specified\n",
    "                    if filter_area and cat_info['Area'] != filter_area: continue\n",
    "\n",
    "                    # Get the value for the level we are plotting (Area or SubArea)\n",
    "                    # Use the code itself as a fallback if the Area/SubArea name is missing in category_df\n",
    "                    value_to_append = cat_info.get(level_to_plot)\n",
    "                    if value_to_append is None or pd.isna(value_to_append):\n",
    "                        value_to_append = code # Fallback to code\n",
    "\n",
    "                    # Extract timestamps for matching categories\n",
    "                    for version_info in versions_list:\n",
    "                        if isinstance(version_info, dict) and 'created' in version_info:\n",
    "                            timestamp_str = version_info.get('created')\n",
    "                            if isinstance(timestamp_str, str) and timestamp_str:\n",
    "                                all_timestamps.append(timestamp_str)\n",
    "                                grouping_values.append(value_to_append)\n",
    "\n",
    "    # --- 3. Create Flattened DataFrame & Convert Timestamps ---\n",
    "    exploded_df = pd.DataFrame({\n",
    "        grouping_col: grouping_values,\n",
    "        'timestamp_str': all_timestamps\n",
    "    })\n",
    "    del all_timestamps, grouping_values # Free memory\n",
    "\n",
    "    # Transform timestamps to datetime\n",
    "    exploded_df['timestamp'] = pd.to_datetime(exploded_df['timestamp_str'], errors='coerce', utc=True)\n",
    "\n",
    "    exploded_df.drop(columns=['timestamp_str'], inplace=True)\n",
    "    print(f\"Shape before dropping NaT timestamps: {exploded_df.shape}\")\n",
    "    initial_rows = len(exploded_df)\n",
    "    exploded_df.dropna(subset=['timestamp'], inplace=True) # Remove rows where conversion failed\n",
    "    dropped_rows = initial_rows - len(exploded_df)\n",
    "    if dropped_rows > 0:\n",
    "        print(f\"Dropped {dropped_rows} rows due to NaT timestamps.\")\n",
    "    print(f\"Shape AFTER dropping NaT timestamps: {exploded_df.shape}\") # Check if empty\n",
    "\n",
    "    exploded_df_processed = exploded_df # Assign before checking if empty\n",
    "\n",
    "    # --- Apply Start Year Filter (if specified) ---\n",
    "    if start_year is not None:\n",
    "        exploded_df_processed = exploded_df_processed[exploded_df_processed['timestamp'].dt.year >= start_year]\n",
    "\n",
    "    # --- 4. Aggregate & Calculate Percentages ---\n",
    "    counts_over_time = exploded_df_processed.set_index('timestamp').groupby([\n",
    "        pd.Grouper(freq=time_freq),\n",
    "        grouping_col\n",
    "    ]).size()\n",
    "    del exploded_df_processed # Free memory\n",
    "\n",
    "    # Total counts *within the filtered scope* for each time period\n",
    "    total_counts_over_time = counts_over_time.groupby(level=0).sum()\n",
    "    # Handle potential division by zero if a period has zero total counts\n",
    "    percentage_over_time = counts_over_time.div(total_counts_over_time.replace(0, np.nan), level=0).fillna(0) * 100\n",
    "    percentage_df = percentage_over_time.reset_index(name='percentage')\n",
    "    del counts_over_time, total_counts_over_time # Free memory\n",
    "\n",
    "    # --- 5. Plot Top N with Plotly ---\n",
    "    if not percentage_df.empty:\n",
    "        # Calculate average percentage to find the top N\n",
    "        avg_percentage = percentage_df.groupby(grouping_col)['percentage'].mean()\n",
    "        top_categories = avg_percentage.nlargest(top_n).index.tolist()\n",
    "        plot_df = percentage_df[percentage_df[grouping_col].isin(top_categories)]\n",
    "\n",
    "        if not plot_df.empty:\n",
    "            # Dynamically set title and labels\n",
    "            plot_title = f'Top {len(top_categories)} {level_to_plot} Distribution within {filter_domain}'\n",
    "            if filter_area:\n",
    "                plot_title += f' ({filter_area})'\n",
    "            # --- Added start_year to title ---\n",
    "            if start_year is not None:\n",
    "                 plot_title += f' (from {start_year})'\n",
    "            plot_title += ' Over Time'\n",
    "\n",
    "\n",
    "            y_label = f'Percentage within {filter_domain}{f\" ({filter_area})\" if filter_area else \"\"} [%]'\n",
    "            color_label = level_to_plot # Label for the legend\n",
    "\n",
    "            print(f\"Plotting top {len(top_categories)} categories...\")\n",
    "            fig = px.line(plot_df,\n",
    "                          x='timestamp',\n",
    "                          y='percentage',\n",
    "                          color=grouping_col, # Dynamic color based on level\n",
    "                          title=plot_title,\n",
    "                          labels={'timestamp': 'Time Period',\n",
    "                                  'percentage': y_label,\n",
    "                                  grouping_col: color_label}, # Dynamic legend title\n",
    "                          markers=False)\n",
    "\n",
    "            fig.update_layout(yaxis_tickformat='.1f%')\n",
    "            fig.update_traces(hovertemplate=f'<b>%{{fullData.name}}</b><br>Period: %{{x|%Y-%m-%d}}<br>Percentage: %{{y:.2f}}%<extra></extra>') # Format date in hover\n",
    "            return fig # Return the figure object\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad511fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='Area', filter_domain='Computer Science', time_freq='Y', top_n=5, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='Area', filter_domain='Quantitative Biology', time_freq='Y', top_n=5, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='Area', filter_domain='Economics', time_freq='Y', top_n=5, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='Area', filter_domain='Electrical Engineering and Systems Science', time_freq='Y', top_n=5, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='Area', filter_domain='Mathematics', time_freq='Y', top_n=10, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471eae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='Area', filter_domain='Physics', time_freq='Y', top_n=5, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='Area', filter_domain='Statistics', time_freq='Y', top_n=5, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f39743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='SubArea', filter_domain='Physics', filter_area='Astrophysics', time_freq='Y', top_n=5, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df, category_df, level_to_plot='SubArea', filter_domain='Physics', filter_area='Physics', time_freq='Y', top_n=5, start_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_filtered_data(\n",
    "    df: pd.DataFrame,\n",
    "    category_df: pd.DataFrame,\n",
    "    start_date_str: str = \"2020-01-01\",\n",
    "    end_date_str: str = \"2025-12-31\"\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Internal helper function to flatten data, convert timestamps, and filter\n",
    "    by a specific date range.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Original DataFrame with 'categories', 'versions'.\n",
    "        category_df (pd.DataFrame): DataFrame with category hierarchy.\n",
    "        start_date_str (str): Start date string (YYYY-MM-DD).\n",
    "        end_date_str (str): End date string (YYYY-MM-DD).\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: Filtered DataFrame with 'timestamp', 'domain',\n",
    "                                 'area', 'subarea' columns, or None if processing fails.\n",
    "    \"\"\"\n",
    "    start_time_prep = time.time()\n",
    "    print(f\"--- Starting Data Preparation ({start_date_str} to {end_date_str}) ---\")\n",
    "\n",
    "    start_date = pd.Timestamp(start_date_str, tz='UTC')\n",
    "    end_date = pd.Timestamp(end_date_str, tz='UTC')\n",
    "\n",
    "    # --- 1. Prepare Category Lookup ---\n",
    "    category_lookup = defaultdict(lambda: None)\n",
    "    for _, row in category_df.iterrows():\n",
    "        area_val = str(row['Area']) if pd.notna(row['Area']) else None\n",
    "        subarea_val = str(row['SubArea']) if pd.notna(row['SubArea']) else None\n",
    "        category_lookup[row['Code']] = {\n",
    "            'Domain': row['Domain'],\n",
    "            'Area': area_val,\n",
    "            'SubArea': subarea_val,\n",
    "            'Code': row['Code']\n",
    "        }\n",
    "    # print(\"Category lookup created.\") # Less verbose for helper\n",
    "\n",
    "    # --- 2. Flatten Data ---\n",
    "    all_data = []\n",
    "    errors_parsing_versions = 0\n",
    "    # print(\"Starting data flattening...\") # Less verbose\n",
    "\n",
    "    for row in df.itertuples(index=False, name='Record'):\n",
    "        category_str = getattr(row, 'categories', None)\n",
    "        versions_data = getattr(row, 'versions', None)\n",
    "        versions_list = None\n",
    "\n",
    "        # Safely parse/convert 'versions'\n",
    "        if isinstance(versions_data, str) and versions_data.startswith('[') and versions_data.endswith(']'):\n",
    "            try:\n",
    "                versions_list = ast.literal_eval(versions_data)\n",
    "                if not isinstance(versions_list, list): versions_list = None; errors_parsing_versions += 1\n",
    "            except: versions_list = None; errors_parsing_versions += 1\n",
    "        elif isinstance(versions_data, list): versions_list = versions_data\n",
    "        elif isinstance(versions_data, np.ndarray): versions_list = versions_data.tolist()\n",
    "\n",
    "        if versions_list and isinstance(category_str, str) and category_str.strip():\n",
    "            categories = category_str.split()\n",
    "            for code in categories:\n",
    "                code = code.strip()\n",
    "                if not code: continue\n",
    "                cat_info = category_lookup[code]\n",
    "\n",
    "                if cat_info:\n",
    "                    domain = cat_info['Domain']\n",
    "                    area = cat_info['Area'] if cat_info['Area'] else f\"Code:{cat_info['Code']}\"\n",
    "                    subarea = cat_info['SubArea'] if cat_info['SubArea'] else f\"Code:{cat_info['Code']}\"\n",
    "\n",
    "                    for version_info in versions_list:\n",
    "                        if isinstance(version_info, dict) and 'created' in version_info:\n",
    "                            timestamp_str = version_info.get('created')\n",
    "                            if isinstance(timestamp_str, str) and timestamp_str:\n",
    "                                all_data.append({\n",
    "                                    'timestamp_str': timestamp_str,\n",
    "                                    'domain': domain,\n",
    "                                    'area': area,\n",
    "                                    'subarea': subarea\n",
    "                                })\n",
    "\n",
    "    if errors_parsing_versions > 0: print(f\"Note: Encountered {errors_parsing_versions} errors parsing 'versions'.\")\n",
    "    if not all_data: print(f\"ERROR: No category/version data found during flattening.\"); return None\n",
    "\n",
    "    # --- 3. Create DataFrame & Convert/Filter Timestamps ---\n",
    "    flat_df = pd.DataFrame(all_data)\n",
    "    print(f\"Flattened DataFrame shape: {flat_df.shape}\")\n",
    "    # del all_data\n",
    "\n",
    "    # print(\"Converting timestamps...\")\n",
    "    flat_df['timestamp'] = pd.to_datetime(flat_df['timestamp_str'], errors='coerce', utc=True)\n",
    "\n",
    "    failed_timestamps_mask = flat_df['timestamp'].isna()\n",
    "    num_failed = failed_timestamps_mask.sum()\n",
    "    if num_failed > 0: print(f\"Warning: {num_failed} timestamps failed conversion.\")\n",
    "\n",
    "    flat_df.drop(columns=['timestamp_str'], inplace=True)\n",
    "    initial_rows = len(flat_df)\n",
    "    flat_df.dropna(subset=['timestamp'], inplace=True)\n",
    "    dropped_rows = initial_rows - len(flat_df)\n",
    "    if dropped_rows > 0: print(f\"Dropped {dropped_rows} rows due to NaT timestamps.\") \n",
    "\n",
    "    if flat_df.empty:\n",
    "        print(\"ERROR: DataFrame is empty after timestamp conversion. Cannot proceed.\")\n",
    "        return None\n",
    "\n",
    "    # --- Apply Date Range Filter ---\n",
    "    # print(f\"Filtering for dates between {start_date.date()} and {end_date.date()}...\") \n",
    "    initial_rows_before_date_filter = len(flat_df)\n",
    "    flat_df = flat_df[(flat_df['timestamp'] >= start_date) & (flat_df['timestamp'] <= end_date)]\n",
    "    rows_after_date_filter = len(flat_df)\n",
    "    removed_rows = initial_rows_before_date_filter - rows_after_date_filter\n",
    "    if removed_rows > 0: print(f\"Removed {removed_rows} rows outside the date range.\") \n",
    "\n",
    "    if flat_df.empty:\n",
    "        print(f\"ERROR: DataFrame is empty after applying date filter ({start_date.date()} to {end_date.date()}). Cannot proceed.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Data preparation complete. Filtered data shape: {flat_df.shape}. Time: {time.time() - start_time_prep:.2f}s\")\n",
    "    return all_data, flat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, flat_df = _prepare_filtered_data(df, category_df, \"2015-01-01\", \"2025-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_10_domains(\n",
    "    flat_df: pd.DataFrame\n",
    ") -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Generates a bar chart showing the overall Top 10 Domains\n",
    "    based on total counts between Jan 1, 2020, and Dec 31, 2025.\n",
    "\n",
    "    Args:\n",
    "        flat_df (pd.DataFrame): DataFrame containing at least a 'domain' column.\n",
    "                                Assumed to represent data within the target date range.\n",
    "\n",
    "    Returns:\n",
    "        Optional[matplotlib.figure.Figure]: The generated Matplotlib figure object,\n",
    "                                             or None if plotting is not possible (e.g., empty data).\n",
    "    \"\"\"\n",
    "\n",
    "    # Aggregate counts per Domain\n",
    "    domain_counts = flat_df['domain'].value_counts().reset_index()\n",
    "    domain_counts.columns = ['domain', 'count']\n",
    "\n",
    "    # Get the top 10 domains\n",
    "    top_10_domains = domain_counts.nlargest(10, 'count')\n",
    "\n",
    "    if top_10_domains.empty:\n",
    "        print(\"No domain data found after aggregation. Cannot generate plot.\")\n",
    "        return None\n",
    "\n",
    "    num_domains = len(top_10_domains)\n",
    "\n",
    "    # Create the plot using seaborn.barplot\n",
    "    plt.figure(figsize=(12, 7))  # Adjust figure size for better label readability\n",
    "    ax = sns.barplot(\n",
    "        data=top_10_domains,\n",
    "        x='domain',\n",
    "        y='count',\n",
    "        palette=\"colorblind\",\n",
    "        order=top_10_domains['domain'].tolist()  # Ensure bars are ordered by count descending\n",
    "    )\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title(f'Overall Top {num_domains} Domains by Count (2020-2025)', fontsize=16, pad=20)\n",
    "    ax.set_xlabel(\"Domain\", fontsize=12)\n",
    "    ax.set_ylabel(\"Total Count\", fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=10)  # Rotate labels for better fit\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Add line breaks to x-axis labels\n",
    "    # Define the maximum width of a label line\n",
    "    max_label_width = 20  # Adjust this value as needed for desired wrapping\n",
    "    # Get current labels, wrap them, and set them back\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    wrapped_labels = [textwrap.fill(label, width=max_label_width) for label in labels]\n",
    "    ax.set_xticklabels(wrapped_labels)\n",
    "    # Adjust rotation mode for potentially multi-line rotated labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation_mode=\"anchor\", ha=\"right\")\n",
    "\n",
    "    # Add count labels to bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%d', label_type='edge', fontsize=9, padding=3)\n",
    "\n",
    "    # Adjust y-axis limits for padding above labels\n",
    "    ax.margins(y=0.1)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "\n",
    "    # Return the figure object associated with the axes\n",
    "    fig = ax.get_figure()\n",
    "    return fig\n",
    "\n",
    "fig_top10 = plot_top_10_domains(flat_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_10_areas(\n",
    "    flat_df: pd.DataFrame\n",
    ") -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Generates a bar chart showing the overall Top 10 Areas\n",
    "    based on total counts between Jan 1, 2020, and Dec 31, 2025.\n",
    "\n",
    "    Args:\n",
    "        flat_df (pd.DataFrame): DataFrame containing at least an 'subarea' column.\n",
    "                                Assumed to represent data within the target date range.\n",
    "\n",
    "    Returns:\n",
    "        Optional[matplotlib.figure.Figure]: The generated Matplotlib figure object,\n",
    "                                             or None if plotting is not possible (e.g., empty data).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Aggregate counts per Area\n",
    "    area_counts = flat_df['subarea'].value_counts().reset_index()\n",
    "    area_counts.columns = ['subarea', 'count']\n",
    "\n",
    "    # Get the top 10 areas\n",
    "    top_10_areas = area_counts.nlargest(10, 'count')\n",
    "\n",
    "    if top_10_areas.empty:\n",
    "        print(\"No area data found after aggregation. Cannot generate plot.\")\n",
    "        return None\n",
    "\n",
    "    num_areas = len(top_10_areas)\n",
    "\n",
    "    # Create the plot using seaborn.barplot [[1]] [[5]]\n",
    "    plt.figure(figsize=(12, 7)) # Adjust figure size for better label readability\n",
    "    ax = sns.barplot(\n",
    "        data=top_10_areas,\n",
    "        x='subarea',\n",
    "        y='count',\n",
    "        palette=\"colorblind\",\n",
    "        order=top_10_areas['subarea'].tolist() # Ensure bars are ordered by count descending\n",
    "    )\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title(f'Overall Top {num_areas} Areas by Count (2020-2025)', fontsize=16, pad=20)\n",
    "    ax.set_xlabel(\"Area\", fontsize=12)\n",
    "    ax.set_ylabel(\"Total Count\", fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=10) # Rotate labels for better fit [[6]]\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Add line breaks to x-axis labels\n",
    "    # Define the maximum width of a label line\n",
    "    max_label_width = 20 # Adjust this value as needed for desired wrapping\n",
    "    # Get current labels, wrap them, and set them back\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    wrapped_labels = [textwrap.fill(label, width=max_label_width) for label in labels]\n",
    "    ax.set_xticklabels(wrapped_labels)\n",
    "    # Adjust rotation mode for potentially multi-line rotated labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation_mode=\"anchor\", ha=\"right\")\n",
    "\n",
    "    # Add count labels to bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%d', label_type='edge', fontsize=9, padding=3)\n",
    "\n",
    "    # Adjust y-axis limits for padding above labels\n",
    "    ax.margins(y=0.1)\n",
    "\n",
    "    plt.tight_layout() # Adjust layout\n",
    "\n",
    "    # Return the figure object associated with the axes\n",
    "    fig = ax.get_figure()\n",
    "    return fig\n",
    "\n",
    "fig_top10 = plot_top_10_areas(flat_df)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3db1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_10_areas_stacked_by_domain(\n",
    "    flat_df: pd.DataFrame\n",
    ") -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Generates a static stacked bar chart showing the overall Top 10 Areas\n",
    "    based on total counts between Jan 1, 2020, and Dec 31, 2025,\n",
    "    with bars stacked by the 'Domain' column.\n",
    "\n",
    "    Args:\n",
    "        flat_df (pd.DataFrame): DataFrame containing at least 'area' and 'Domain' columns.\n",
    "                                Assumed to represent data within the target date range.\n",
    "\n",
    "    Returns:\n",
    "        Optional[matplotlib.figure.Figure]: The generated Matplotlib figure object,\n",
    "                                             or None if plotting is not possible.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "\n",
    "    # 1. Calculate total counts per Area to find the top 10\n",
    "    total_area_counts = flat_df['subarea'].value_counts()\n",
    "    top_10_area_names = total_area_counts.nlargest(10).index.tolist()\n",
    "\n",
    "    if not top_10_area_names:\n",
    "        print(\"No area data found after aggregation. Cannot generate plot.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Filter the original DataFrame to include only data for the top 10 areas\n",
    "    top_10_df = flat_df[flat_df['subarea'].isin(top_10_area_names)]\n",
    "\n",
    "    # 3. Aggregate counts grouped by Area and Domain for the filtered data\n",
    "    # Use size() and unstack() to pivot Domains into columns [[9]]\n",
    "    stacked_data = top_10_df.groupby(['subarea', 'domain']).size().unstack(fill_value=0)\n",
    "\n",
    "    # 4. Reorder the stacked_data index based on the total counts (descending)\n",
    "    # This ensures the bars are plotted in the order of total magnitude\n",
    "    stacked_data = stacked_data.loc[top_10_area_names]\n",
    "\n",
    "    num_areas = len(stacked_data)\n",
    "    print(f\"Plotting Top {num_areas} Areas overall, stacked by Domain...\")\n",
    "\n",
    "    # --- Plotting ---\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 8)) \n",
    "    stacked_data.plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        colormap='tab20b'\n",
    "    )\n",
    "\n",
    "    # --- Customization ---\n",
    "    ax.set_title(f'Overall Top {num_areas} Areas by Count (2020-2025), Stacked by Domain', fontsize=16, pad=20)\n",
    "    ax.set_xlabel(\"Area\", fontsize=12)\n",
    "    ax.set_ylabel(\"Total Count\", fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=10) # Rotate labels\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Add line breaks to x-axis labels\n",
    "    max_label_width = 20\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    wrapped_labels = [textwrap.fill(label, width=max_label_width) for label in labels]\n",
    "    ax.set_xticklabels(wrapped_labels)\n",
    "    plt.setp(ax.get_xticklabels(), rotation_mode=\"anchor\", ha=\"right\") # Adjust rotation for wrapped labels\n",
    "\n",
    "    # Add total count labels on top of the stacked bars\n",
    "    # Calculate total height for each bar\n",
    "    totals = stacked_data.sum(axis=1)\n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(i, total + (ax.get_ylim()[1] * 0.01), f'{total}', ha='center', va='bottom', fontsize=9) # Adjust position slightly above bar\n",
    "\n",
    "    # Adjust y-axis limits for padding above labels\n",
    "    ax.margins(y=0.1) # Increase top margin\n",
    "\n",
    "    # Add Legend\n",
    "    ax.legend(title='Domain', bbox_to_anchor=(1.02, 1), loc='upper left') # Place legend outside plot area\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust layout to make space for legend\n",
    "\n",
    "    return fig\n",
    "\n",
    "# --- Execution ---\n",
    "fig_top10_stacked = plot_top_10_areas_stacked_by_domain(flat_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d302aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flop_10_areas_stacked_by_domain(\n",
    "    flat_df: pd.DataFrame\n",
    ") -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Generates a static stacked bar chart showing the overall Flop 10 Areas\n",
    "    based on total counts between Jan 1, 2020, and Dec 31, 2025,\n",
    "    with bars stacked by the 'Domain' column.\n",
    "\n",
    "    Args:\n",
    "        flat_df (pd.DataFrame): DataFrame containing at least 'area' and 'Domain' columns.\n",
    "                                Assumed to represent data within the target date range.\n",
    "\n",
    "    Returns:\n",
    "        Optional[matplotlib.figure.Figure]: The generated Matplotlib figure object,\n",
    "                                             or None if plotting is not possible.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "\n",
    "    # 1. Calculate total counts per Area to find the \"flop\" 10\n",
    "    total_area_counts = flat_df['subarea'].value_counts()\n",
    "    flop_10_area_names = total_area_counts.nsmallest(10).index.tolist()\n",
    "\n",
    "    flop_10_df = flat_df[flat_df['subarea'].isin(flop_10_area_names)]\n",
    "\n",
    "    # 3. Aggregate counts grouped by Area and Domain for the filtered data\n",
    "    # Use size() and unstack() to pivot Domains into columns\n",
    "    stacked_data = flop_10_df.groupby(['subarea', 'domain']).size().unstack(fill_value=0)\n",
    "\n",
    "    # 4. Reorder the stacked_data index based on the total counts (descending)\n",
    "    # This ensures the bars are plotted in the order of total magnitude\n",
    "    stacked_data = stacked_data.loc[flop_10_area_names]\n",
    "\n",
    "    num_areas = len(stacked_data)\n",
    "    print(f\"Plotting Top {num_areas} Areas overall, stacked by Domain...\")\n",
    "\n",
    "    # --- Plotting ---\n",
    "\n",
    "    # Create the plot using pandas plotting\n",
    "    fig, ax = plt.subplots(figsize=(14, 8)) # Adjust figure size\n",
    "    stacked_data.plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        colormap='tab20b'\n",
    "    )\n",
    "\n",
    "    # --- Customization ---\n",
    "    ax.set_title(f'Overall Least Represented {num_areas} Areas by Count (2020-2025), Stacked by Domain', fontsize=16, pad=20)\n",
    "    ax.set_xlabel(\"Area\", fontsize=12)\n",
    "    ax.set_ylabel(\"Total Count\", fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=10) # Rotate labels\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Add line breaks to x-axis labels\n",
    "    max_label_width = 20\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    wrapped_labels = [textwrap.fill(label, width=max_label_width) for label in labels]\n",
    "    ax.set_xticklabels(wrapped_labels)\n",
    "    plt.setp(ax.get_xticklabels(), rotation_mode=\"anchor\", ha=\"right\") # Adjust rotation for wrapped labels\n",
    "\n",
    "    # Add total count labels on top of the stacked bars\n",
    "    # Calculate total height for each bar\n",
    "    totals = stacked_data.sum(axis=1)\n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(i, total + (ax.get_ylim()[1] * 0.01), f'{total}', ha='center', va='bottom', fontsize=9) # Adjust position slightly above bar\n",
    "\n",
    "    # Adjust y-axis limits for padding above labels\n",
    "    ax.margins(y=0.1) # Increase top margin\n",
    "\n",
    "    # Add Legend\n",
    "    ax.legend(title='Domain', bbox_to_anchor=(1.02, 1), loc='upper left') # Place legend outside plot area\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust layout to make space for legend\n",
    "\n",
    "    return fig\n",
    "\n",
    "# --- Execution ---\n",
    "fig_flop10_stacked = plot_flop_10_areas_stacked_by_domain(flat_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ddc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_physics_subareas_from_physics_area(\n",
    "    flat_df: pd.DataFrame, # Corrected type hint based on usage\n",
    ") -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Generates a static bar chart showing the Top 10 SubAreas within the 'Physics'\n",
    "    Area based on total counts between Jan 1, 2020, and Dec 31, 2025.\n",
    "    Adds line breaks to long subarea names on the x-axis.\n",
    "\n",
    "    Args:\n",
    "        flat_df (pd.DataFrame): DataFrame containing at least 'area', 'subarea' columns.\n",
    "                                 Expected to have one row per item to count.\n",
    "\n",
    "    Returns:\n",
    "        Optional[matplotlib.figure.Figure]: The generated Matplotlib figure object,\n",
    "                                             or None if plotting is not possible (e.g., no data).\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for Physics area\n",
    "    physics_df = flat_df[flat_df['area'] == 'Physics'].copy()\n",
    "\n",
    "    if physics_df.empty:\n",
    "        print(\"No data found for the 'Physics' area.\")\n",
    "        return None\n",
    "\n",
    "    # Aggregate counts per SubArea within Physics\n",
    "    subarea_counts = physics_df.groupby('subarea', observed=False).size().reset_index(name='count')\n",
    "\n",
    "    # Find the top 10 subareas\n",
    "    top_10_physics_subareas = subarea_counts.nlargest(10, 'count')\n",
    "\n",
    "    if top_10_physics_subareas.empty:\n",
    "        print(\"No subareas found within the 'Physics' area after filtering.\")\n",
    "        return None\n",
    "\n",
    "    num_subareas = len(top_10_physics_subareas)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 7)) # Consider adjusting figsize if labels overlap\n",
    "\n",
    "    sns.barplot(\n",
    "        data=top_10_physics_subareas,\n",
    "        x='subarea',\n",
    "        y='count',\n",
    "        ax=ax,\n",
    "        palette='colorblind'\n",
    "        # order=top_10_physics_subareas['subarea'] # Order is implicitly handled by nlargest\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'Top {num_subareas} Physics Areas by Paper Count (2020-2025)', fontsize=16)\n",
    "    ax.set_xlabel(\"Physics Area\", fontsize=12)\n",
    "    ax.set_ylabel(\"Total Count\", fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=8) # Keep rotation 90 as requested\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Add line breaks to x-axis labels\n",
    "    # Define the maximum width of a label line\n",
    "    max_label_width = 20 # Adjust this value as needed for desired wrapping\n",
    "    # Get current labels, wrap them, and set them back\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    wrapped_labels = [textwrap.fill(label, width=max_label_width) for label in labels]\n",
    "    ax.set_xticklabels(wrapped_labels)\n",
    "    # Adjust rotation mode for potentially multi-line rotated labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation_mode=\"anchor\", ha=\"right\")\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%d', label_type='edge', fontsize=9, padding=3)\n",
    "\n",
    "    # Adjust y-limit slightly\n",
    "    ax.margins(y=0.1)\n",
    "\n",
    "    plt.tight_layout() # Adjust layout AFTER setting new labels\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_top_physics_subareas_from_physics_area(flat_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49253fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_areas_per_domain(\n",
    "    flat_df = flat_df\n",
    ") -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Generates a static, faceted bar chart showing the Top 3 Areas per Domain\n",
    "    based on total counts between Jan 1, 2020, and Dec 31, 2025.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame with 'categories', 'versions'.\n",
    "        category_df (pd.DataFrame): DataFrame with category hierarchy.\n",
    "\n",
    "    Returns:\n",
    "        Optional[matplotlib.figure.Figure]: The generated Matplotlib figure object,\n",
    "                                             or None if plotting is not possible.\n",
    "    \"\"\"\n",
    "\n",
    "    # Aggregate counts per Domain and Area\n",
    "    area_counts = flat_df.groupby(['domain', 'subarea'], observed=False).size().reset_index(name='count')\n",
    "\n",
    "    # Find the top 3 areas within each domain\n",
    "    top_areas_list = []\n",
    "    for domain, group in area_counts.groupby('domain', observed=False):\n",
    "        top_areas_list.append(group.nlargest(3, 'count'))\n",
    "\n",
    "    top_areas_per_domain = pd.concat(top_areas_list).reset_index(drop=True)\n",
    "\n",
    "    num_domains = top_areas_per_domain['domain'].nunique()\n",
    "    print(f\"Plotting Top Areas across {num_domains} Domains...\")\n",
    "\n",
    "    # Determine layout for facets\n",
    "    cols = 3\n",
    "    rows = (num_domains + cols - 1) // cols\n",
    "\n",
    "    # Create the plot\n",
    "    g = sns.catplot(\n",
    "        data=top_areas_per_domain,\n",
    "        x='subarea',\n",
    "        y='count',\n",
    "        col='domain',\n",
    "        kind='bar',\n",
    "        col_wrap=cols,\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "        height=4, \n",
    "        aspect=1.2,\n",
    "       #palette=['#FCBA12','#448D76','#AE0D7A']\n",
    "        palette=\"colorblind\"\n",
    "    )\n",
    "\n",
    "    # Customize plot\n",
    "    g.fig.suptitle('Top 3 Areas per Domain by Count (2020-2025)', y=1.03, fontsize=16)\n",
    "    g.set_titles(\"Domain: {col_name}\")\n",
    "    g.set_axis_labels(\"Area\", \"Total Count\")\n",
    "\n",
    "    # Add count labels to bars\n",
    "    for ax in g.axes.flat:\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%d', label_type='edge', fontsize=8, padding=2)\n",
    "        ax.margins(y=0.1)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    return g.fig\n",
    "\n",
    "fig = plot_top_areas_per_domain()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347628ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_physics_subareas_from_physics_domain(\n",
    "    flat_df: pd.DataFrame, # Corrected type hint based on usage\n",
    ") -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Generates a static bar chart showing the Top 10 SubAreas within the 'Physics'\n",
    "    Domain based on total counts between Jan 1, 2020, and Dec 31, 2025.\n",
    "    Adds line breaks to long subarea names on the x-axis.\n",
    "\n",
    "    Args:\n",
    "        flat_df (pd.DataFrame): DataFrame containing at least 'domain', 'subarea' columns.\n",
    "                                 Expected to have one row per item to count.\n",
    "\n",
    "    Returns:\n",
    "        Optional[matplotlib.figure.Figure]: The generated Matplotlib figure object,\n",
    "                                             or None if plotting is not possible (e.g., no data).\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for Physics domain\n",
    "    physics_df = flat_df[flat_df['domain'] == 'Physics'].copy()\n",
    "\n",
    "    if physics_df.empty:\n",
    "        print(\"No data found for the 'Physics' domain.\")\n",
    "        return None\n",
    "\n",
    "    # Aggregate counts per SubArea within Physics\n",
    "    subarea_counts = physics_df.groupby('subarea', observed=False).size().reset_index(name='count')\n",
    "\n",
    "    # Find the top 10 subareas\n",
    "    top_10_physics_subareas = subarea_counts.nlargest(10, 'count')\n",
    "\n",
    "    if top_10_physics_subareas.empty:\n",
    "        print(\"No subareas found within the 'Physics' domain after filtering.\")\n",
    "        return None\n",
    "\n",
    "    num_subareas = len(top_10_physics_subareas)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 7)) # Consider adjusting figsize if labels overlap\n",
    "\n",
    "    sns.barplot(\n",
    "        data=top_10_physics_subareas,\n",
    "        x='subarea',\n",
    "        y='count',\n",
    "        ax=ax,\n",
    "        palette='colorblind'\n",
    "        # order=top_10_physics_subareas['subarea'] # Order is implicitly handled by nlargest\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'Top {num_subareas} Physics Areas by Paper Count (2020-2025)', fontsize=16)\n",
    "    ax.set_xlabel(\"Physics Area\", fontsize=12)\n",
    "    ax.set_ylabel(\"Total Count\", fontsize=12)\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=8) # Keep rotation 90 as requested\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Add line breaks to x-axis labels\n",
    "    # Define the maximum width of a label line\n",
    "    max_label_width = 20 # Adjust this value as needed for desired wrapping\n",
    "    # Get current labels, wrap them, and set them back\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    wrapped_labels = [textwrap.fill(label, width=max_label_width) for label in labels]\n",
    "    ax.set_xticklabels(wrapped_labels)\n",
    "    # Adjust rotation mode for potentially multi-line rotated labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation_mode=\"anchor\", ha=\"right\")\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%d', label_type='edge', fontsize=9, padding=3)\n",
    "\n",
    "    # Adjust y-limit slightly\n",
    "    ax.margins(y=0.1)\n",
    "\n",
    "    plt.tight_layout() # Adjust layout AFTER setting new labels\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_top_physics_subareas_from_physics_domain(flat_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91985022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
