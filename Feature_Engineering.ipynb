{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ce8518",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18c241",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e81767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils import resample\n",
    "import umap\n",
    "import scipy.sparse\n",
    "from typing import List, Dict, Tuple, Any, Set, Optional\n",
    "import inflect\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.ticker as mtick\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import pickle\n",
    "import hdbscan\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de7c59",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('arxiv_cleaned.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d196e9",
   "metadata": {},
   "source": [
    "## Non-Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3248d",
   "metadata": {},
   "source": [
    "### Number of Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_authors'] = df['authors_parsed'].apply(len)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(columns=['authors_parsed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5881df8",
   "metadata": {},
   "source": [
    "### Flatten Domain and Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549a589",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Split the 'Domain' column into individual domains and explode into rows\n",
    "df_domains = df['Domain'].str.split(';').explode().str.strip()\n",
    "\n",
    "# Get all unique domains\n",
    "unique_domains = df_domains.unique()\n",
    "\n",
    "# Create binary columns for each unique domain\n",
    "for domain in unique_domains:\n",
    "    df[domain] = df['Domain'].str.contains(domain, case=False, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e92c4",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Split the 'Area' column into individual areas and explode into rows\n",
    "df_areas = df['Area'].str.split(';').explode().str.strip()\n",
    "\n",
    "# Get all unique areas\n",
    "unique_areas = df_areas.unique()\n",
    "\n",
    "# Create a DataFrame with binary columns for each unique area (more memory efficient, many columns...)\n",
    "binary_columns = {area: df['Area'].str.contains(area, case=False, na=False) for area in unique_areas}\n",
    "binary_df = pd.DataFrame(binary_columns)\n",
    "\n",
    "# Concatenate the original DataFrame with the new binary columns\n",
    "df = pd.concat([df, binary_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e568ca",
   "metadata": {},
   "source": [
    "## Text derived Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24bed48",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af495f9",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30dd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['abstract'] = df['abstract'].fillna('')\n",
    "\n",
    "df['abstract'] = df['abstract'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823fdac",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text: str) -> str:\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"  # Return empty string for NaN or non-string inputs\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df['abstract'] = df['abstract'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5a3a0",
   "metadata": {},
   "source": [
    "Standardize Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Standardizing Abbreviations ---\")\n",
    "abbreviation_map = {\n",
    "    # CS / General ML\n",
    "    'machine learning ml': 'machine learning',\n",
    "    'learning ml': 'machine learning',\n",
    "    'ml models': 'machine learning model',\n",
    "    'reinforcement learning rl': 'reinforcement learning',\n",
    "    'artificial intelligence ai': 'artificial intelligence',\n",
    "    'internet things iot': 'internet things',\n",
    "    'convolutional neural network cnn': 'convolutional neural network',\n",
    "    'deep learning dl': 'deep learning',\n",
    "    'deep neural network dnn': 'deep neural network',\n",
    "    'neural networks cnns': 'neural networks',\n",
    "    'language models llms': 'language model',\n",
    "    'language model lm': 'language model',\n",
    "    'language processing nlp': 'natural language processing',\n",
    "    'natural language processing nlp': 'natural language processing',\n",
    "    'federated learning fl': 'federated learning',\n",
    "    'principal component analysis pca': 'principal component analysis',\n",
    "    'structural equation modelling sem': 'structural equation modelling',\n",
    "    'extended reality xr': 'extended reality',\n",
    "    # Physics\n",
    "    'black hole bh': 'black hole',\n",
    "    'dark matter dm': 'dark matter',\n",
    "    'density functional theory dft': 'density functional theory',\n",
    "    'molecular dynamics md': 'molecular dynamics',\n",
    "    # EESS\n",
    "    'computed tomography ct': 'computed tomography',\n",
    "    'magnetic resonance imaging mri': 'magnetic resonance imaging',\n",
    "    'electroencephalography eeg': 'electroencephalography',\n",
    "    'base station bs': 'base station',\n",
    "    'channel state information csi': 'channel state information',\n",
    "    'multipleinput multipleoutput mimo': 'multipleinput multipleoutput',\n",
    "    'signaltonoise ratio snr': 'signaltonoise ratio',\n",
    "    'automatic speech recognition asr': 'automatic speech recognition',\n",
    "    'massive machinetype communications mmtc': 'massive machinetype communications',\n",
    "    'reconfigurable intelligent surface ris': 'reconfigurable intelligent surface',\n",
    "    'user equipment ue': 'user equipment',\n",
    "    # QB\n",
    "    # molecular dynamics md (already listed)\n",
    "    # magnetic resonance imaging mri (already listed)\n",
    "    # electroencephalography eeg (already listed)\n",
    "    # artificial intelligence ai (already listed)\n",
    "    # Econ\n",
    "    # structural equation modelling sem (already listed)\n",
    "    # QF\n",
    "    'agentbased models abms': 'agentbased models',\n",
    "    'agentbased model gabm': 'agentbased model',\n",
    "    'environmental social governance esg': 'environmental social governance',\n",
    "    'value risk var': 'value risk',\n",
    "}\n",
    "\n",
    "def standardize_abbreviations(text, abbreviation_map):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return text, 0  # Return as-is if NaN or not a string, with 0 replacements\n",
    "\n",
    "    replacement_count = 0  # Track the number of replacements\n",
    "    for abbr, full_form in abbreviation_map.items():\n",
    "        if abbr in text:  # Check if the abbreviation exists in the text\n",
    "            text = text.replace(abbr, full_form)  # Replace abbreviations with full forms\n",
    "            replacement_count += 1  # Increment the counter for each replacement\n",
    "    return text, replacement_count\n",
    "\n",
    "# Apply the function to the \"abstract\" column and track replacements\n",
    "adjustment_counts = []\n",
    "df['abstract'], adjustment_counts = zip(*df['abstract'].apply(lambda x: standardize_abbreviations(x, abbreviation_map)))\n",
    "\n",
    "# Print total adjustments\n",
    "total_adjustments = sum(adjustment_counts)\n",
    "print(f\"Total number of adjustments made: {total_adjustments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa17fa",
   "metadata": {},
   "source": [
    "Creating Checkpoint to load dataframe from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "#df.to_parquet(\"checkpoint_lemmatized.parquet\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "df = pd.read_parquet(\"checkpoint_lemmatized.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcd29a",
   "metadata": {},
   "source": [
    "#### Keyword Extraction (per domain and month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_monthly_top_keywords(\n",
    "    dataframe: pd.DataFrame,\n",
    "    category_column: str,\n",
    "    date_column: str = 'first_date',\n",
    "    text_column: str = 'abstract',\n",
    "    top_n: int = 30,\n",
    "    boost_bigrams: float = 1.0,\n",
    "    boost_trigrams: float = 2.0,\n",
    "    max_df: float = 0.8,\n",
    "    min_df: int = 1,\n",
    "    ngram_range: Tuple[int, int] = (2, 3)\n",
    ") -> Tuple[List[str], Dict[Any, List[Tuple[str, float]]]]:\n",
    "    \"\"\"\n",
    "    Analyzes text data within a specific category of a DataFrame, grouped by month,\n",
    "    to extract the top N keywords using TF-IDF, with optional boosting for n-grams.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing the data.\n",
    "        category_column (str): The name of the boolean column used to filter the\n",
    "                               DataFrame for the relevant category.\n",
    "        date_column (str): The name of the column containing datetime objects.\n",
    "        text_column (str): The name of the column containing the text data (e.g., abstracts).\n",
    "        top_n (int): The number of top keywords to extract for each month.\n",
    "        boost_bigrams (float): Factor to boost the TF-IDF score of bigrams.\n",
    "        boost_trigrams (float): Factor to boost the TF-IDF score of trigrams.\n",
    "        tfidf_max_df (float): max_df parameter for TfidfVectorizer. Ignore terms\n",
    "                              that appear in more than this fraction of documents.\n",
    "        tfidf_min_df (int): min_df parameter for TfidfVectorizer. Ignore terms\n",
    "                            that appear in less than this number of documents.\n",
    "        tfidf_ngram_range (Tuple[int, int]): ngram_range parameter for TfidfVectorizer.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[Any, List[Tuple[str, float]]]]:\n",
    "            - A list of unique top keywords found across all months.\n",
    "            - A dictionary where keys are the year-month periods and values are\n",
    "              lists of (keyword, score) tuples for that month's top keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        raise TypeError(\"Input 'dataframe' must be a pandas DataFrame.\")\n",
    "    if category_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Column '{category_column}' not found in DataFrame.\")\n",
    "    if date_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Column '{date_column}' not found in DataFrame.\")\n",
    "    if text_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "    if not pd.api.types.is_datetime64_any_dtype(dataframe[date_column]):\n",
    "         try:\n",
    "             # Attempt conversion if not already datetime\n",
    "             dataframe[date_column] = pd.to_datetime(dataframe[date_column])\n",
    "             print(f\"Warning: Column '{date_column}' converted to datetime objects.\")\n",
    "         except Exception as e:\n",
    "            raise TypeError(f\"Column '{date_column}' must be of datetime type or convertible to it. Error: {e}\")\n",
    "    if not pd.api.types.is_bool_dtype(dataframe[category_column]):\n",
    "        # Attempt conversion if possible (e.g., 0/1)\n",
    "        try:\n",
    "            dataframe[category_column] = dataframe[category_column].astype(bool)\n",
    "            print(f\"Warning: Column '{category_column}' converted to boolean type.\")\n",
    "        except Exception as e:\n",
    "            raise TypeError(f\"Column '{category_column}' must be of boolean type or convertible to it. Error: {e}\")\n",
    "    if not isinstance(top_n, int) or top_n <= 0:\n",
    "        raise ValueError(\"'top_n' must be a positive integer.\")\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    # Filter for the specified category and create a copy to avoid SettingWithCopyWarning\n",
    "    df_filtered = dataframe[dataframe[category_column]].copy()\n",
    "\n",
    "    # Handle potential NaNs in text column before processing\n",
    "    df_filtered.dropna(subset=[text_column], inplace=True)\n",
    "    df_filtered[text_column] = df_filtered[text_column].astype(str) # Ensure text is string\n",
    "\n",
    "    # Create the year_month period\n",
    "    df_filtered['year_month'] = df_filtered[date_column].dt.to_period('M')\n",
    "\n",
    "    monthly_top_keywords = {} # Dictionary to store results {year_month: [(keyword, score), ...]}\n",
    "\n",
    "    # Sort by year_month to process chronologically\n",
    "    df_filtered = df_filtered.sort_values('year_month')\n",
    "\n",
    "    # --- Group by Month and Analyze ---\n",
    "    # Group by the created year_month period\n",
    "    for year_month, group_df in df_filtered.groupby('year_month'):\n",
    "\n",
    "        # Get the processed text data for the current month\n",
    "        texts_this_month = group_df[text_column]\n",
    "\n",
    "        # Skip if no valid (non-empty) texts for this month\n",
    "        if texts_this_month.empty or texts_this_month.str.strip().eq('').all():\n",
    "            print(f\"No valid text data found for month {year_month}. Skipping.\")\n",
    "            monthly_top_keywords[year_month] = []\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Initialize TF-IDF Vectorizer FOR THIS MONTH'S DATA\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                max_df=max_df,\n",
    "                min_df=min_df,\n",
    "                ngram_range=ngram_range\n",
    "            )\n",
    "\n",
    "            # Fit and transform the texts *for this month*\n",
    "            tfidf_matrix = vectorizer.fit_transform(texts_this_month)\n",
    "\n",
    "            # Get the feature names (keywords) learned from this month's data\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "            # Calculate the sum of TF-IDF scores for each term across all docs in this month\n",
    "            sum_tfidf = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "            # Map scores to feature names\n",
    "            scores = [(feature_names[col], sum_tfidf[0, col]) for col in range(tfidf_matrix.shape[1])]\n",
    "\n",
    "            # Boost N-grams\n",
    "            boosted_scores = []\n",
    "            for term, score in scores:\n",
    "                num_spaces = term.count(' ')\n",
    "                boosted_score = score\n",
    "                if num_spaces == 1: \n",
    "                    boosted_score *= boost_bigrams\n",
    "                elif num_spaces == 2: \n",
    "                    boosted_score *= boost_trigrams\n",
    "\n",
    "                if boosted_score > 0: # Only add terms with a positive boosted score\n",
    "                     boosted_scores.append((term, boosted_score))\n",
    "\n",
    "            # Sort terms by the potentially boosted score in descending order\n",
    "            sorted_scores = sorted(boosted_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Get the top N keywords for the month\n",
    "            top_keywords_this_month = sorted_scores[:top_n]\n",
    "\n",
    "            # Store the results\n",
    "            monthly_top_keywords[year_month] = top_keywords_this_month\n",
    "\n",
    "        except ValueError as e:\n",
    "            # Handle cases where TF-IDF might fail (e.g., all terms are stop words after filtering)\n",
    "            print(f\"Could not process month {year_month} with TF-IDF: {e}\")\n",
    "            monthly_top_keywords[year_month] = []\n",
    "\n",
    "    # --- Collect Unique Keywords ---\n",
    "    unique_keywords_set = set()\n",
    "    for keywords in monthly_top_keywords.values():\n",
    "        unique_keywords_set.update([keyword for keyword, _ in keywords])\n",
    "\n",
    "    unique_keywords_list = sorted(list(unique_keywords_set))\n",
    "\n",
    "    return unique_keywords_list, monthly_top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns_to_process: List[str] = [\n",
    "        \"Physics\",\n",
    "        \"Computer Science\",\n",
    "        \"Statistics\",\n",
    "        \"Mathematics\",\n",
    "        \"Electrical Engineering and Systems Science\",\n",
    "        \"Quantitative Biology\",\n",
    "        \"Economics\",\n",
    "        \"Quantitative Finance\"\n",
    "]\n",
    "\n",
    "domain_specific_params: Dict[str, Dict[str, Any]] = {\n",
    "    \"Physics\": {\"min_df\": 0.0005, \"max_df\": 0.6, \"top_n\": 100}, # Many records in Physics domain\n",
    "    \"Computer Science\": {\"min_df\": 0.0005, \"max_df\": 0.6, \"top_n\": 60}, # Many records in Computer Science domain\n",
    "    \"Statistics\": {\"min_df\": 0.04, \"max_df\": 0.6},\n",
    "    \"Mathematics\": {\"min_df\": 0.01, \"max_df\": 0.6},\n",
    "    \"Electrical Engineering and Systems Science\": {\"min_df\": 0.04, \"max_df\": 0.6}, # Missing keywords mid 2015 - beginning of 2017 even with very unrestrictive parameters\n",
    "    # https://info.arxiv.org/new/eess_announce.html  Electrical Engineering and Systems Science archive (eess) was introduced 18 September 2017\n",
    "    \"Quantitative Biology\": {\"min_df\": 0.04, \"max_df\": 0.6},\n",
    "    \"Economics\": {\"min_df\": 0.06, \"max_df\": 0.6},\n",
    "    \"Quantitative Finance\": {\"min_df\": 0.06, \"max_df\": 0.6},\n",
    "}\n",
    "\n",
    "domain_keywords: Dict[str, List[str]] = {}\n",
    "\n",
    "print(\"Starting keyword extraction across multiple categories...\")\n",
    "\n",
    "# Loop through each category\n",
    "for category_col in category_columns_to_process:\n",
    "    print(f\"\\nProcessing category: '{category_col}'...\")\n",
    "\n",
    "    # Check if the category column exists in the DataFrame\n",
    "    if category_col not in df.columns:\n",
    "        print(f\"Warning: Column '{category_col}' not found in DataFrame. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Check if there's any data for this category\n",
    "    if not df[category_col].any():\n",
    "        print(f\"No entries found for category '{category_col}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get the domain-specific parameters for this category\n",
    "        params = domain_specific_params.get(category_col, {})\n",
    "        top_n = params.get(\"top_n\", 30)\n",
    "        min_df = params.get(\"min_df\", 1)\n",
    "        max_df = params.get(\"max_df\", 0.8)\n",
    "        ngram_range = params.get(\"ngram_range\", (2, 3))\n",
    "\n",
    "        # Call the keyword extraction function with the specific parameters\n",
    "        unique_keywords_list, monthly_keywords_dict = extract_monthly_top_keywords(\n",
    "            dataframe=df,\n",
    "            category_column=category_col,\n",
    "            date_column='first_date',\n",
    "            text_column='abstract',\n",
    "            top_n=top_n,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            ngram_range=ngram_range\n",
    "        )\n",
    "\n",
    "        # Store the unique keywords for this category in the dictionary\n",
    "        domain_keywords[category_col] = unique_keywords_list\n",
    "\n",
    "        print(f\"Finished processing '{category_col}'. Added {len(unique_keywords_list)} unique keywords.\")\n",
    "        if not unique_keywords_list:\n",
    "            print(f\"(No keywords met the TF-IDF criteria for '{category_col}')\")\n",
    "\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Error processing category '{category_col}': {e}. Skipping this category.\")\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors\n",
    "        print(f\"An unexpected error occurred while processing category '{category_col}': {e}. Skipping this category.\")\n",
    "\n",
    "print(\"\\n--- Keyword Extraction Complete ---\")\n",
    "\n",
    "# Print the total number of keywords per domain\n",
    "for domain, keywords in domain_keywords.items():\n",
    "    print(f\"Domain: {domain}, Total Keywords: {len(keywords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfcc2e",
   "metadata": {},
   "source": [
    "Creating Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3654734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint a saving\n",
    "\n",
    "with open('domain_keywords.json', 'w') as json_file:\n",
    "    json.dump(domain_keywords, json_file, indent=4)\n",
    "    \n",
    "\n",
    "export_data = {\n",
    "    \"unique_domains\": unique_domains.tolist(),\n",
    "    \"unique_areas\": unique_areas.tolist(),\n",
    "    \"unique_keywords_list\": unique_keywords_list\n",
    "}\n",
    "\n",
    "with open(\"unique_data.json\", \"w\") as json_file:\n",
    "    json.dump(export_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint b loading\n",
    "\n",
    "df = pd.read_parquet(\"checkpoint_lemmatized.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "with open('domain_keywords.json', 'r') as json_file:\n",
    "    domain_keywords = json.load(json_file)\n",
    "\n",
    "with open(\"unique_data.json\", \"r\") as json_file:\n",
    "    imported_data = json.load(json_file)\n",
    "\n",
    "unique_domains = np.array(imported_data[\"unique_domains\"])\n",
    "unique_areas = np.array(imported_data[\"unique_areas\"])\n",
    "unique_keywords_list = imported_data[\"unique_keywords_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf17a2e",
   "metadata": {},
   "source": [
    "#### Keyword Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = []\n",
    "\n",
    "\n",
    "for domain, keyword_list in domain_keywords.items():\n",
    "    if not isinstance(keyword_list, list):\n",
    "        print(f\"  - Warning: Keywords for domain '{domain}' is not a list. Skipping this domain for DataFrame.\")\n",
    "        continue\n",
    "    if not keyword_list:\n",
    "        print(f\"  - No keywords for domain '{domain}'. This domain will have no entries in DataFrame.\")\n",
    "        # If you want to represent domains with no keywords, you could add a row with None or empty string for keyword\n",
    "        # df_data.append({'domain': domain, 'keyword': None}) # Example\n",
    "        continue\n",
    "    for keyword in keyword_list:\n",
    "        # Append a dictionary for each keyword, associating it with its domain\n",
    "        df_data.append({'domain': domain, 'keyword': keyword})\n",
    "\n",
    "domain_keywords_df = pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Remove Generic Phrases ---\n",
    "print(\"\\n--- Removing Generic Phrases ---\")\n",
    "generic_phrases_to_remove = [\n",
    "    'et al', 'results indicate', 'results suggest', 'results demonstrate',\n",
    "    'paper propose', 'paper present', 'work propose', 'introduce novel', 'propose new',\n",
    "    'experimental results', 'numerical results', 'simulation results',\n",
    "    'demonstrate effectiveness', 'effectiveness proposed', 'proposed method',\n",
    "    'proposed approach', 'proposed model', 'proposed algorithm', 'proposed scheme',\n",
    "    'recent years', 'widely used', 'commonly used', 'wide range', 'large number',\n",
    "    'based on', 'compared to', 'address problem', 'paper deals', 'paper investigates',\n",
    "    'paper study', 'study investigates', 'study shows', 'study aims', 'aim paper',\n",
    "    'demonstrate proposed', 'results proposed', 'findings indicate', 'findings suggest',\n",
    "    'existing methods', 'existing approaches', 'existing literature', 'previous work',\n",
    "    'previous studies', 'recent work', 'recent studies', 'recent developments',\n",
    "    'provide evidence', 'paper provide', 'study provides', 'present paper',\n",
    "    'main result', 'key role', 'important role', 'crucial role', 'significant role',\n",
    "    'better understanding', 'deeper understanding', 'valuable insights', 'shed light',\n",
    "    'future research', 'possible future research', 'directions future research',\n",
    "    'case study', 'numerical examples', 'experimental data', 'real data', 'real world',\n",
    "    'publicly available', 'code available','good agreement', 'high accuracy', 'better performance',\n",
    "    'superior performance', 'computational cost', 'computational complexity', 'computationally efficient',\n",
    "    'state art', 'stateoftheart methods','stateoftheart performance','results obtained', 'results confirm',\n",
    "    'results reveal', 'address challenges', 'address issue', 'address gap',\n",
    "    'apply method', 'apply results', 'approach based', 'method based', 'framework based',\n",
    "    'consider problem', 'develop novel', 'develop framework',\n",
    "    'empirical evidence', 'empirical application', 'empirical analysis',\n",
    "    'establish existence', 'evaluate performance', 'examine potential',\n",
    "    'explain sustainability', 'explore key', 'extract important',\n",
    "    'findings emphasize', 'focus specifically', 'gain insights',\n",
    "    'highlight importance', 'illustrate method', 'implementable pricebased',\n",
    "    'improve performance', 'increase probability', 'investigate effect',\n",
    "    'make use', 'obtain new', 'outperforms existing',\n",
    "    'play crucial role', 'plays important role', 'present comprehensive',\n",
    "    'provide new', 'purpose paper', 'purpose research',\n",
    "    'showcase potential', 'solve problem', 'study aimed', 'study demonstrates',\n",
    "    'study examines', 'suggests evaluate', 'theoretical analysis', 'theoretical results',\n",
    "    'understand relationship', 'use cases', 'using data', 'using numerical', 'et al phys',\n",
    "    'data used', 'question answering', 'study propose', 'use case', 'present new',\n",
    "    'evidence suggests', 'model used', 'plays crucial', 'plays crucial role', 'present novel',\n",
    "    'propose novel', 'paves way', 'significant challenge', 'outperforms state-of-the-art', 'introduces novel',\n",
    "    'prove existence', 'new results', 'previous results', 'new examples', 'new method',\n",
    "    'novel approach', 'similar results', 'open problem', 'new proof', 'recent results',\n",
    "    'closely related', 'previously known', 'recently introduced', 'new approach', 'present new',\n",
    "    'gives rise', 'model based', 'extensive experiments', 'upper bound', 'recently proposed',\n",
    "    'present study', 'standard model', 'work present', 'pave way', 'present results', 'present work',\n",
    "    'provide valuable insights', 'results provide', 'results pave way', 'standard model sm', 'taken account', \n",
    "    'taking account', 'best practices', 'challenging task', 'demonstrate effectiveness proposed', 'demonstrate proposed method',\n",
    "    'effectiveness proposed approach', 'effectiveness proposed model', 'effectiveness proposed method',\n",
    "    'novel method', 'findings reveal', 'experimental results demonstrate', 'experimental results proposed', 'experiments conducted', 'experiments demonstrate',\n",
    "    'extensive experimental results', 'extensive experiments conducted', 'extensive experiments demonstrate', 'extensive experiments realworld', 'paper consider',\n",
    "    'paper explores', 'paper introduce novel', 'paper introduces novel', 'paper present novel', 'paper presents comprehensive',\n",
    "    'paper presents new', 'paper presents novel', 'paper propose new', 'paper propose novel', 'paper proposes novel',\n",
    "    'result suggest', 'result demonstrate', 'result indicate', 'result obtained', 'result pave way', 'result provide', 'result suggest',\n",
    "    'address challenge', 'address challenge propose', 'address issue paper', 'address issue propose', 'address limitation propose',\n",
    "    'existing approach', 'existing method', 'experimental research data', 'experimental result', 'experimental result demonstrate',\n",
    "    'experimental result proposed', 'extensive experiment', 'paper address problem', 'paper describes', 'paper introduce',\n",
    "    'paper introduce new', 'paper investigate', 'paper present new', 'paper study problem', 'present novel approach',\n",
    "    'research performance', 'result demonstrate', 'result demonstrate proposed', 'result obtained', 'result proposed',\n",
    "    'result proposed method', 'data set', 'data set different', 'data point', 'data based',\n",
    "    'experimental result', 'data collection', 'introduce new', 'paper aim', 'paper analysis',\n",
    "    'paper analyze', 'paper deal', 'paper introduces', 'paper investigate', 'paper model approximate',\n",
    "    'paper provides', 'recent advance', 'recent development', 'recent year', 'recently used',\n",
    "    'significantly outperforms', 'special case', 'open question', 'stateoftheart method', 'provide example',\n",
    "    'provide explicit', 'provide insight', 'provide new proof', 'recent advancement', 'recent paper',\n",
    "    'recent result', 'result hold', 'result numerical example', 'result obtained', 'result paper',\n",
    "    'study conducted', 'study investigate', 'study investigated', 'study present', 'study reported'\n",
    "\n",
    "    # Synonym / Incomplete expressions section\n",
    "    'deep neural', 'large language', 'neural networks', 'data sets', 'machine learning algorithms',\n",
    "    'models llms', 'convolutional neural', 'van der', 'der waals', 'deep neural', 'language models',\n",
    "    'intelligence ai', 'deep learning models', 'learning models', 'resonance imaging', 'magnetic resonance',\n",
    "    'network cnn', 'neural network cnn', 'learning dl', 'markov chain monte', 'chain monte carlo',\n",
    "    'monte carlo mcmc', 'chain monte', 'carlo mcmc', 'natural language', 'quantum manybody',\n",
    "    'manybody systems', 'galactic nuclei agn', 'galactic nuclei', 'active galactic', 'nuclei agn',\n",
    "    'quantum information', 'information processing', 'carlo simulations', 'md simulations', 'blood glucose values',\n",
    "    'computer vision tasks', 'datasets demonstrate', 'deep learningbased', 'deep reinforcement', 'learning model'\n",
    "]\n",
    "\n",
    "def remove_keywords_with_prefixes(df, column, prefixes):\n",
    "    \"\"\"\n",
    "    Removes rows from the DataFrame where the specified column's values start with any of the given prefixes.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column (str): The column to check for prefixes.\n",
    "        prefixes (list): A list of prefixes to check.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the filtered rows.\n",
    "    \"\"\"\n",
    "    pattern = f\"^({'|'.join(prefixes)})\"\n",
    "    mask = ~df[column].str.lower().str.match(pattern)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "count_before_generic_removal = len(domain_keywords_df)\n",
    "mask = ~domain_keywords_df['keyword'].str.lower().isin(generic_phrases_to_remove)\n",
    "refined_df = domain_keywords_df[mask].copy()\n",
    "print(f\"Keyword count after removing generic phrases: {len(refined_df)} (Removed {count_before_generic_removal - len(refined_df)})\")\n",
    "\n",
    "count_before_prefix_removal = len(refined_df)\n",
    "prefixes_to_remove = ['result', 'paper', 'provide', 'recent', 'address', 'model ', 'method ', 'significantly', 'consider ', 'considers ', 'considered ']\n",
    "refined_df = remove_keywords_with_prefixes(refined_df, 'keyword', prefixes_to_remove)\n",
    "print(f\"Keyword count after removing prefixes: {len(refined_df)} (Removed {count_before_prefix_removal - len(refined_df)})\")\n",
    "\n",
    "# --- Display final counts ---\n",
    "print(\"\\nFinal keyword counts per domain:\")\n",
    "print(refined_df['domain'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa77695",
   "metadata": {},
   "source": [
    "#### Keyword Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2272f",
   "metadata": {},
   "source": [
    "##### Overall in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette_hex = [\"#89043d\",\"#8a817c\",\"#bcb8b1\",\"#f4f3ee\",\"#e0afa0\"]\n",
    "\n",
    "def palette_color_func(word, **kwargs):\n",
    "    return random.choice(custom_palette_hex)\n",
    "\n",
    "domain_keyword_frequencies: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "# Loop through unique domains in the refined keyword DataFrame\n",
    "for domain_name in refined_df['domain'].unique():\n",
    "    print(f\"\\nProcessing: '{domain_name}'...\")\n",
    "\n",
    "    # Get refined keywords for this domain\n",
    "    refined_keywords_list = refined_df[refined_df['domain'] == domain_name]['keyword'].tolist()\n",
    "\n",
    "    # Filter original DataFrame for abstracts of this domain\n",
    "    category_mask = df[domain_name] == True\n",
    "    category_abstracts = df.loc[category_mask, \"abstract\"].astype(str).fillna('')\n",
    "\n",
    "    if category_abstracts.empty or not refined_keywords_list:\n",
    "        print(f\"  - Skipping '{domain_name}' (no abstracts or keywords).\")\n",
    "        continue\n",
    "\n",
    "    # Calculate frequencies\n",
    "    frequencies = {}\n",
    "    lower_abstracts_series = category_abstracts.str.lower()\n",
    "    for keyword in refined_keywords_list:\n",
    "        escaped_keyword = re.escape(keyword.lower())\n",
    "        # Use regex=True because re.escape is used.\n",
    "        count = lower_abstracts_series.str.count(escaped_keyword).sum()\n",
    "        if count > 0:\n",
    "            frequencies[keyword] = int(count)\n",
    "\n",
    "    domain_keyword_frequencies[domain_name] = frequencies\n",
    "\n",
    "    if not frequencies:\n",
    "        print(f\"  - Skipping '{domain_name}' (no keywords found in abstracts).\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  - Generating word cloud for '{domain_name}'...\")\n",
    "\n",
    "    # Generate and display Word Cloud\n",
    "    wordcloud_generator = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color='black',\n",
    "        collocations=False,\n",
    "        min_font_size=10,\n",
    "        #colormap='magma',\n",
    "        color_func=palette_color_func,\n",
    "        random_state=None\n",
    "    ).generate_from_frequencies(frequencies)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(wordcloud_generator, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Domain: {domain_name}', fontsize=18)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export domain_keyword_frequencies to a JSON file\n",
    "with open('domain_keyword_frequencies.json', 'w') as json_file:\n",
    "    json.dump(domain_keyword_frequencies, json_file, indent=4)\n",
    "\n",
    "# # Reimport domain_keyword_frequencies from the JSON file\n",
    "# with open('domain_keyword_frequencies.json', 'r') as json_file:\n",
    "#     domain_keyword_frequencies = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfa446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary local variables\n",
    "try:\n",
    "    del category_abstracts, category_mask, count, count_before_generic_removal, count_before_prefix_removal\n",
    "    del custom_palette_hex, df_data, domain, escaped_keyword\n",
    "    del frequencies, generic_phrases_to_remove, imported_data, json_file\n",
    "    del keyword, keywords, lower_abstracts_series, mask, prefixes_to_remove\n",
    "    del refined_keywords_list, wordcloud_generator\n",
    "except NameError as e:\n",
    "    print(f\"Variable not found: {e}\")\n",
    "else:\n",
    "    print(\"All specified variables deleted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce38a7",
   "metadata": {},
   "source": [
    "##### Category specific - DEPRECATED - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9602af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out keywords based on cross-domain frequency (keywords appearing in >1 domains more than 20 times)\n",
    "\n",
    "freq_df = pd.DataFrame([{'domain': dom, 'keyword': kw, 'frequency': fq}\n",
    "                        for dom, kw_fq_dict in domain_keyword_frequencies.items()\n",
    "                        for kw, fq in kw_fq_dict.items()])\n",
    "\n",
    "# 2. Identify keywords frequent (>=5) in >1 domain\n",
    "domain_counts = freq_df.query('frequency >= 20').groupby('keyword')['domain'].nunique()\n",
    "keywords_to_exclude = domain_counts[domain_counts > 1].index\n",
    "\n",
    "# 3. Filter the original refined_df to exclude these keywords\n",
    "domain_specific_keywords_df = refined_df[~refined_df['keyword'].isin(keywords_to_exclude)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain_name in domain_specific_keywords_df['domain'].unique():\n",
    "    print(f\"\\nProcessing: '{domain_name}'...\")\n",
    "\n",
    "    # Get the list of domain-specific keywords for this domain\n",
    "    specific_keywords_set = set(domain_specific_keywords_df[\n",
    "        domain_specific_keywords_df['domain'] == domain_name\n",
    "    ]['keyword'])\n",
    "\n",
    "    # Get the original frequencies for ONLY these specific keywords\n",
    "    # Check if the domain exists in the original frequency dictionary\n",
    "    if domain_name not in domain_keyword_frequencies:\n",
    "        print(f\"  - Skipping '{domain_name}' (no original frequencies found).\")\n",
    "        continue\n",
    "\n",
    "    # Filter the original frequencies for the current domain's specific keywords\n",
    "    frequencies_for_cloud = {\n",
    "        keyword: freq\n",
    "        for keyword, freq in domain_keyword_frequencies[domain_name].items()\n",
    "        if keyword in specific_keywords_set and freq > 0 \n",
    "    }\n",
    "\n",
    "    print(f\"  - Generating word cloud for '{domain_name}' with {len(frequencies_for_cloud)} specific keywords...\")\n",
    "\n",
    "    # Generate and display Word Cloud using the filtered frequencies\n",
    "    wordcloud_generator = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color='black',\n",
    "        collocations=False,\n",
    "        min_font_size=10,\n",
    "        color_func=palette_color_func,\n",
    "        random_state=None\n",
    "    ).generate_from_frequencies(frequencies_for_cloud)\n",
    "\n",
    "    # Display Plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(wordcloud_generator, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Domain: {domain_name}', fontsize=18)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbceecdc",
   "metadata": {},
   "source": [
    "## Features and Final Dataframe for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d26a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N_KEYWORDS_PER_DOMAIN = 400    \n",
    "RANDOM_STATE = 42              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0115bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Keyword Selection -----\n",
    "\n",
    "domain_top_keywords_map: Dict[str, List[str]] = {}\n",
    "# Set to store the global union of all top N keywords\n",
    "final_keyword_list_set = set()\n",
    "\n",
    "# Check if domain_keyword_frequencies exists and is populated\n",
    "if 'domain_keyword_frequencies' not in locals() or not domain_keyword_frequencies:\n",
    "    print(\"Error: 'domain_keyword_frequencies' dictionary not found or empty. Cannot select top keywords.\")\n",
    "    final_keyword_list = [] # Initialize empty list\n",
    "else:\n",
    "    # Iterate through the domains present in the frequency dictionary\n",
    "    for domain_name, keyword_freqs in domain_keyword_frequencies.items():\n",
    "        print(f\"  Processing domain: {domain_name}\")\n",
    "\n",
    "        if not keyword_freqs:\n",
    "            print(f\"    No keywords/frequencies found for domain '{domain_name}'.\")\n",
    "            domain_top_keywords_map[domain_name] = [] # Store empty list for this domain\n",
    "            continue\n",
    "\n",
    "        # Sort keywords in this domain by frequency (descending)\n",
    "        valid_items = [(kw, fq) for kw, fq in keyword_freqs.items() if isinstance(fq, (int, float))]\n",
    "        sorted_keywords = sorted(valid_items, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        # Select the top N keywords for this specific domain\n",
    "        top_n_for_domain = [kw for kw, freq in sorted_keywords[:TOP_N_KEYWORDS_PER_DOMAIN]]\n",
    "        print(f\"    Selected {len(top_n_for_domain)} keywords for '{domain_name}'.\")\n",
    "\n",
    "        # Store this list in the map\n",
    "        domain_top_keywords_map[domain_name] = top_n_for_domain\n",
    "\n",
    "        # Add these keywords to the global set for vectorizer vocabulary\n",
    "        final_keyword_list_set.update(top_n_for_domain)\n",
    "\n",
    "    # Convert the final global set to a sorted list for consistent column order in TfidfVectorizer\n",
    "    final_keyword_list = sorted(list(final_keyword_list_set))\n",
    "    print(f\"\\nCreated map 'domain_top_keywords_map' with top keywords per domain.\")\n",
    "    print(f\"Created global list 'final_keyword_list' with {len(final_keyword_list)} unique keywords across all domains (for vectorizer).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fffbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Create Binary Keyword Features -----\n",
    "\n",
    "if not final_keyword_list:\n",
    "     print(\"Warning: final_keyword_list is empty. No keyword features will be created.\")\n",
    "     keyword_cols_created = []\n",
    "else:\n",
    "    print(f\"\\n--- Creating Binary Features for {len(final_keyword_list)} Keywords ---\")\n",
    "\n",
    "    # Ensure abstract is string type and handle potential NaNs\n",
    "    lower_abstracts = df['abstract'].astype(str).fillna('').str.lower()\n",
    "\n",
    "    # --- Configure TfidfVectorizer ---\n",
    "    max_ngram_length = 1\n",
    "    if final_keyword_list:\n",
    "         max_ngram_length = max(len(kw.split()) for kw in final_keyword_list)\n",
    "\n",
    "    print(f\"  Configuring TfidfVectorizer (max_ngram={max_ngram_length})...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        vocabulary=final_keyword_list,\n",
    "        lowercase=True,\n",
    "        binary=True,\n",
    "        use_idf=False,\n",
    "        norm=None,\n",
    "        ngram_range=(1, max_ngram_length)\n",
    "    )\n",
    "\n",
    "    print(\"  Applying vectorizer to abstracts...\")\n",
    "    X_keywords_sparse = vectorizer.fit_transform(lower_abstracts)\n",
    "    print(f\"  Vectorizer finished. Output shape (sparse): {X_keywords_sparse.shape}\")\n",
    "\n",
    "    # --- Create DataFrame from Sparse Matrix ---\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    keyword_cols_created = [f'kw_{name}' for name in feature_names] # These are the columns added\n",
    "    print(f\"  Creating DataFrame from sparse matrix for {len(keyword_cols_created)} keyword features...\")\n",
    "    try:\n",
    "        df_keywords = pd.DataFrame.sparse.from_spmatrix(\n",
    "            X_keywords_sparse,\n",
    "            index=df.index,\n",
    "            columns=keyword_cols_created\n",
    "        )\n",
    "        print(\"  Keyword DataFrame created.\")\n",
    "\n",
    "        # --- Concatenate with original DataFrame ---\n",
    "        cols_to_drop_from_df = [col for col in keyword_cols_created if col in df.columns]\n",
    "        if cols_to_drop_from_df:\n",
    "             print(f\"  Dropping existing columns from df before concat: {cols_to_drop_from_df}\")\n",
    "             df = df.drop(columns=cols_to_drop_from_df)\n",
    "\n",
    "        df = pd.concat([df, df_keywords], axis=1)\n",
    "        # Convert sparse keyword columns to int (0/1) AFTER concat if needed by downstream steps\n",
    "        # This might increase memory usage significantly.\n",
    "        # df[keyword_cols_created] = df[keyword_cols_created].astype(int)\n",
    "        print(f\"Concatenated keyword features. New df shape: {df.shape}\")\n",
    "\n",
    "    except MemoryError:\n",
    "        print(\"\\nError: MemoryError encountered while creating keyword DataFrame.\")\n",
    "        print(\"Consider reducing TOP_N_KEYWORDS_PER_DOMAIN or using algorithms accepting sparse input.\")\n",
    "        keyword_cols_created = [] # Prevent errors later\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred creating keyword DataFrame: {e}\")\n",
    "        keyword_cols_created = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Final Feature Lists -----\n",
    "\n",
    "# --- Metadata Features ---\n",
    "metadata_features = ['number_of_authors'] + list(unique_areas)\n",
    "\n",
    "# --- Keyword Features ---\n",
    "# keyword_cols_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ccb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in df.columns:\n",
    "    if pd.api.types.is_sparse(df[col_name].dtype):\n",
    "        df[col_name] = df[col_name].sparse.to_dense()\n",
    "\n",
    "df.to_parquet(\"checkpoint_with_keywords.parquet\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "variables_to_export = {\n",
    "    \"domain_top_keywords_map\": domain_top_keywords_map,\n",
    "    \"final_keyword_list\": final_keyword_list,\n",
    "    \"final_keyword_list_set\": list(final_keyword_list_set),\n",
    "    \"keyword_cols_created\": keyword_cols_created,\n",
    "    \"metadata_features\": metadata_features,\n",
    "    \"unique_areas\": unique_areas.tolist(),\n",
    "    \"unique_domains\": unique_domains.tolist(),\n",
    "    \"unique_keywords_list\": unique_keywords_list,\n",
    "}\n",
    "\n",
    "with open(\"checkpoint_variables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(variables_to_export, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
