{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ce8518",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18c241",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e81767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils import resample\n",
    "import umap\n",
    "import scipy.sparse\n",
    "from typing import List, Dict, Tuple, Any, Set, Optional\n",
    "import inflect\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.ticker as mtick\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import pickle\n",
    "import hdbscan\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de7c59",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('arxiv_cleaned.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d196e9",
   "metadata": {},
   "source": [
    "## Non-Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3248d",
   "metadata": {},
   "source": [
    "### Number of Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_authors'] = df['authors_parsed'].apply(len)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(columns=['authors_parsed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c42870",
   "metadata": {},
   "source": [
    "### Journal - DEPRECATED -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81f322",
   "metadata": {},
   "source": [
    "Retriving Journal names from Crossref API by doi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84360c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Parameters ---\n",
    "# CHUNK_SIZE = 100          # How many DOIs to process in each batch\n",
    "# CONCURRENCY_LIMIT = 4     # Max simultaneous requests\n",
    "# DELAY_PER_REQUEST = 0.01   # Seconds to wait BEFORE each request attempt (e.g., 0.3 = ~3 req/sec max)\n",
    "# RETRY_INITIAL_DELAY = 5   # Seconds to wait after first 429 error\n",
    "# MAX_RETRIES = 3           # Max attempts for a single DOI\n",
    "# DELAY_BETWEEN_CHUNKS = 0.5 # Seconds to pause between processing chunks\n",
    "# OUTPUT_FILE = 'doi_journal_results.csv' # File to save progress\n",
    "# USER_AGENT = 'mailto:martina.esser@iu-study.org; purpose: Academic Research Mapping DOIs, Feature Engineering for a Study Project' \n",
    "# # -----------------------------\n",
    "\n",
    "# async def fetch_journal_async(session, doi, max_retries=MAX_RETRIES, initial_delay=RETRY_INITIAL_DELAY):\n",
    "#     \"\"\"\n",
    "#     Asynchronously fetches the journal name for a given DOI using the Crossref API.\n",
    "#     Includes retry logic, pre-request delay, and respects configured parameters.\n",
    "#     \"\"\"\n",
    "#     # --- Polite Pre-Request Delay ---\n",
    "#     await asyncio.sleep(DELAY_PER_REQUEST)\n",
    "#     # -------------------------------\n",
    "\n",
    "#     if pd.isna(doi):\n",
    "#         return doi, None\n",
    "\n",
    "#     url = f\"https://api.crossref.org/works/{doi}\"\n",
    "#     headers = {'User-Agent': USER_AGENT}\n",
    "\n",
    "#     current_delay = initial_delay\n",
    "#     for attempt in range(max_retries):\n",
    "#         try:\n",
    "#             async with session.get(url, headers=headers, timeout=30) as response:\n",
    "#                 if response.status == 200:\n",
    "#                     try:\n",
    "#                         data = await response.json()\n",
    "#                         journal_name = data.get('message', {}).get('container-title', [None])[0]\n",
    "#                         return doi, journal_name\n",
    "#                     except aiohttp.ContentTypeError:\n",
    "#                          print(f\"JSON Decode Error (ContentTypeError) for DOI {doi}. Status: {response.status}. Returning None.\")\n",
    "#                          return doi, None\n",
    "#                     except Exception as json_err: # Catch other potential JSON errors\n",
    "#                          print(f\"JSON Parsing Error for DOI {doi}: {json_err}. Returning None.\")\n",
    "#                          return doi, None\n",
    "#                 elif response.status == 404:\n",
    "#                     # print(f\"DOI not found (404): {doi}\")\n",
    "#                     return doi, None # Not found, don't retry\n",
    "#                 elif response.status == 429:\n",
    "#                     print(f\"Rate limit hit (429) for {doi}. Retrying in {current_delay}s (Attempt {attempt+1}/{max_retries})...\")\n",
    "#                     retry_after = response.headers.get(\"Retry-After\")\n",
    "#                     sleep_time = int(retry_after) if retry_after else current_delay\n",
    "#                     await asyncio.sleep(sleep_time)\n",
    "#                     current_delay *= 2 # Exponential backoff\n",
    "#                 else:\n",
    "#                     # print(f\"HTTP Error {response.status} for DOI {doi}\")\n",
    "#                     response.raise_for_status() # Raise error for other bad statuses\n",
    "#                     return doi, None # Should not be reached if raise_for_status works\n",
    "\n",
    "#         except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "#             print(f\"Network/Timeout Error for DOI {doi}: {e}. Retrying in {current_delay}s (Attempt {attempt+1}/{max_retries})...\")\n",
    "#             await asyncio.sleep(current_delay)\n",
    "#             current_delay *= 2\n",
    "#         except Exception as e:\n",
    "#             print(f\"Unexpected error processing DOI {doi}: {e}\")\n",
    "#             return doi, None # Don't retry on unexpected errors\n",
    "\n",
    "#     print(f\"Failed to fetch DOI {doi} after {max_retries} attempts.\")\n",
    "#     return doi, None\n",
    "\n",
    "\n",
    "# async def process_chunk_async(doi_chunk_series):\n",
    "#     \"\"\"\n",
    "#     Processes a chunk of DOIs asynchronously.\n",
    "#     \"\"\"\n",
    "#     connector = aiohttp.TCPConnector(limit=CONCURRENCY_LIMIT)\n",
    "#     async with aiohttp.ClientSession(connector=connector) as session:\n",
    "#         tasks = [asyncio.create_task(fetch_journal_async(session, doi)) for doi in doi_chunk_series]\n",
    "#         print(f\"  Created {len(tasks)} tasks for this chunk. Fetching...\")\n",
    "#         # Gather results, which will be tuples of (doi, journal_name)\n",
    "#         results = await asyncio.gather(*tasks)\n",
    "#         print(f\"  Chunk fetching complete.\")\n",
    "#         return results # List of (doi, journal_name_or_None) tuples\n",
    "\n",
    "# # --- Main Processing Logic with Chunking and Saving ---\n",
    "\n",
    "# # Split DataFrame into chunks\n",
    "# list_of_df_chunks = np.array_split(df, len(df) // CHUNK_SIZE + 1)\n",
    "# print(f\"\\nSplit DataFrame into {len(list_of_df_chunks)} chunks of approx size {CHUNK_SIZE}.\")\n",
    "\n",
    "# all_results_map = {} # Use a dictionary to store results {doi: journal}\n",
    "\n",
    "# # --- Check if output file exists to potentially resume ---\n",
    "# if os.path.exists(OUTPUT_FILE):\n",
    "#     print(f\"--- Found existing results file: {OUTPUT_FILE}. Loading previous results. ---\")\n",
    "#     try:\n",
    "#         df_existing = pd.read_csv(OUTPUT_FILE)\n",
    "#         # Handle potential missing 'doi' column or empty file\n",
    "#         if 'doi' in df_existing.columns and not df_existing.empty:\n",
    "#              all_results_map = pd.Series(df_existing.journal.values, index=df_existing.doi).to_dict()\n",
    "#              print(f\"--- Loaded {len(all_results_map)} results from file. ---\")\n",
    "#         else:\n",
    "#              print(f\"--- Existing file is empty or missing 'doi' column. Starting fresh. ---\")\n",
    "#     except pd.errors.EmptyDataError:\n",
    "#         print(f\"--- Existing file {OUTPUT_FILE} is empty. Starting fresh. ---\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"--- Error loading existing file {OUTPUT_FILE}: {e}. Starting fresh. ---\")\n",
    "# # -------------------------------------------------------\n",
    "\n",
    "\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# for i, chunk_df in enumerate(list_of_df_chunks):\n",
    "#     chunk_start_time = time.time()\n",
    "#     print(f\"\\n--- Processing Chunk {i+1}/{len(list_of_df_chunks)} ---\")\n",
    "\n",
    "#     # --- Filter out DOIs already processed ---\n",
    "#     dois_in_chunk = chunk_df['doi'].dropna().unique() # Get unique non-null DOIs\n",
    "#     dois_to_process = [doi for doi in dois_in_chunk if doi not in all_results_map]\n",
    "#     # -----------------------------------------\n",
    "\n",
    "#     if not dois_to_process:\n",
    "#         print(f\"--- All DOIs in this chunk already processed. Skipping. ---\")\n",
    "#         continue # Skip to the next chunk\n",
    "\n",
    "#     print(f\"--- Need to process {len(dois_to_process)} DOIs in this chunk. ---\")\n",
    "\n",
    "#     # --- Run async processing for the filtered DOIs ---\n",
    "#     # Use await directly if in Jupyter/IPython, otherwise use asyncio.run()\n",
    "#     try:\n",
    "#         # Pass the list of DOIs to process\n",
    "#         chunk_results_tuples = await process_chunk_async(dois_to_process)\n",
    "#         # Or: chunk_results_tuples = asyncio.run(process_chunk_async(dois_to_process))\n",
    "\n",
    "#         # Update the main results map\n",
    "#         for doi, journal in chunk_results_tuples:\n",
    "#             if doi: # Ensure DOI is not None before adding\n",
    "#                  all_results_map[doi] = journal\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"!!!!!!!!!! ERROR processing chunk {i+1}: {e} !!!!!!!!!!!\")\n",
    "#         print(\"Saving progress before potentially stopping...\")\n",
    "#         # Fall through to save progress even if an error occurred in gather/run\n",
    "\n",
    "#     # --- Save Progress After Each Chunk ---\n",
    "#     try:\n",
    "#         # Convert results map back to DataFrame for saving\n",
    "#         df_to_save = pd.DataFrame(list(all_results_map.items()), columns=['doi', 'journal'])\n",
    "#         df_to_save.to_csv(OUTPUT_FILE, index=False)\n",
    "#         print(f\"--- Progress saved to {OUTPUT_FILE} ({len(all_results_map)} total results). ---\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"--- ERROR saving progress to {OUTPUT_FILE}: {e} ---\")\n",
    "#     # ------------------------------------\n",
    "\n",
    "#     chunk_end_time = time.time()\n",
    "#     print(f\"--- Chunk {i+1} processing took: {chunk_end_time - chunk_start_time:.2f} seconds ---\")\n",
    "\n",
    "#     # --- Pause Between Chunks ---\n",
    "#     if i < len(list_of_df_chunks) - 1: # Don't sleep after the last chunk\n",
    "#         print(f\"--- Pausing for {DELAY_BETWEEN_CHUNKS} seconds before next chunk... ---\")\n",
    "#         time.sleep(DELAY_BETWEEN_CHUNKS)\n",
    "#     # --------------------------\n",
    "\n",
    "# total_end_time = time.time()\n",
    "# print(f\"\\n\\n======================================================\")\n",
    "# print(f\"Total processing finished in: {(total_end_time - total_start_time) / 60:.2f} minutes\")\n",
    "# print(f\"Final results saved to {OUTPUT_FILE}\")\n",
    "# print(f\"Total unique DOIs processed and saved: {len(all_results_map)}\")\n",
    "# print(f\"======================================================\")\n",
    "\n",
    "# # --- Finally, map results back to the original DataFrame (optional) ---\n",
    "# # This can be done after the script finishes by loading the CSV\n",
    "# print(\"\\nMapping results back to the original DataFrame...\")\n",
    "# df_final_results = pd.read_csv(OUTPUT_FILE)\n",
    "# # Create a mapping series\n",
    "# journal_map = pd.Series(df_final_results.journal.values, index=df_final_results.doi)\n",
    "# # Map values, keeping existing NaNs if DOI wasn't found or processed\n",
    "# df['journal'] = df['doi'].map(journal_map)\n",
    "\n",
    "# print(\"Mapping complete.\")\n",
    "# print(df.head())\n",
    "# print(f\"\\nJournal names found in final DataFrame: {df['journal'].notna().sum()}\")\n",
    "# print(f\"Nulls (Original NaN, Not Found, Errors): {df['journal'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated after finalization of \"doi_journal_results.csv\"\n",
    "# # --- Reprocess Nulls ---\n",
    "# print(\"\\n--- Reprocessing Nulls ---\")\n",
    "\n",
    "# # Filter for DOIs with null journal names\n",
    "# null_dois = df[df['journal'].isna()]['doi'].dropna().unique()\n",
    "\n",
    "# if len(null_dois) == 0:\n",
    "#     print(\"No null DOIs found. All records are complete.\")\n",
    "# else:\n",
    "#     print(f\"Found {len(null_dois)} DOIs with null journal names. Reprocessing...\")\n",
    "\n",
    "#     # Split null DOIs into chunks\n",
    "#     null_chunks = np.array_split(null_dois, len(null_dois) // CHUNK_SIZE + 1)\n",
    "#     print(f\"Split null DOIs into {len(null_chunks)} chunks of approx size {CHUNK_SIZE}.\")\n",
    "\n",
    "#     for i, null_chunk in enumerate(null_chunks):\n",
    "#         print(f\"\\n--- Processing Null Chunk {i+1}/{len(null_chunks)} ---\")\n",
    "#         try:\n",
    "#             # Process the chunk asynchronously\n",
    "#             null_results = await process_chunk_async(null_chunk)\n",
    "\n",
    "#             # Update the results map with new data\n",
    "#             for doi, journal in null_results:\n",
    "#                 if doi:  # Ensure DOI is not None before adding\n",
    "#                     all_results_map[doi] = journal\n",
    "\n",
    "#             # Save updated results to the CSV file\n",
    "#             df_to_save = pd.DataFrame(list(all_results_map.items()), columns=['doi', 'journal'])\n",
    "#             df_to_save.to_csv(OUTPUT_FILE, index=False)\n",
    "#             print(f\"--- Progress saved to {OUTPUT_FILE} ({len(all_results_map)} total results). ---\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"!!!!!!!!!! ERROR processing null chunk {i+1}: {e} !!!!!!!!!!!\")\n",
    "#             print(\"Saving progress before potentially stopping...\")\n",
    "#             # Save progress even if an error occurs\n",
    "#             df_to_save = pd.DataFrame(list(all_results_map.items()), columns=['doi', 'journal'])\n",
    "#             df_to_save.to_csv(OUTPUT_FILE, index=False)\n",
    "#             print(f\"--- Progress saved to {OUTPUT_FILE} ({len(all_results_map)} total results). ---\")\n",
    "\n",
    "#     print(\"\\n--- Reprocessing Nulls Complete ---\")\n",
    "\n",
    "# # Map the updated results back to the DataFrame\n",
    "# df_final_results = pd.read_csv(OUTPUT_FILE)\n",
    "# journal_map = pd.Series(df_final_results.journal.values, index=df_final_results.doi)\n",
    "# df['journal'] = df['doi'].map(journal_map)\n",
    "\n",
    "# print(\"\\nMapping complete.\")\n",
    "# print(df.head())\n",
    "# print(f\"\\nJournal names found in final DataFrame: {df['journal'].notna().sum()}\")\n",
    "# print(f\"Nulls (Original NaN, Not Found, Errors): {df['journal'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b742e69",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing entries where doi could not be associated with a journal and journal is NaN\n",
    "# print(f'DataFrame dimension before removing NaN values in Journals: {df.shape}')\n",
    "\n",
    "# df = df[~df[\"journal\"].isna()]\n",
    "\n",
    "# print(f'DataFrame dimension after removing NaN values in Journals: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0c3c3",
   "metadata": {},
   "source": [
    "### Type - DEPRECATED -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d3146",
   "metadata": {},
   "source": [
    "Retriving types from Crossref API by doi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7931f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration Parameters ---\n",
    "# CHUNK_SIZE = 100          # How many DOIs to process in each batch\n",
    "# CONCURRENCY_LIMIT = 4     # Max simultaneous requests\n",
    "# DELAY_PER_REQUEST = 0.01   # Seconds to wait BEFORE each request attempt (e.g., 0.3 = ~3 req/sec max)\n",
    "# RETRY_INITIAL_DELAY = 5   # Seconds to wait after first 429 error\n",
    "# MAX_RETRIES = 3           # Max attempts for a single DOI\n",
    "# DELAY_BETWEEN_CHUNKS = 0.5 # Seconds to pause between processing chunks\n",
    "# OUTPUT_FILE = 'doi_type_results.csv' # File to save progress\n",
    "# USER_AGENT = 'mailto:martina.esser@iu-study.org; purpose: Academic Research Mapping DOIs, Feature Engineering for a Study Project'\n",
    "# # -----------------------------\n",
    "\n",
    "# async def fetch_journal_async(session, doi, max_retries=MAX_RETRIES, initial_delay=RETRY_INITIAL_DELAY):\n",
    "#     \"\"\"\n",
    "#     Asynchronously fetches the type for a given DOI using the Crossref API.\n",
    "#     Includes retry logic, pre-request delay, and respects configured parameters.\n",
    "#     Returns a tuple (doi, item_type). item_type is None if not found or on error.\n",
    "#     \"\"\"\n",
    "#     # --- Polite Pre-Request Delay ---\n",
    "#     await asyncio.sleep(DELAY_PER_REQUEST)\n",
    "#     # -------------------------------\n",
    "\n",
    "#     if pd.isna(doi):\n",
    "#         # Return None for type if DOI is NaN\n",
    "#         return doi, None\n",
    "\n",
    "#     url = f\"https://api.crossref.org/works/{doi}\"\n",
    "#     headers = {'User-Agent': USER_AGENT}\n",
    "\n",
    "#     current_delay = initial_delay\n",
    "#     for attempt in range(max_retries):\n",
    "#         try:\n",
    "#             async with session.get(url, headers=headers, timeout=30) as response:\n",
    "#                 if response.status == 200:\n",
    "#                     try:\n",
    "#                         data = await response.json()\n",
    "#                         # --- CORRECTED LINE ---\n",
    "#                         # Get the 'type' string directly, default to None if not found\n",
    "#                         item_type = data.get('message', {}).get('type', None)\n",
    "#                         # --- Return the full type string ---\n",
    "#                         return doi, item_type\n",
    "#                     except aiohttp.ContentTypeError:\n",
    "#                          print(f\"JSON Decode Error (ContentTypeError) for DOI {doi}. Status: {response.status}. Returning None.\")\n",
    "#                          return doi, None\n",
    "#                     except Exception as json_err:\n",
    "#                          print(f\"JSON Parsing Error for DOI {doi}: {json_err}. Returning None.\")\n",
    "#                          return doi, None\n",
    "#                 elif response.status == 404:\n",
    "#                     # DOI not found, return None for type\n",
    "#                     # print(f\"DOI {doi} not found (404).\") # Optional: uncomment for verbose logging\n",
    "#                     return doi, None\n",
    "#                 elif response.status == 429:\n",
    "#                     print(f\"Rate limit hit (429) for {doi}. Retrying in {current_delay}s (Attempt {attempt+1}/{max_retries})...\")\n",
    "#                     retry_after = response.headers.get(\"Retry-After\")\n",
    "#                     # Use Retry-After header if available, otherwise use exponential backoff\n",
    "#                     sleep_time = int(retry_after) if retry_after and retry_after.isdigit() else current_delay\n",
    "#                     await asyncio.sleep(sleep_time)\n",
    "#                     current_delay *= 2 # Exponential backoff for next potential 429\n",
    "#                 else:\n",
    "#                     # Log other HTTP errors but treat as 'not found' for this purpose\n",
    "#                     print(f\"HTTP Error {response.status} for DOI {doi}. Treating as not found.\")\n",
    "#                     # response.raise_for_status() # Optionally raise error for other bad statuses\n",
    "#                     return doi, None # Return None for type on other HTTP errors\n",
    "\n",
    "#         except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "#             print(f\"Network/Timeout Error for DOI {doi}: {e}. Retrying in {current_delay}s (Attempt {attempt+1}/{max_retries})...\")\n",
    "#             await asyncio.sleep(current_delay)\n",
    "#             current_delay *= 2 # Exponential backoff for network issues\n",
    "#         except Exception as e:\n",
    "#             print(f\"Unexpected error processing DOI {doi}: {e}\")\n",
    "#             # Don't retry on unexpected errors, return None for type\n",
    "#             return doi, None\n",
    "\n",
    "#     print(f\"Failed to fetch DOI {doi} after {max_retries} attempts.\")\n",
    "#     # Failed after all retries, return None for type\n",
    "#     return doi, None\n",
    "\n",
    "\n",
    "# async def process_chunk_async(doi_chunk_list):\n",
    "#     \"\"\"\n",
    "#     Processes a chunk (list) of DOIs asynchronously.\n",
    "#     \"\"\"\n",
    "#     # Limit concurrency at the connector level\n",
    "#     connector = aiohttp.TCPConnector(limit=CONCURRENCY_LIMIT)\n",
    "#     async with aiohttp.ClientSession(connector=connector) as session:\n",
    "#         tasks = [asyncio.create_task(fetch_journal_async(session, doi)) for doi in doi_chunk_list]\n",
    "#         print(f\"  Created {len(tasks)} tasks for this chunk. Fetching...\")\n",
    "#         # Gather results, which will be tuples of (doi, item_type_or_None)\n",
    "#         results = await asyncio.gather(*tasks)\n",
    "#         print(f\"  Chunk fetching complete.\")\n",
    "#         return results # List of (doi, item_type_or_None) tuples\n",
    "\n",
    "# # --- Main Processing Logic with Chunking and Saving ---\n",
    "\n",
    "# # Split DataFrame into chunks based on the 'doi' column\n",
    "# # Ensure we handle potential NaNs gracefully during splitting if needed, though processing logic handles them\n",
    "# list_of_doi_chunks = np.array_split(df['doi'].dropna().unique(), len(df['doi'].dropna().unique()) // CHUNK_SIZE + 1)\n",
    "# print(f\"\\nSplit unique non-null DOIs into {len(list_of_doi_chunks)} chunks of max size {CHUNK_SIZE}.\")\n",
    "\n",
    "# all_results_map = {} # Dictionary to store {doi: item_type}\n",
    "\n",
    "# # --- Check if output file exists to potentially resume ---\n",
    "# if os.path.exists(OUTPUT_FILE):\n",
    "#     print(f\"--- Found existing results file: {OUTPUT_FILE}. Loading previous results. ---\")\n",
    "#     try:\n",
    "#         df_existing = pd.read_csv(OUTPUT_FILE)\n",
    "#         # Handle potential missing 'doi'/'type' columns or empty file\n",
    "#         if 'doi' in df_existing.columns and 'type' in df_existing.columns and not df_existing.empty:\n",
    "#              # Convert loaded data to dictionary, handling potential NaNs in the 'type' column\n",
    "#              all_results_map = pd.Series(df_existing.type.values, index=df_existing.doi).where(pd.notna, None).to_dict()\n",
    "#              print(f\"--- Loaded {len(all_results_map)} results from file. ---\")\n",
    "#         else:\n",
    "#              print(f\"--- Existing file is empty or missing 'doi'/'type' columns. Starting fresh. ---\")\n",
    "#              all_results_map = {} # Ensure it's empty if file is invalid\n",
    "#     except pd.errors.EmptyDataError:\n",
    "#         print(f\"--- Existing file {OUTPUT_FILE} is empty. Starting fresh. ---\")\n",
    "#         all_results_map = {}\n",
    "#     except Exception as e:\n",
    "#         print(f\"--- Error loading existing file {OUTPUT_FILE}: {e}. Starting fresh. ---\")\n",
    "#         all_results_map = {}\n",
    "# # -------------------------------------------------------\n",
    "\n",
    "\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# for i, doi_chunk in enumerate(list_of_doi_chunks):\n",
    "#     chunk_start_time = time.time()\n",
    "#     print(f\"\\n--- Processing Chunk {i+1}/{len(list_of_doi_chunks)} ---\")\n",
    "\n",
    "#     # --- Filter out DOIs already processed ---\n",
    "#     dois_to_process = [doi for doi in doi_chunk if pd.notna(doi) and doi not in all_results_map]\n",
    "#     # -----------------------------------------\n",
    "\n",
    "#     if not dois_to_process:\n",
    "#         print(f\"--- All DOIs in this chunk already processed or invalid. Skipping. ---\")\n",
    "#         continue # Skip to the next chunk\n",
    "\n",
    "#     print(f\"--- Need to process {len(dois_to_process)} DOIs in this chunk. ---\")\n",
    "\n",
    "#     try:\n",
    "#         chunk_results_tuples = await process_chunk_async(dois_to_process) # List of (doi, item_type)\n",
    "\n",
    "#         # Update the main results map\n",
    "#         for doi, item_type in chunk_results_tuples:\n",
    "#             if pd.notna(doi): # Ensure DOI is not None/NaN before adding\n",
    "#                  all_results_map[doi] = item_type # item_type can be None if fetch failed/404\n",
    "\n",
    "#     except RuntimeError as e:\n",
    "#          if \"cannot run current event loop\" in str(e) and 'await' in open(__file__).read():\n",
    "#               print(\"\\nERROR: Detected use of 'await' outside an async function or compatible environment (like Jupyter).\")\n",
    "#               print(\"If running as a standard .py script, please modify the script to use 'asyncio.run(process_chunk_async(dois_to_process))' instead of 'await process_chunk_async(...)' in the main loop.\")\n",
    "#               print(\"Stopping execution.\")\n",
    "#               exit()\n",
    "#          else:\n",
    "#               print(f\"!!!!!!!!!! RUNTIME ERROR processing chunk {i+1}: {e} !!!!!!!!!!!\")\n",
    "#               print(\"Saving progress before potentially stopping...\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"!!!!!!!!!! UNEXPECTED ERROR processing chunk {i+1}: {e} !!!!!!!!!!!\")\n",
    "#         print(\"Saving progress before potentially stopping...\")\n",
    "\n",
    "#     # --- Save Progress After Each Chunk ---\n",
    "#     try:\n",
    "#         df_to_save = pd.DataFrame(list(all_results_map.items()), columns=['doi', 'type'])\n",
    "#         df_to_save.to_csv(OUTPUT_FILE, index=False)\n",
    "#         print(f\"--- Progress saved to {OUTPUT_FILE} ({len(all_results_map)} total results). ---\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"--- ERROR saving progress to {OUTPUT_FILE}: {e} ---\")\n",
    "#     # ------------------------------------\n",
    "\n",
    "#     chunk_end_time = time.time()\n",
    "#     print(f\"--- Chunk {i+1} processing took: {chunk_end_time - chunk_start_time:.2f} seconds ---\")\n",
    "\n",
    "#     # --- Pause Between Chunks ---\n",
    "#     if i < len(list_of_doi_chunks) - 1: # Don't sleep after the last chunk\n",
    "#         print(f\"--- Pausing for {DELAY_BETWEEN_CHUNKS} seconds before next chunk... ---\")\n",
    "#         time.sleep(DELAY_BETWEEN_CHUNKS)\n",
    "#     # --------------------------\n",
    "\n",
    "# total_end_time = time.time()\n",
    "# print(f\"\\n\\n======================================================\")\n",
    "# print(f\"Total processing finished in: {(total_end_time - total_start_time) / 60:.2f} minutes\")\n",
    "# print(f\"Final results saved to {OUTPUT_FILE}\")\n",
    "# print(f\"Total unique DOIs processed and saved: {len(all_results_map)}\")\n",
    "# print(f\"======================================================\")\n",
    "\n",
    "# print(\"\\nMapping results back to the original DataFrame...\")\n",
    "# try:\n",
    "#     df_final_results = pd.read_csv(OUTPUT_FILE)\n",
    "#     type_map = pd.Series(df_final_results.type.values, index=df_final_results.doi)\n",
    "\n",
    "#     # Map values to a new 'type' column in the original df\n",
    "#     df['type'] = df['doi'].map(type_map)\n",
    "\n",
    "#     print(\"Mapping complete.\")\n",
    "#     print(\"\\nDataFrame head with mapped types:\")\n",
    "#     print(df.head())\n",
    "#     print(f\"\\nPublication types found in final DataFrame: {df['type'].notna().sum()}\")\n",
    "#     print(f\"Nulls (Original NaN DOI, Not Found, Errors, or DOI not processed): {df['type'].isna().sum()}\")\n",
    "#     print(f\"\\nValue counts for fetched types:\")\n",
    "#     print(df['type'].value_counts(dropna=False)) # Show counts including NaNs\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: Output file {OUTPUT_FILE} not found. Cannot map results back.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error mapping results back to DataFrame: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b3d9b",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7720d6",
   "metadata": {},
   "source": [
    "Not necessary, no NaNs in types after removal of journal NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5881df8",
   "metadata": {},
   "source": [
    "### Flatten Domain and Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549a589",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Split the 'Domain' column into individual domains and explode into rows\n",
    "df_domains = df['Domain'].str.split(';').explode().str.strip()\n",
    "\n",
    "# Get all unique domains\n",
    "unique_domains = df_domains.unique()\n",
    "\n",
    "# Create binary columns for each unique domain\n",
    "for domain in unique_domains:\n",
    "    df[domain] = df['Domain'].str.contains(domain, case=False, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e92c4",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Split the 'Area' column into individual areas and explode into rows\n",
    "df_areas = df['Area'].str.split(';').explode().str.strip()\n",
    "\n",
    "# Get all unique areas\n",
    "unique_areas = df_areas.unique()\n",
    "\n",
    "# Create a DataFrame with binary columns for each unique area (more memory efficient, many columns...)\n",
    "binary_columns = {area: df['Area'].str.contains(area, case=False, na=False) for area in unique_areas}\n",
    "binary_df = pd.DataFrame(binary_columns)\n",
    "\n",
    "# Concatenate the original DataFrame with the new binary columns\n",
    "df = pd.concat([df, binary_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e568ca",
   "metadata": {},
   "source": [
    "## Text derived Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24bed48",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af495f9",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30dd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['abstract'] = df['abstract'].fillna('')\n",
    "\n",
    "df['abstract'] = df['abstract'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823fdac",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text: str) -> str:\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"  # Return empty string for NaN or non-string inputs\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df['abstract'] = df['abstract'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5a3a0",
   "metadata": {},
   "source": [
    "Standardize Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Standardizing Abbreviations ---\")\n",
    "abbreviation_map = {\n",
    "    # CS / General ML\n",
    "    'machine learning ml': 'machine learning',\n",
    "    'learning ml': 'machine learning',\n",
    "    'ml models': 'machine learning model',\n",
    "    'reinforcement learning rl': 'reinforcement learning',\n",
    "    'artificial intelligence ai': 'artificial intelligence',\n",
    "    'internet things iot': 'internet things',\n",
    "    'convolutional neural network cnn': 'convolutional neural network',\n",
    "    'deep learning dl': 'deep learning',\n",
    "    'deep neural network dnn': 'deep neural network',\n",
    "    'neural networks cnns': 'neural networks',\n",
    "    'language models llms': 'language model',\n",
    "    'language model lm': 'language model',\n",
    "    'language processing nlp': 'natural language processing',\n",
    "    'natural language processing nlp': 'natural language processing',\n",
    "    'federated learning fl': 'federated learning',\n",
    "    'principal component analysis pca': 'principal component analysis',\n",
    "    'structural equation modelling sem': 'structural equation modelling',\n",
    "    'extended reality xr': 'extended reality',\n",
    "    # Physics\n",
    "    'black hole bh': 'black hole',\n",
    "    'dark matter dm': 'dark matter',\n",
    "    'density functional theory dft': 'density functional theory',\n",
    "    'molecular dynamics md': 'molecular dynamics',\n",
    "    # EESS\n",
    "    'computed tomography ct': 'computed tomography',\n",
    "    'magnetic resonance imaging mri': 'magnetic resonance imaging',\n",
    "    'electroencephalography eeg': 'electroencephalography',\n",
    "    'base station bs': 'base station',\n",
    "    'channel state information csi': 'channel state information',\n",
    "    'multipleinput multipleoutput mimo': 'multipleinput multipleoutput',\n",
    "    'signaltonoise ratio snr': 'signaltonoise ratio',\n",
    "    'automatic speech recognition asr': 'automatic speech recognition',\n",
    "    'massive machinetype communications mmtc': 'massive machinetype communications',\n",
    "    'reconfigurable intelligent surface ris': 'reconfigurable intelligent surface',\n",
    "    'user equipment ue': 'user equipment',\n",
    "    # QB\n",
    "    # molecular dynamics md (already listed)\n",
    "    # magnetic resonance imaging mri (already listed)\n",
    "    # electroencephalography eeg (already listed)\n",
    "    # artificial intelligence ai (already listed)\n",
    "    # Econ\n",
    "    # structural equation modelling sem (already listed)\n",
    "    # QF\n",
    "    'agentbased models abms': 'agentbased models',\n",
    "    'agentbased model gabm': 'agentbased model',\n",
    "    'environmental social governance esg': 'environmental social governance',\n",
    "    'value risk var': 'value risk',\n",
    "}\n",
    "\n",
    "def standardize_abbreviations(text, abbreviation_map):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return text, 0  # Return as-is if NaN or not a string, with 0 replacements\n",
    "\n",
    "    replacement_count = 0  # Track the number of replacements\n",
    "    for abbr, full_form in abbreviation_map.items():\n",
    "        if abbr in text:  # Check if the abbreviation exists in the text\n",
    "            text = text.replace(abbr, full_form)  # Replace abbreviations with full forms\n",
    "            replacement_count += 1  # Increment the counter for each replacement\n",
    "    return text, replacement_count\n",
    "\n",
    "# Apply the function to the \"abstract\" column and track replacements\n",
    "adjustment_counts = []\n",
    "df['abstract'], adjustment_counts = zip(*df['abstract'].apply(lambda x: standardize_abbreviations(x, abbreviation_map)))\n",
    "\n",
    "# Print total adjustments\n",
    "total_adjustments = sum(adjustment_counts)\n",
    "print(f\"Total number of adjustments made: {total_adjustments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dbaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils import resample\n",
    "import umap\n",
    "import scipy.sparse\n",
    "from typing import List, Dict, Tuple, Any, Set, Optional\n",
    "import inflect\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.ticker as mtick\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b5c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "#df.to_parquet(\"checkpoint_lemmatized.parquet\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "df = pd.read_parquet(\"checkpoint_lemmatized.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcd29a",
   "metadata": {},
   "source": [
    "#### Keyword Extraction (per domain and month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_monthly_top_keywords(\n",
    "    dataframe: pd.DataFrame,\n",
    "    category_column: str,\n",
    "    date_column: str = 'first_date',\n",
    "    text_column: str = 'abstract',\n",
    "    top_n: int = 30,\n",
    "    boost_bigrams: float = 1.0,\n",
    "    boost_trigrams: float = 2.0,\n",
    "    max_df: float = 0.8,\n",
    "    min_df: int = 1,\n",
    "    ngram_range: Tuple[int, int] = (2, 3)\n",
    ") -> Tuple[List[str], Dict[Any, List[Tuple[str, float]]]]:\n",
    "    \"\"\"\n",
    "    Analyzes text data within a specific category of a DataFrame, grouped by month,\n",
    "    to extract the top N keywords using TF-IDF, with optional boosting for n-grams.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing the data.\n",
    "        category_column (str): The name of the boolean column used to filter the\n",
    "                               DataFrame for the relevant category.\n",
    "        date_column (str): The name of the column containing datetime objects.\n",
    "        text_column (str): The name of the column containing the text data (e.g., abstracts).\n",
    "        top_n (int): The number of top keywords to extract for each month.\n",
    "        boost_bigrams (float): Factor to boost the TF-IDF score of bigrams.\n",
    "        boost_trigrams (float): Factor to boost the TF-IDF score of trigrams.\n",
    "        tfidf_max_df (float): max_df parameter for TfidfVectorizer. Ignore terms\n",
    "                              that appear in more than this fraction of documents.\n",
    "        tfidf_min_df (int): min_df parameter for TfidfVectorizer. Ignore terms\n",
    "                            that appear in less than this number of documents.\n",
    "        tfidf_ngram_range (Tuple[int, int]): ngram_range parameter for TfidfVectorizer.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[Any, List[Tuple[str, float]]]]:\n",
    "            - A list of unique top keywords found across all months.\n",
    "            - A dictionary where keys are the year-month periods and values are\n",
    "              lists of (keyword, score) tuples for that month's top keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        raise TypeError(\"Input 'dataframe' must be a pandas DataFrame.\")\n",
    "    if category_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Column '{category_column}' not found in DataFrame.\")\n",
    "    if date_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Column '{date_column}' not found in DataFrame.\")\n",
    "    if text_column not in dataframe.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "    if not pd.api.types.is_datetime64_any_dtype(dataframe[date_column]):\n",
    "         try:\n",
    "             # Attempt conversion if not already datetime\n",
    "             dataframe[date_column] = pd.to_datetime(dataframe[date_column])\n",
    "             print(f\"Warning: Column '{date_column}' converted to datetime objects.\")\n",
    "         except Exception as e:\n",
    "            raise TypeError(f\"Column '{date_column}' must be of datetime type or convertible to it. Error: {e}\")\n",
    "    if not pd.api.types.is_bool_dtype(dataframe[category_column]):\n",
    "        # Attempt conversion if possible (e.g., 0/1)\n",
    "        try:\n",
    "            dataframe[category_column] = dataframe[category_column].astype(bool)\n",
    "            print(f\"Warning: Column '{category_column}' converted to boolean type.\")\n",
    "        except Exception as e:\n",
    "            raise TypeError(f\"Column '{category_column}' must be of boolean type or convertible to it. Error: {e}\")\n",
    "    if not isinstance(top_n, int) or top_n <= 0:\n",
    "        raise ValueError(\"'top_n' must be a positive integer.\")\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    # Filter for the specified category and create a copy to avoid SettingWithCopyWarning\n",
    "    df_filtered = dataframe[dataframe[category_column]].copy()\n",
    "\n",
    "    # Handle potential NaNs in text column before processing\n",
    "    df_filtered.dropna(subset=[text_column], inplace=True)\n",
    "    df_filtered[text_column] = df_filtered[text_column].astype(str) # Ensure text is string\n",
    "\n",
    "    # Create the year_month period\n",
    "    df_filtered['year_month'] = df_filtered[date_column].dt.to_period('M')\n",
    "\n",
    "    monthly_top_keywords = {} # Dictionary to store results {year_month: [(keyword, score), ...]}\n",
    "\n",
    "    # Sort by year_month to process chronologically\n",
    "    df_filtered = df_filtered.sort_values('year_month')\n",
    "\n",
    "    # --- Group by Month and Analyze ---\n",
    "    # Group by the created year_month period\n",
    "    for year_month, group_df in df_filtered.groupby('year_month'):\n",
    "\n",
    "        # Get the processed text data for the current month\n",
    "        texts_this_month = group_df[text_column]\n",
    "\n",
    "        # Skip if no valid (non-empty) texts for this month\n",
    "        if texts_this_month.empty or texts_this_month.str.strip().eq('').all():\n",
    "            print(f\"No valid text data found for month {year_month}. Skipping.\")\n",
    "            monthly_top_keywords[year_month] = []\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Initialize TF-IDF Vectorizer FOR THIS MONTH'S DATA\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                max_df=max_df,\n",
    "                min_df=min_df,\n",
    "                ngram_range=ngram_range\n",
    "            )\n",
    "\n",
    "            # Fit and transform the texts *for this month*\n",
    "            tfidf_matrix = vectorizer.fit_transform(texts_this_month)\n",
    "\n",
    "            # Get the feature names (keywords) learned from this month's data\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "            # Calculate the sum of TF-IDF scores for each term across all docs in this month\n",
    "            sum_tfidf = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "            # Map scores to feature names\n",
    "            scores = [(feature_names[col], sum_tfidf[0, col]) for col in range(tfidf_matrix.shape[1])]\n",
    "\n",
    "            # Boost N-grams\n",
    "            boosted_scores = []\n",
    "            for term, score in scores:\n",
    "                num_spaces = term.count(' ')\n",
    "                boosted_score = score\n",
    "                if num_spaces == 1: \n",
    "                    boosted_score *= boost_bigrams\n",
    "                elif num_spaces == 2: \n",
    "                    boosted_score *= boost_trigrams\n",
    "\n",
    "                if boosted_score > 0: # Only add terms with a positive boosted score\n",
    "                     boosted_scores.append((term, boosted_score))\n",
    "\n",
    "            # Sort terms by the potentially boosted score in descending order\n",
    "            sorted_scores = sorted(boosted_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Get the top N keywords for the month\n",
    "            top_keywords_this_month = sorted_scores[:top_n]\n",
    "\n",
    "            # Store the results\n",
    "            monthly_top_keywords[year_month] = top_keywords_this_month\n",
    "\n",
    "        except ValueError as e:\n",
    "            # Handle cases where TF-IDF might fail (e.g., all terms are stop words after filtering)\n",
    "            print(f\"Could not process month {year_month} with TF-IDF: {e}\")\n",
    "            monthly_top_keywords[year_month] = []\n",
    "\n",
    "    # --- Collect Unique Keywords ---\n",
    "    unique_keywords_set = set()\n",
    "    for keywords in monthly_top_keywords.values():\n",
    "        unique_keywords_set.update([keyword for keyword, _ in keywords])\n",
    "\n",
    "    unique_keywords_list = sorted(list(unique_keywords_set))\n",
    "\n",
    "    return unique_keywords_list, monthly_top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns_to_process: List[str] = [\n",
    "        \"Physics\",\n",
    "        \"Computer Science\",\n",
    "        \"Statistics\",\n",
    "        \"Mathematics\",\n",
    "        \"Electrical Engineering and Systems Science\",\n",
    "        \"Quantitative Biology\",\n",
    "        \"Economics\",\n",
    "        \"Quantitative Finance\"\n",
    "]\n",
    "\n",
    "domain_specific_params: Dict[str, Dict[str, Any]] = {\n",
    "    \"Physics\": {\"min_df\": 0.0005, \"max_df\": 0.6, \"top_n\": 100}, # Many records in Physics domain\n",
    "    \"Computer Science\": {\"min_df\": 0.0005, \"max_df\": 0.6, \"top_n\": 60}, # Many records in Computer Science domain\n",
    "    \"Statistics\": {\"min_df\": 0.04, \"max_df\": 0.6},\n",
    "    \"Mathematics\": {\"min_df\": 0.01, \"max_df\": 0.6},\n",
    "    \"Electrical Engineering and Systems Science\": {\"min_df\": 0.04, \"max_df\": 0.6}, # Missing keywords mid 2015 - beginning of 2017 even with very unrestrictive parameters\n",
    "    # https://info.arxiv.org/new/eess_announce.html  Electrical Engineering and Systems Science archive (eess) was introduced 18 September 2017\n",
    "    \"Quantitative Biology\": {\"min_df\": 0.04, \"max_df\": 0.6},\n",
    "    \"Economics\": {\"min_df\": 0.06, \"max_df\": 0.6},\n",
    "    \"Quantitative Finance\": {\"min_df\": 0.06, \"max_df\": 0.6},\n",
    "}\n",
    "\n",
    "domain_keywords: Dict[str, List[str]] = {}\n",
    "\n",
    "print(\"Starting keyword extraction across multiple categories...\")\n",
    "\n",
    "# Loop through each category\n",
    "for category_col in category_columns_to_process:\n",
    "    print(f\"\\nProcessing category: '{category_col}'...\")\n",
    "\n",
    "    # Check if the category column exists in the DataFrame\n",
    "    if category_col not in df.columns:\n",
    "        print(f\"Warning: Column '{category_col}' not found in DataFrame. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Check if there's any data for this category\n",
    "    if not df[category_col].any():\n",
    "        print(f\"No entries found for category '{category_col}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get the domain-specific parameters for this category\n",
    "        params = domain_specific_params.get(category_col, {})\n",
    "        top_n = params.get(\"top_n\", 30)\n",
    "        min_df = params.get(\"min_df\", 1)\n",
    "        max_df = params.get(\"max_df\", 0.8)\n",
    "        ngram_range = params.get(\"ngram_range\", (2, 3))\n",
    "\n",
    "        # Call the keyword extraction function with the specific parameters\n",
    "        unique_keywords_list, monthly_keywords_dict = extract_monthly_top_keywords(\n",
    "            dataframe=df,\n",
    "            category_column=category_col,\n",
    "            date_column='first_date',\n",
    "            text_column='abstract',\n",
    "            top_n=top_n,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            ngram_range=ngram_range\n",
    "        )\n",
    "\n",
    "        # Store the unique keywords for this category in the dictionary\n",
    "        domain_keywords[category_col] = unique_keywords_list\n",
    "\n",
    "        print(f\"Finished processing '{category_col}'. Added {len(unique_keywords_list)} unique keywords.\")\n",
    "        if not unique_keywords_list:\n",
    "            print(f\"(No keywords met the TF-IDF criteria for '{category_col}')\")\n",
    "\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Error processing category '{category_col}': {e}. Skipping this category.\")\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors\n",
    "        print(f\"An unexpected error occurred while processing category '{category_col}': {e}. Skipping this category.\")\n",
    "\n",
    "print(\"\\n--- Keyword Extraction Complete ---\")\n",
    "\n",
    "# Print the total number of keywords per domain\n",
    "for domain, keywords in domain_keywords.items():\n",
    "    print(f\"Domain: {domain}, Total Keywords: {len(keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3654734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint a saving\n",
    "\n",
    "with open('domain_keywords.json', 'w') as json_file:\n",
    "    json.dump(domain_keywords, json_file, indent=4)\n",
    "    \n",
    "\n",
    "export_data = {\n",
    "    \"unique_domains\": unique_domains.tolist(),\n",
    "    \"unique_areas\": unique_areas.tolist(),\n",
    "    \"unique_keywords_list\": unique_keywords_list\n",
    "}\n",
    "\n",
    "with open(\"unique_data.json\", \"w\") as json_file:\n",
    "    json.dump(export_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint b loading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils import resample\n",
    "import umap\n",
    "import scipy.sparse\n",
    "from typing import List, Dict, Tuple, Any, Set, Optional\n",
    "import inflect\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.ticker as mtick\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "df = pd.read_parquet(\"checkpoint_lemmatized.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "with open('domain_keywords.json', 'r') as json_file:\n",
    "    domain_keywords = json.load(json_file)\n",
    "\n",
    "with open(\"unique_data.json\", \"r\") as json_file:\n",
    "    imported_data = json.load(json_file)\n",
    "\n",
    "unique_domains = np.array(imported_data[\"unique_domains\"])\n",
    "unique_areas = np.array(imported_data[\"unique_areas\"])\n",
    "unique_keywords_list = imported_data[\"unique_keywords_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf17a2e",
   "metadata": {},
   "source": [
    "#### Keyword Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = []\n",
    "\n",
    "\n",
    "for domain, keyword_list in domain_keywords.items():\n",
    "    if not isinstance(keyword_list, list):\n",
    "        print(f\"  - Warning: Keywords for domain '{domain}' is not a list. Skipping this domain for DataFrame.\")\n",
    "        continue\n",
    "    if not keyword_list:\n",
    "        print(f\"  - No keywords for domain '{domain}'. This domain will have no entries in DataFrame.\")\n",
    "        # If you want to represent domains with no keywords, you could add a row with None or empty string for keyword\n",
    "        # df_data.append({'domain': domain, 'keyword': None}) # Example\n",
    "        continue\n",
    "    for keyword in keyword_list:\n",
    "        # Append a dictionary for each keyword, associating it with its domain\n",
    "        df_data.append({'domain': domain, 'keyword': keyword})\n",
    "\n",
    "domain_keywords_df = pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Remove Generic Phrases ---\n",
    "print(\"\\n--- Removing Generic Phrases ---\")\n",
    "generic_phrases_to_remove = [\n",
    "    'et al', 'results indicate', 'results suggest', 'results demonstrate',\n",
    "    'paper propose', 'paper present', 'work propose', 'introduce novel', 'propose new',\n",
    "    'experimental results', 'numerical results', 'simulation results',\n",
    "    'demonstrate effectiveness', 'effectiveness proposed', 'proposed method',\n",
    "    'proposed approach', 'proposed model', 'proposed algorithm', 'proposed scheme',\n",
    "    'recent years', 'widely used', 'commonly used', 'wide range', 'large number',\n",
    "    'based on', 'compared to', 'address problem', 'paper deals', 'paper investigates',\n",
    "    'paper study', 'study investigates', 'study shows', 'study aims', 'aim paper',\n",
    "    'demonstrate proposed', 'results proposed', 'findings indicate', 'findings suggest',\n",
    "    'existing methods', 'existing approaches', 'existing literature', 'previous work',\n",
    "    'previous studies', 'recent work', 'recent studies', 'recent developments',\n",
    "    'provide evidence', 'paper provide', 'study provides', 'present paper',\n",
    "    'main result', 'key role', 'important role', 'crucial role', 'significant role',\n",
    "    'better understanding', 'deeper understanding', 'valuable insights', 'shed light',\n",
    "    'future research', 'possible future research', 'directions future research',\n",
    "    'case study', 'numerical examples', 'experimental data', 'real data', 'real world',\n",
    "    'publicly available', 'code available','good agreement', 'high accuracy', 'better performance',\n",
    "    'superior performance', 'computational cost', 'computational complexity', 'computationally efficient',\n",
    "    'state art', 'stateoftheart methods','stateoftheart performance','results obtained', 'results confirm',\n",
    "    'results reveal', 'address challenges', 'address issue', 'address gap',\n",
    "    'apply method', 'apply results', 'approach based', 'method based', 'framework based',\n",
    "    'consider problem', 'develop novel', 'develop framework',\n",
    "    'empirical evidence', 'empirical application', 'empirical analysis',\n",
    "    'establish existence', 'evaluate performance', 'examine potential',\n",
    "    'explain sustainability', 'explore key', 'extract important',\n",
    "    'findings emphasize', 'focus specifically', 'gain insights',\n",
    "    'highlight importance', 'illustrate method', 'implementable pricebased',\n",
    "    'improve performance', 'increase probability', 'investigate effect',\n",
    "    'make use', 'obtain new', 'outperforms existing',\n",
    "    'play crucial role', 'plays important role', 'present comprehensive',\n",
    "    'provide new', 'purpose paper', 'purpose research',\n",
    "    'showcase potential', 'solve problem', 'study aimed', 'study demonstrates',\n",
    "    'study examines', 'suggests evaluate', 'theoretical analysis', 'theoretical results',\n",
    "    'understand relationship', 'use cases', 'using data', 'using numerical', 'et al phys',\n",
    "    'data used', 'question answering', 'study propose', 'use case', 'present new',\n",
    "    'evidence suggests', 'model used', 'plays crucial', 'plays crucial role', 'present novel',\n",
    "    'propose novel', 'paves way', 'significant challenge', 'outperforms state-of-the-art', 'introduces novel',\n",
    "    'prove existence', 'new results', 'previous results', 'new examples', 'new method',\n",
    "    'novel approach', 'similar results', 'open problem', 'new proof', 'recent results',\n",
    "    'closely related', 'previously known', 'recently introduced', 'new approach', 'present new',\n",
    "    'gives rise', 'model based', 'extensive experiments', 'upper bound', 'recently proposed',\n",
    "    'present study', 'standard model', 'work present', 'pave way', 'present results', 'present work',\n",
    "    'provide valuable insights', 'results provide', 'results pave way', 'standard model sm', 'taken account', \n",
    "    'taking account', 'best practices', 'challenging task', 'demonstrate effectiveness proposed', 'demonstrate proposed method',\n",
    "    'effectiveness proposed approach', 'effectiveness proposed model', 'effectiveness proposed method',\n",
    "    'novel method', 'findings reveal', 'experimental results demonstrate', 'experimental results proposed', 'experiments conducted', 'experiments demonstrate',\n",
    "    'extensive experimental results', 'extensive experiments conducted', 'extensive experiments demonstrate', 'extensive experiments realworld', 'paper consider',\n",
    "    'paper explores', 'paper introduce novel', 'paper introduces novel', 'paper present novel', 'paper presents comprehensive',\n",
    "    'paper presents new', 'paper presents novel', 'paper propose new', 'paper propose novel', 'paper proposes novel',\n",
    "    'result suggest', 'result demonstrate', 'result indicate', 'result obtained', 'result pave way', 'result provide', 'result suggest',\n",
    "    'address challenge', 'address challenge propose', 'address issue paper', 'address issue propose', 'address limitation propose',\n",
    "    'existing approach', 'existing method', 'experimental research data', 'experimental result', 'experimental result demonstrate',\n",
    "    'experimental result proposed', 'extensive experiment', 'paper address problem', 'paper describes', 'paper introduce',\n",
    "    'paper introduce new', 'paper investigate', 'paper present new', 'paper study problem', 'present novel approach',\n",
    "    'research performance', 'result demonstrate', 'result demonstrate proposed', 'result obtained', 'result proposed',\n",
    "    'result proposed method', 'data set', 'data set different', 'data point', 'data based',\n",
    "    'experimental result', 'data collection', 'introduce new', 'paper aim', 'paper analysis',\n",
    "    'paper analyze', 'paper deal', 'paper introduces', 'paper investigate', 'paper model approximate',\n",
    "    'paper provides', 'recent advance', 'recent development', 'recent year', 'recently used',\n",
    "    'significantly outperforms', 'special case', 'open question', 'stateoftheart method', 'provide example',\n",
    "    'provide explicit', 'provide insight', 'provide new proof', 'recent advancement', 'recent paper',\n",
    "    'recent result', 'result hold', 'result numerical example', 'result obtained', 'result paper',\n",
    "    'study conducted', 'study investigate', 'study investigated', 'study present', 'study reported'\n",
    "\n",
    "    # Synonym / Incomplete expressions section\n",
    "    'deep neural', 'large language', 'neural networks', 'data sets', 'machine learning algorithms',\n",
    "    'models llms', 'convolutional neural', 'van der', 'der waals', 'deep neural', 'language models',\n",
    "    'intelligence ai', 'deep learning models', 'learning models', 'resonance imaging', 'magnetic resonance',\n",
    "    'network cnn', 'neural network cnn', 'learning dl', 'markov chain monte', 'chain monte carlo',\n",
    "    'monte carlo mcmc', 'chain monte', 'carlo mcmc', 'natural language', 'quantum manybody',\n",
    "    'manybody systems', 'galactic nuclei agn', 'galactic nuclei', 'active galactic', 'nuclei agn',\n",
    "    'quantum information', 'information processing', 'carlo simulations', 'md simulations', 'blood glucose values',\n",
    "    'computer vision tasks', 'datasets demonstrate', 'deep learningbased', 'deep reinforcement', 'learning model'\n",
    "]\n",
    "\n",
    "def remove_keywords_with_prefixes(df, column, prefixes):\n",
    "    \"\"\"\n",
    "    Removes rows from the DataFrame where the specified column's values start with any of the given prefixes.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column (str): The column to check for prefixes.\n",
    "        prefixes (list): A list of prefixes to check.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the filtered rows.\n",
    "    \"\"\"\n",
    "    pattern = f\"^({'|'.join(prefixes)})\"\n",
    "    mask = ~df[column].str.lower().str.match(pattern)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "count_before_generic_removal = len(domain_keywords_df)\n",
    "mask = ~domain_keywords_df['keyword'].str.lower().isin(generic_phrases_to_remove)\n",
    "refined_df = domain_keywords_df[mask].copy()\n",
    "print(f\"Keyword count after removing generic phrases: {len(refined_df)} (Removed {count_before_generic_removal - len(refined_df)})\")\n",
    "\n",
    "count_before_prefix_removal = len(refined_df)\n",
    "prefixes_to_remove = ['result', 'paper', 'provide', 'recent', 'address', 'model ', 'method ', 'significantly', 'consider ', 'considers ', 'considered ']\n",
    "refined_df = remove_keywords_with_prefixes(refined_df, 'keyword', prefixes_to_remove)\n",
    "print(f\"Keyword count after removing prefixes: {len(refined_df)} (Removed {count_before_prefix_removal - len(refined_df)})\")\n",
    "\n",
    "# --- Display final counts ---\n",
    "print(\"\\nFinal keyword counts per domain:\")\n",
    "print(refined_df['domain'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export refined_df to a text file\n",
    "#refined_df.to_csv('refined_keywords.txt', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa77695",
   "metadata": {},
   "source": [
    "#### Keyword Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2272f",
   "metadata": {},
   "source": [
    "##### Overall in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette_hex = [\"#89043d\",\"#8a817c\",\"#bcb8b1\",\"#f4f3ee\",\"#e0afa0\"]\n",
    "\n",
    "def palette_color_func(word, **kwargs):\n",
    "    return random.choice(custom_palette_hex)\n",
    "\n",
    "domain_keyword_frequencies: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "# Loop through unique domains in the refined keyword DataFrame\n",
    "for domain_name in refined_df['domain'].unique():\n",
    "    print(f\"\\nProcessing: '{domain_name}'...\")\n",
    "\n",
    "    # Get refined keywords for this domain\n",
    "    refined_keywords_list = refined_df[refined_df['domain'] == domain_name]['keyword'].tolist()\n",
    "\n",
    "    # Filter original DataFrame for abstracts of this domain\n",
    "    category_mask = df[domain_name] == True\n",
    "    category_abstracts = df.loc[category_mask, \"abstract\"].astype(str).fillna('')\n",
    "\n",
    "    if category_abstracts.empty or not refined_keywords_list:\n",
    "        print(f\"  - Skipping '{domain_name}' (no abstracts or keywords).\")\n",
    "        continue\n",
    "\n",
    "    # Calculate frequencies\n",
    "    frequencies = {}\n",
    "    lower_abstracts_series = category_abstracts.str.lower()\n",
    "    for keyword in refined_keywords_list:\n",
    "        escaped_keyword = re.escape(keyword.lower())\n",
    "        # Use regex=True because re.escape is used.\n",
    "        count = lower_abstracts_series.str.count(escaped_keyword).sum()\n",
    "        if count > 0:\n",
    "            frequencies[keyword] = int(count)\n",
    "\n",
    "    domain_keyword_frequencies[domain_name] = frequencies\n",
    "\n",
    "    if not frequencies:\n",
    "        print(f\"  - Skipping '{domain_name}' (no keywords found in abstracts).\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  - Generating word cloud for '{domain_name}'...\")\n",
    "\n",
    "    # Generate and display Word Cloud\n",
    "    wordcloud_generator = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color='black',\n",
    "        collocations=False,\n",
    "        min_font_size=10,\n",
    "        #colormap='magma',\n",
    "        color_func=palette_color_func,\n",
    "        random_state=None\n",
    "    ).generate_from_frequencies(frequencies)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(wordcloud_generator, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Domain: {domain_name}', fontsize=18)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export domain_keyword_frequencies to a JSON file\n",
    "with open('domain_keyword_frequencies.json', 'w') as json_file:\n",
    "    json.dump(domain_keyword_frequencies, json_file, indent=4)\n",
    "\n",
    "# # Reimport domain_keyword_frequencies from the JSON file\n",
    "# with open('domain_keyword_frequencies.json', 'r') as json_file:\n",
    "#     domain_keyword_frequencies = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfa446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary local variables\n",
    "try:\n",
    "    del category_abstracts, category_mask, count, count_before_generic_removal, count_before_prefix_removal\n",
    "    del custom_palette_hex, df_data, domain, escaped_keyword\n",
    "    del frequencies, generic_phrases_to_remove, imported_data, json_file\n",
    "    del keyword, keywords, lower_abstracts_series, mask, prefixes_to_remove\n",
    "    del refined_keywords_list, wordcloud_generator\n",
    "except NameError as e:\n",
    "    print(f\"Variable not found: {e}\")\n",
    "else:\n",
    "    print(\"All specified variables deleted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce38a7",
   "metadata": {},
   "source": [
    "##### Category specific - DEPRECATED - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9602af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out keywords based on cross-domain frequency (keywords appearing in >1 domains more than 20 times)\n",
    "\n",
    "freq_df = pd.DataFrame([{'domain': dom, 'keyword': kw, 'frequency': fq}\n",
    "                        for dom, kw_fq_dict in domain_keyword_frequencies.items()\n",
    "                        for kw, fq in kw_fq_dict.items()])\n",
    "\n",
    "# 2. Identify keywords frequent (>=5) in >1 domain\n",
    "domain_counts = freq_df.query('frequency >= 20').groupby('keyword')['domain'].nunique()\n",
    "keywords_to_exclude = domain_counts[domain_counts > 1].index\n",
    "\n",
    "# 3. Filter the original refined_df to exclude these keywords\n",
    "domain_specific_keywords_df = refined_df[~refined_df['keyword'].isin(keywords_to_exclude)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain_name in domain_specific_keywords_df['domain'].unique():\n",
    "    print(f\"\\nProcessing: '{domain_name}'...\")\n",
    "\n",
    "    # Get the list of domain-specific keywords for this domain\n",
    "    specific_keywords_set = set(domain_specific_keywords_df[\n",
    "        domain_specific_keywords_df['domain'] == domain_name\n",
    "    ]['keyword'])\n",
    "\n",
    "    # Get the original frequencies for ONLY these specific keywords\n",
    "    # Check if the domain exists in the original frequency dictionary\n",
    "    if domain_name not in domain_keyword_frequencies:\n",
    "        print(f\"  - Skipping '{domain_name}' (no original frequencies found).\")\n",
    "        continue\n",
    "\n",
    "    # Filter the original frequencies for the current domain's specific keywords\n",
    "    frequencies_for_cloud = {\n",
    "        keyword: freq\n",
    "        for keyword, freq in domain_keyword_frequencies[domain_name].items()\n",
    "        if keyword in specific_keywords_set and freq > 0 \n",
    "    }\n",
    "\n",
    "    print(f\"  - Generating word cloud for '{domain_name}' with {len(frequencies_for_cloud)} specific keywords...\")\n",
    "\n",
    "    # Generate and display Word Cloud using the filtered frequencies\n",
    "    wordcloud_generator = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color='black',\n",
    "        collocations=False,\n",
    "        min_font_size=10,\n",
    "        color_func=palette_color_func,\n",
    "        random_state=None\n",
    "    ).generate_from_frequencies(frequencies_for_cloud)\n",
    "\n",
    "    # Display Plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(wordcloud_generator, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Domain: {domain_name}', fontsize=18)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71e30c",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5e9a9",
   "metadata": {},
   "source": [
    "### Default Variables and Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d26a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N_KEYWORDS_PER_DOMAIN = 400 \n",
    "UMAP_N_COMPONENTS = 15         \n",
    "UMAP_N_NEIGHBORS = 10          \n",
    "UMAP_MIN_DIST = 0.1            \n",
    "UMAP_METRIC = 'euclidean'      \n",
    "RANDOM_STATE = 42              \n",
    "SILHOUETTE_SAMPLE_SIZE = 10000 \n",
    "K_RANGE_FOR_EVALUATION = range(2, 25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbceecdc",
   "metadata": {},
   "source": [
    "### Features for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0115bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Keyword Selection -----\n",
    "\n",
    "domain_top_keywords_map: Dict[str, List[str]] = {}\n",
    "# Set to store the global union of all top N keywords\n",
    "final_keyword_list_set = set()\n",
    "\n",
    "# Check if domain_keyword_frequencies exists and is populated\n",
    "if 'domain_keyword_frequencies' not in locals() or not domain_keyword_frequencies:\n",
    "    print(\"Error: 'domain_keyword_frequencies' dictionary not found or empty. Cannot select top keywords.\")\n",
    "    final_keyword_list = [] # Initialize empty list\n",
    "else:\n",
    "    # Iterate through the domains present in the frequency dictionary\n",
    "    for domain_name, keyword_freqs in domain_keyword_frequencies.items():\n",
    "        print(f\"  Processing domain: {domain_name}\")\n",
    "\n",
    "        if not keyword_freqs:\n",
    "            print(f\"    No keywords/frequencies found for domain '{domain_name}'.\")\n",
    "            domain_top_keywords_map[domain_name] = [] # Store empty list for this domain\n",
    "            continue\n",
    "\n",
    "        # Sort keywords in this domain by frequency (descending)\n",
    "        valid_items = [(kw, fq) for kw, fq in keyword_freqs.items() if isinstance(fq, (int, float))]\n",
    "        sorted_keywords = sorted(valid_items, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        # Select the top N keywords for this specific domain\n",
    "        top_n_for_domain = [kw for kw, freq in sorted_keywords[:TOP_N_KEYWORDS_PER_DOMAIN]]\n",
    "        print(f\"    Selected {len(top_n_for_domain)} keywords for '{domain_name}'.\")\n",
    "\n",
    "        # Store this list in the map\n",
    "        domain_top_keywords_map[domain_name] = top_n_for_domain\n",
    "\n",
    "        # Add these keywords to the global set for vectorizer vocabulary\n",
    "        final_keyword_list_set.update(top_n_for_domain)\n",
    "\n",
    "    # Convert the final global set to a sorted list for consistent column order in TfidfVectorizer\n",
    "    final_keyword_list = sorted(list(final_keyword_list_set))\n",
    "    print(f\"\\nCreated map 'domain_top_keywords_map' with top keywords per domain.\")\n",
    "    print(f\"Created global list 'final_keyword_list' with {len(final_keyword_list)} unique keywords across all domains (for vectorizer).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fffbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Create Binary Keyword Features -----\n",
    "\n",
    "if not final_keyword_list:\n",
    "     print(\"Warning: final_keyword_list is empty. No keyword features will be created.\")\n",
    "     keyword_cols_created = []\n",
    "else:\n",
    "    print(f\"\\n--- Creating Binary Features for {len(final_keyword_list)} Keywords ---\")\n",
    "\n",
    "    # Ensure abstract is string type and handle potential NaNs\n",
    "    lower_abstracts = df['abstract'].astype(str).fillna('').str.lower()\n",
    "\n",
    "    # --- Configure TfidfVectorizer ---\n",
    "    max_ngram_length = 1\n",
    "    if final_keyword_list:\n",
    "         max_ngram_length = max(len(kw.split()) for kw in final_keyword_list)\n",
    "\n",
    "    print(f\"  Configuring TfidfVectorizer (max_ngram={max_ngram_length})...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        vocabulary=final_keyword_list,\n",
    "        lowercase=True,\n",
    "        binary=True,\n",
    "        use_idf=False,\n",
    "        norm=None,\n",
    "        ngram_range=(1, max_ngram_length)\n",
    "    )\n",
    "\n",
    "    print(\"  Applying vectorizer to abstracts...\")\n",
    "    X_keywords_sparse = vectorizer.fit_transform(lower_abstracts)\n",
    "    print(f\"  Vectorizer finished. Output shape (sparse): {X_keywords_sparse.shape}\")\n",
    "\n",
    "    # --- Create DataFrame from Sparse Matrix ---\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    keyword_cols_created = [f'kw_{name}' for name in feature_names] # These are the columns added\n",
    "    print(f\"  Creating DataFrame from sparse matrix for {len(keyword_cols_created)} keyword features...\")\n",
    "    try:\n",
    "        df_keywords = pd.DataFrame.sparse.from_spmatrix(\n",
    "            X_keywords_sparse,\n",
    "            index=df.index,\n",
    "            columns=keyword_cols_created\n",
    "        )\n",
    "        print(\"  Keyword DataFrame created.\")\n",
    "\n",
    "        # --- Concatenate with original DataFrame ---\n",
    "        cols_to_drop_from_df = [col for col in keyword_cols_created if col in df.columns]\n",
    "        if cols_to_drop_from_df:\n",
    "             print(f\"  Dropping existing columns from df before concat: {cols_to_drop_from_df}\")\n",
    "             df = df.drop(columns=cols_to_drop_from_df)\n",
    "\n",
    "        df = pd.concat([df, df_keywords], axis=1)\n",
    "        # Convert sparse keyword columns to int (0/1) AFTER concat if needed by downstream steps\n",
    "        # This might increase memory usage significantly.\n",
    "        # df[keyword_cols_created] = df[keyword_cols_created].astype(int)\n",
    "        print(f\"Concatenated keyword features. New df shape: {df.shape}\")\n",
    "\n",
    "    except MemoryError:\n",
    "        print(\"\\nError: MemoryError encountered while creating keyword DataFrame.\")\n",
    "        print(\"Consider reducing TOP_N_KEYWORDS_PER_DOMAIN or using algorithms accepting sparse input.\")\n",
    "        keyword_cols_created = [] # Prevent errors later\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred creating keyword DataFrame: {e}\")\n",
    "        keyword_cols_created = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Final Feature Lists -----\n",
    "\n",
    "# --- Metadata Features ---\n",
    "metadata_features = ['number_of_authors'] + list(unique_areas)\n",
    "# type_dummies = pd.get_dummies(df['type'], prefix='type', drop_first=False, dummy_na=False)\n",
    "# df = pd.concat([df, type_dummies], axis=1)\n",
    "# metadata_features.extend(type_dummies.columns.tolist())\n",
    "\n",
    "# --- Keyword Features ---\n",
    "print(f\"Keyword features defined: {len(keyword_cols_created)} columns starting with 'kw_'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ccb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint a saving\n",
    "\n",
    "for col_name in df.columns:\n",
    "    if pd.api.types.is_sparse(df[col_name].dtype):\n",
    "        df[col_name] = df[col_name].sparse.to_dense()\n",
    "\n",
    "df.to_parquet(\"checkpoint_with_keywords.parquet\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "variables_to_export = {\n",
    "    \"domain_top_keywords_map\": domain_top_keywords_map,\n",
    "    \"final_keyword_list\": final_keyword_list,\n",
    "    \"final_keyword_list_set\": list(final_keyword_list_set),\n",
    "    \"keyword_cols_created\": keyword_cols_created,\n",
    "    \"metadata_features\": metadata_features,\n",
    "    \"unique_areas\": unique_areas.tolist(),\n",
    "    \"unique_domains\": unique_domains.tolist(),\n",
    "    \"unique_keywords_list\": unique_keywords_list,\n",
    "}\n",
    "\n",
    "with open(\"checkpoint_variables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(variables_to_export, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc84fd",
   "metadata": {},
   "source": [
    "# CONSIDER DELETING FROM HERE WHEN \"Clustering.ipynb\" IS FINALIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ec4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint b loading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import scipy.sparse\n",
    "from typing import List, Dict, Tuple, Any, Set, Optional\n",
    "import inflect\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import pickle\n",
    "import hdbscan\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import textwrap\n",
    "\n",
    "df = pd.read_parquet(\"checkpoint_with_keywords.parquet\", engine=\"pyarrow\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "keyword_column_names = [col for col in df.columns if col.startswith('kw_')]\n",
    "\n",
    "for col_name in keyword_column_names:\n",
    "    if col_name in df.columns:\n",
    "        current_dtype = df[col_name].dtype\n",
    "        if pd.api.types.is_numeric_dtype(current_dtype):\n",
    "            df[col_name] = df[col_name].astype(pd.SparseDtype(current_dtype, fill_value=0.0))\n",
    "        else:\n",
    "            print(f\"Column '{col_name}' is not numeric, skipping sparse conversion.\")\n",
    "    else:\n",
    "        print(f\"Keyword column '{col_name}' not found in loaded DataFrame.\")\n",
    "\n",
    "\n",
    "with open(\"checkpoint_variables.pkl\", \"rb\") as f:\n",
    "    loaded_variables = pickle.load(f)\n",
    "\n",
    "domain_top_keywords_map = loaded_variables[\"domain_top_keywords_map\"]\n",
    "final_keyword_list = loaded_variables[\"final_keyword_list\"]\n",
    "final_keyword_list_set = set(loaded_variables[\"final_keyword_list_set\"])\n",
    "keyword_cols_created = loaded_variables[\"keyword_cols_created\"]\n",
    "metadata_features = loaded_variables[\"metadata_features\"]\n",
    "unique_areas = np.array(loaded_variables[\"unique_areas\"])\n",
    "unique_domains = np.array(loaded_variables[\"unique_domains\"])\n",
    "unique_keywords_list = loaded_variables[\"unique_keywords_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results per domain\n",
    "domain_cluster_results = {}\n",
    "# Add a new column for sub-cluster labels, initialize with NaN or -1\n",
    "df['sub_cluster'] = pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48761b85",
   "metadata": {},
   "source": [
    "### Overall Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c8484",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File Paths ---\n",
    "DATA_FILE = \"checkpoint_with_keywords.parquet\"\n",
    "VARIABLES_FILE = \"checkpoint_variables.pkl\"\n",
    "\n",
    "# --- Pipeline Parameters ---\n",
    "# PCA Configuration\n",
    "PCA_N_COMPONENTS = 150\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# UMAP Configuration (for clustering)\n",
    "UMAP_CLUSTERING_PARAMS = {\n",
    "    'n_neighbors': 30,\n",
    "    'n_components': 15,\n",
    "    'min_dist': 0.0,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "# UMAP Configuration (for 2D visualization)\n",
    "UMAP_VISUALIZATION_PARAMS = {\n",
    "    'n_neighbors': 30,\n",
    "    'n_components': 2,\n",
    "    'min_dist': 0.1,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "# HDBSCAN Configuration\n",
    "HDBSCAN_PARAMS = {\n",
    "    'min_cluster_size': 50,\n",
    "    'min_samples': 25,\n",
    "    'metric': 'euclidean',\n",
    "    'cluster_selection_method': 'eom',\n",
    "    'gen_min_span_tree': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fe0f6",
   "metadata": {},
   "source": [
    "Data Loading and Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading data from '{DATA_FILE}'...\")\n",
    "df = pd.read_parquet(DATA_FILE, engine=\"pyarrow\")\n",
    "print(f\"DataFrame loaded. Shape: {df.shape}\")\n",
    "\n",
    "# Load the helper variables (column lists)\n",
    "print(f\"Loading feature lists from '{VARIABLES_FILE}'...\")\n",
    "with open(VARIABLES_FILE, \"rb\") as f:\n",
    "    loaded_variables = pickle.load(f)\n",
    "metadata_features = loaded_variables[\"metadata_features\"]\n",
    "unique_domains = np.array(loaded_variables[\"unique_domains\"])\n",
    "keyword_cols_created = loaded_variables[\"keyword_cols_created\"]\n",
    "print(\"Helper variables loaded.\")\n",
    "\n",
    "# Assemble the full feature set\n",
    "print(\"Assembling the final feature set for clustering...\")\n",
    "\n",
    "# Ensure all feature columns exist in the DataFrame\n",
    "domain_features = [col for col in unique_domains if col in df.columns]\n",
    "meta_features = [col for col in metadata_features if col in df.columns]\n",
    "keyword_features = [col for col in keyword_cols_created if col in df.columns]\n",
    "\n",
    "# Combine all feature lists\n",
    "all_features = meta_features + domain_features + keyword_features\n",
    "all_features = sorted(list(set(all_features)))  # Get unique sorted list\n",
    "print(f\"Total number of features: {len(all_features)}\")\n",
    "print(f\" - Metadata & Area features: {len(meta_features)}\")\n",
    "print(f\" - Domain features: {len(domain_features)}\")\n",
    "print(f\" - Keyword features: {len(keyword_features)}\")\n",
    "\n",
    "# Create the feature matrix X\n",
    "X = df[all_features]\n",
    "\n",
    "# Handle Missing Values\n",
    "# Impute with median for numeric columns. This is a robust strategy.\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"NaNs found. Imputing with column medians...\")\n",
    "    X = X.fillna(X.median())\n",
    "else:\n",
    "    print(\"No NaNs found in the feature matrix.\")\n",
    "\n",
    "# Convert to NumPy array for scikit-learn\n",
    "print(\"Converting feature matrix to NumPy array...\")\n",
    "X_np = X.to_numpy(dtype=np.float32)\n",
    "print(f\"Final feature matrix 'X_np' created with shape: {X_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8a0b0",
   "metadata": {},
   "source": [
    "Dimensionality Reduction and Clustering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb89451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Scaling ---\n",
    "print(\"--- Step 1: Scaling data with StandardScaler ---\")\n",
    "start_time = time.time()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_np)\n",
    "end_time = time.time()\n",
    "print(f\"Scaling completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e908671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: PCA ---\n",
    "print(f\"\\n--- Step 2: Applying PCA to reduce to {PCA_N_COMPONENTS} components ---\")\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=PCA_N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"PCA completed in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Shape after PCA: {X_pca.shape}\")\n",
    "print(f\"Total explained variance by {pca.n_components_} components: {np.sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db870b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: UMAP ---\n",
    "print(f\"\\n--- Step 3: Applying UMAP to reduce to {UMAP_CLUSTERING_PARAMS['n_components']} dimensions for clustering ---\")\n",
    "start_time = time.time()\n",
    "umap_reducer_clustering = umap.UMAP(**UMAP_CLUSTERING_PARAMS)\n",
    "X_umap_clustering = umap_reducer_clustering.fit_transform(X_pca)\n",
    "end_time = time.time()\n",
    "print(f\"UMAP for clustering completed in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Shape after UMAP: {X_umap_clustering.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: HDBSCAN Clustering ---\n",
    "print(\"\\n--- Step 4: Applying HDBSCAN to find clusters ---\")\n",
    "start_time = time.time()\n",
    "clusterer = hdbscan.HDBSCAN(**HDBSCAN_PARAMS)\n",
    "cluster_labels = clusterer.fit_predict(X_umap_clustering)\n",
    "end_time = time.time()\n",
    "print(f\"HDBSCAN completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add cluster labels to DataFrame ---\n",
    "df['cluster_label'] = cluster_labels\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = np.sum(cluster_labels == -1)\n",
    "print(\"\\n--- Clustering Results ---\")\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise} ({n_noise / len(df) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea8bd6",
   "metadata": {},
   "source": [
    "Visualization of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b872216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create 2D UMAP embedding for visualization ---\n",
    "print(\"\\n--- Creating 2D UMAP embedding for visualization ---\")\n",
    "start_time = time.time()\n",
    "umap_reducer_viz = umap.UMAP(**UMAP_VISUALIZATION_PARAMS)\n",
    "X_umap_viz = umap_reducer_viz.fit_transform(X_pca)\n",
    "end_time = time.time()\n",
    "print(f\"2D UMAP for visualization completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Add 2D coordinates to DataFrame for plotting\n",
    "df['umap_x'] = X_umap_viz[:, 0]\n",
    "df['umap_y'] = X_umap_viz[:, 1]\n",
    "\n",
    "# %%\n",
    "print(\"--- Generating interactive cluster plot ---\")\n",
    "\n",
    "# Prepare data for Plotly\n",
    "plot_df = df.copy()\n",
    "plot_df['cluster_label_str'] = plot_df['cluster_label'].astype(str)\n",
    "plot_df.loc[plot_df['cluster_label'] == -1, 'cluster_label_str'] = 'Noise'\n",
    "\n",
    "# Get cluster sizes for the legend\n",
    "cluster_sizes = plot_df['cluster_label_str'].value_counts().reset_index()\n",
    "cluster_sizes.columns = ['cluster_label_str', 'count']\n",
    "plot_df = pd.merge(plot_df, cluster_sizes, on='cluster_label_str')\n",
    "plot_df['legend_entry'] = plot_df['cluster_label_str'] + ' (' + plot_df['count'].astype(str) + ')'\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a203e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clustered points first\n",
    "clustered_data = plot_df[plot_df['cluster_label'] != -1].sort_values('cluster_label')\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=clustered_data['umap_x'],\n",
    "    y=clustered_data['umap_y'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=clustered_data['cluster_label'],\n",
    "        colorscale='Viridis',  # A nice colorscale for clusters\n",
    "        showscale=False,\n",
    "        size=3,\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    customdata=clustered_data[['id', 'title', 'cluster_label']],\n",
    "    hovertemplate='<b>Title:</b> %{customdata[1]}<br>' +\n",
    "                  '<b>ID:</b> %{customdata[0]}<br>' +\n",
    "                  '<b>Cluster:</b> %{customdata[2]}<br>' +\n",
    "                  'UMAP-X: %{x:.3f}<br>UMAP-Y: %{y:.3f}<extra></extra>',\n",
    "    name='Clusters'\n",
    "))\n",
    "\n",
    "# Plot noise points on top, in grey\n",
    "noise_data = plot_df[plot_df['cluster_label'] == -1]\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=noise_data['umap_x'],\n",
    "    y=noise_data['umap_y'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color='lightgrey',\n",
    "        size=2,\n",
    "        opacity=0.4\n",
    "    ),\n",
    "    customdata=noise_data[['id', 'title']],\n",
    "    hovertemplate='<b>Title:</b> %{customdata[1]}<br>' +\n",
    "                  '<b>ID:</b> %{customdata[0]}<br>' +\n",
    "                  '<b>Cluster:</b> Noise<br>' +\n",
    "                  'UMAP-X: %{x:.3f}<br>UMAP-Y: %{y:.3f}<extra></extra>',\n",
    "    name=f\"Noise ({len(noise_data)})\"\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f'Global Clustering Results: {n_clusters} Clusters Found',\n",
    "    xaxis_title='UMAP Dimension 1',\n",
    "    yaxis_title='UMAP Dimension 2',\n",
    "    height=800,\n",
    "    legend_title_text='Cluster Labels',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Cluster Size Distribution\n",
    "# A quick look at the distribution of records across the identified clusters.\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "cluster_counts = df[df['cluster_label'] != -1]['cluster_label'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')\n",
    "plt.title('Number of Records per Cluster (excluding noise)', fontsize=16)\n",
    "plt.xlabel('Cluster ID', fontsize=12)\n",
    "plt.ylabel('Number of Records', fontsize=12)\n",
    "\n",
    "if len(cluster_counts) > 50:\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    # Show every 5th tick label to avoid clutter\n",
    "    for index, label in enumerate(plt.gca().get_xticklabels()):\n",
    "        if index % 5 != 0:\n",
    "            label.set_visible(False)\n",
    "else:\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197d59a",
   "metadata": {},
   "source": [
    "### Per Domain Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a744991",
   "metadata": {},
   "source": [
    "#### Variables and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_UMAP_PARAMS = {\n",
    "    'n_components': 15,\n",
    "    'n_neighbors': 10,\n",
    "    'min_dist': 0.1,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': 42,\n",
    "    'verbose': True,\n",
    "    #'low_memory': True\n",
    "}\n",
    "DEFAULT_KMEANS_PARAMS = {\n",
    "    'init': 'k-means++',\n",
    "    'n_init': 'auto',\n",
    "    'max_iter': 300,\n",
    "    'random_state': 42\n",
    "}\n",
    "DEFAULT_K_RANGE = range(2, 301)\n",
    "DEFAULT_SILHOUETTE_SAMPLE_SIZE = 5000\n",
    "RANDOM_STATE = 42\n",
    "ROLLING_WINDOW = 6\n",
    "\n",
    "EXCLUSION_COLOR = 'lightgrey'\n",
    "\n",
    "COLORS = {\n",
    "    0: '#e6194b',\n",
    "    1: '#3cb44b',\n",
    "    2: '#e6beff',\n",
    "    3: '#4363d8',\n",
    "    4: '#f58231',\n",
    "    5: '#911eb4',\n",
    "    6: '#46f0f0',\n",
    "    7: '#f032e6',\n",
    "    8: '#f6bb0c',\n",
    "    9: '#fabebe',\n",
    "    10: '#008080',\n",
    "    11: '#808000',\n",
    "    12: '#9a6324',\n",
    "    13: '#aaffc3',\n",
    "    14: '#ffd8b1',\n",
    "    15: '#800000',\n",
    "    16: '#ffe119',\n",
    "    17: '#000075',\n",
    "    18: '#a9a9ff',\n",
    "    19: '#ff4500',\n",
    "    20: '#000000',\n",
    "    21: '#808080',\n",
    "    22: '#bcf60c',\n",
    "    23: '#fffac8',\n",
    "    24: '#dda0dd',\n",
    "    25: '#556b2f'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_colors(num_colors: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a dictionary of random hexadecimal color codes.\n",
    "\n",
    "    Args:\n",
    "        num_colors (int): The number of random colors to generate.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are integers (0 to num_colors-1) and values are hex color codes.\n",
    "    \"\"\"\n",
    "    colors = {}\n",
    "    for i in range(num_colors):\n",
    "        # Generate a random color in hexadecimal format\n",
    "        color = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
    "        colors[i] = color\n",
    "    return colors\n",
    "\n",
    "COLORS_2 = generate_random_colors(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Functions to Prepare Data and Reduce Dimensionality ---\n",
    "def prepare_domain_data(\n",
    "    domain_name: str,\n",
    "    df: pd.DataFrame,\n",
    "    metadata_features: List[str],\n",
    "    keyword_cols_created: List[str],\n",
    "    domain_top_keywords_map: Dict[str, List[str]]\n",
    ") -> Optional[Tuple[pd.DataFrame, np.ndarray, List[str]]]:\n",
    "    \"\"\"\n",
    "    Filters data for a specific domain, selects features based on metadata\n",
    "    and the Top N keywords specific to that domain, handles NaNs, and converts\n",
    "    features to a dense NumPy array.\n",
    "\n",
    "    Args:\n",
    "        domain_name: The name of the domain column to filter by.\n",
    "        df: The main DataFrame containing all data and features.\n",
    "        metadata_features: List of metadata column names to include.\n",
    "        keyword_cols_created: List of all 'kw_...' column names that were created.\n",
    "        domain_top_keywords_map: Dictionary mapping domain name to its list of Top N keywords.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - df_domain: DataFrame subset for the domain.\n",
    "        - X_domain_dense: Dense NumPy array of selected metadata and domain-specific keyword features.\n",
    "        - final_features_used: List of the actual feature columns used (metadata + keywords).\n",
    "        Returns None if no data or relevant features are found, or if errors occur.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Preparing Data for Domain: {domain_name} ---\")\n",
    "\n",
    "    # --- Filter Data ---\n",
    "    if domain_name not in df.columns:\n",
    "        print(f\"  Error: Domain column '{domain_name}' not found in DataFrame.\")\n",
    "        return None\n",
    "    domain_mask = df[domain_name] == True\n",
    "    df_domain = df[domain_mask].copy()\n",
    "\n",
    "    if df_domain.empty:\n",
    "        print(f\"  No data found for domain '{domain_name}'.\")\n",
    "        return None\n",
    "    print(f\"  Found {len(df_domain)} documents.\")\n",
    "\n",
    "    # --- Feature Selection ---\n",
    "\n",
    "    # 1. Select Metadata Features\n",
    "    # Ensure metadata columns exist in the df_domain subset\n",
    "    selected_metadata_cols = [col for col in metadata_features if col in df_domain.columns]\n",
    "    if len(selected_metadata_cols) != len(metadata_features):\n",
    "        missing_meta = [col for col in metadata_features if col not in df_domain.columns]\n",
    "        print(f\"  Warning: The following metadata columns were requested but not found in df_domain: {missing_meta}\")\n",
    "\n",
    "    # 2. Select Domain-Specific Top N Keyword Features\n",
    "    top_keywords_for_this_domain = domain_top_keywords_map.get(domain_name, [])\n",
    "\n",
    "    if not top_keywords_for_this_domain and not selected_metadata_cols: # If no keywords AND no metadata\n",
    "        print(f\"  No Top N keywords defined for domain '{domain_name}' and no metadata features selected.\")\n",
    "        return None\n",
    "    elif not top_keywords_for_this_domain:\n",
    "        print(f\"  Warning: No Top N keywords defined for domain '{domain_name}'. Proceeding with metadata only.\")\n",
    "\n",
    "\n",
    "    domain_specific_kw_cols = [\n",
    "        f'kw_{kw}' for kw in top_keywords_for_this_domain\n",
    "        if f'kw_{kw}' in keyword_cols_created\n",
    "    ]\n",
    "\n",
    "    missing_kw_cols = [\n",
    "        f'kw_{kw}' for kw in top_keywords_for_this_domain\n",
    "        if f'kw_{kw}' not in keyword_cols_created\n",
    "    ]\n",
    "    if missing_kw_cols:\n",
    "        print(f\"  Warning: The following Top N keyword columns were expected but not found: {missing_kw_cols}\")\n",
    "\n",
    "    if not domain_specific_kw_cols and not selected_metadata_cols:\n",
    "        print(f\"  No valid keyword columns found for Top N keywords and no metadata features for '{domain_name}'.\")\n",
    "        return None\n",
    "    elif not domain_specific_kw_cols:\n",
    "        print(f\"  Warning: No valid keyword columns found for Top N keywords for '{domain_name}'. Proceeding with metadata only.\")\n",
    "\n",
    "\n",
    "    # 3. Combine Metadata and Keyword Features\n",
    "    # Ensure no overlap between metadata and keyword column names (unlikely but good practice)\n",
    "    final_features_used = selected_metadata_cols + [\n",
    "        kw_col for kw_col in domain_specific_kw_cols if kw_col not in selected_metadata_cols\n",
    "    ]\n",
    "\n",
    "    print(f\"  Using {len(selected_metadata_cols)} metadata features and \"\n",
    "          f\"{len([kw_col for kw_col in domain_specific_kw_cols if kw_col not in selected_metadata_cols])} \"\n",
    "          f\"domain-specific keyword features. Total: {len(final_features_used)} features.\")\n",
    "\n",
    "    # Ensure all selected features actually exist in df_domain before creating X_domain\n",
    "    # This is a final check, especially if df_domain is a very small subset.\n",
    "    valid_current_features = [col for col in final_features_used if col in df_domain.columns]\n",
    "    if len(valid_current_features) != len(final_features_used):\n",
    "         missing_from_subset = [col for col in final_features_used if col not in df_domain.columns]\n",
    "         print(f\"  Warning: Some selected features missing from df_domain subset (should be rare): {missing_from_subset}\")\n",
    "    if not valid_current_features:\n",
    "         print(f\"  Error: No valid feature columns left after checking df_domain.\")\n",
    "         return None\n",
    "\n",
    "    X_domain = df_domain[valid_current_features]\n",
    "\n",
    "    # --- Handle NaNs (More likely now with metadata) and Convert to Dense ---\n",
    "    try:\n",
    "        # Check for NaNs, especially in metadata columns\n",
    "        has_nans = False\n",
    "        # Iterate over a copy of columns to avoid issues if X_domain is modified\n",
    "        for col in list(X_domain.columns):\n",
    "            if pd.isna(X_domain[col]).any():\n",
    "                has_nans = True\n",
    "                print(f\"  Warning: NaNs found in feature column '{col}'. Imputing...\")\n",
    "                if X_domain[col].dtype == object or pd.api.types.is_string_dtype(X_domain[col].dtype):\n",
    "                    # For object/string columns (e.g., if a metadata feature was unexpectedly string)\n",
    "                    # Impute with mode or a placeholder. For clustering, numeric is better.\n",
    "                    # This case should ideally be handled by one-hot encoding earlier.\n",
    "                    mode_val = X_domain[col].mode()\n",
    "                    fill_value = mode_val[0] if not mode_val.empty else \"Unknown\"\n",
    "                    print(f\"    Imputing object column '{col}' with mode: {fill_value}\")\n",
    "                    X_domain[col] = X_domain[col].fillna(fill_value)\n",
    "                    # If it's still object, it might cause issues with StandardScaler.\n",
    "                    # Consider converting to categorical codes or one-hot encoding if this happens.\n",
    "                elif pd.api.types.is_numeric_dtype(X_domain[col].dtype):\n",
    "                    # For numeric columns (includes int, float, potentially sparse numeric)\n",
    "                    median_val = X_domain[col].median()\n",
    "                    print(f\"    Imputing numeric column '{col}' with median: {median_val}\")\n",
    "                    X_domain[col] = X_domain[col].fillna(median_val)\n",
    "                else:\n",
    "                    # For other types (e.g. boolean if not 0/1 already)\n",
    "                    # Impute with the most frequent value (mode) or 0/False\n",
    "                    mode_val = X_domain[col].mode()\n",
    "                    fill_value = mode_val[0] if not mode_val.empty else 0\n",
    "                    print(f\"    Imputing column '{col}' of type {X_domain[col].dtype} with mode: {fill_value}\")\n",
    "                    X_domain[col] = X_domain[col].fillna(fill_value)\n",
    "\n",
    "\n",
    "        # Final check for NaNs after imputation\n",
    "        if has_nans and pd.isna(X_domain).any().any():\n",
    "             print(\"  Error: NaNs still present after imputation. Examine problematic columns.\")\n",
    "             # Identify columns with remaining NaNs\n",
    "             remaining_nan_cols = X_domain.columns[X_domain.isna().any()].tolist()\n",
    "             print(f\"    Columns with remaining NaNs: {remaining_nan_cols}\")\n",
    "             return None\n",
    "\n",
    "\n",
    "        print(\"  Converting feature matrix to dense NumPy array...\")\n",
    "        # StandardScaler expects float input, so convert.\n",
    "        # Binary features (0/1) will remain 0.0/1.0.\n",
    "        X_domain_dense = X_domain.to_numpy(dtype=np.float32)\n",
    "        print(f\"  Created dense feature matrix with shape: {X_domain_dense.shape}\")\n",
    "\n",
    "        # Return the list of columns actually used\n",
    "        return df_domain, X_domain_dense, valid_current_features\n",
    "\n",
    "    except MemoryError:\n",
    "        print(f\"  Error: MemoryError converting features for domain '{domain_name}' to dense array.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Error preparing features for domain '{domain_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "def reduce_dimensionality(\n",
    "    X_domain_dense: np.ndarray,\n",
    "    umap_params: Dict = DEFAULT_UMAP_PARAMS\n",
    ") -> Optional[Tuple[np.ndarray, StandardScaler, umap.UMAP]]:\n",
    "    \"\"\"\n",
    "    Applies StandardScaler and UMAP to the dense feature matrix.\n",
    "\n",
    "    Args:\n",
    "        X_domain_dense: Dense NumPy array of features for the domain.\n",
    "        umap_params: Dictionary of parameters for UMAP.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - X_domain_reduced_umap: NumPy array of reduced features.\n",
    "        - scaler_domain: The fitted StandardScaler object.\n",
    "        - reducer_domain: The fitted UMAP object.\n",
    "        Returns None if input is invalid.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Scaling and Applying UMAP ---\")\n",
    "    if X_domain_dense is None or X_domain_dense.shape[0] == 0:\n",
    "        print(\"  Error: Input feature matrix is empty or None.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # --- Scaling ---\n",
    "        print(\"  Scaling domain features...\")\n",
    "        scaler_domain = StandardScaler()\n",
    "        X_domain_scaled = scaler_domain.fit_transform(X_domain_dense)\n",
    "\n",
    "        # --- UMAP ---\n",
    "        print(\"  Applying UMAP...\")\n",
    "        # Ensure verbose is handled correctly (might be passed via umap_params)\n",
    "        # We'll set verbose=True during fit for progress, then restore original setting\n",
    "        verbose_setting = umap_params.get('verbose', False)\n",
    "        current_umap_params = umap_params.copy() # Don't modify the default dict\n",
    "        current_umap_params['verbose'] = True # Show progress during fit\n",
    "\n",
    "        reducer_domain = umap.UMAP(**current_umap_params)\n",
    "\n",
    "        X_domain_reduced_umap = reducer_domain.fit_transform(X_domain_scaled)\n",
    "\n",
    "        # Restore original verbose setting if needed for the object itself\n",
    "        reducer_domain.verbose = verbose_setting\n",
    "\n",
    "        print(f\"  UMAP complete. Reduced shape: {X_domain_reduced_umap.shape}\")\n",
    "        return X_domain_reduced_umap, scaler_domain, reducer_domain\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during scaling or UMAP: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Clustering Functions ---\n",
    "# K-Means\n",
    "def evaluate_k(\n",
    "    X_reduced_umap: np.ndarray,\n",
    "    domain_name: str,\n",
    "    k_range: range = DEFAULT_K_RANGE,\n",
    "    sample_size: int = DEFAULT_SILHOUETTE_SAMPLE_SIZE,\n",
    "    kmeans_params: Dict = DEFAULT_KMEANS_PARAMS\n",
    ") -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Calculates and plots Elbow and Silhouette scores to help choose k.\n",
    "\n",
    "    Args:\n",
    "        X_reduced_umap: The UMAP-reduced feature matrix for the domain.\n",
    "        domain_name: Name of the domain for plot titles.\n",
    "        k_range: Range of k values to test.\n",
    "        sample_size: Sample size for Silhouette calculation.\n",
    "        kmeans_params: Dictionary of base parameters for KMeans.\n",
    "\n",
    "    Returns:\n",
    "        The suggested optimal k based on the highest Silhouette score,\n",
    "        or the smallest k if scores cannot be calculated. Returns None on error.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Determining Optimal k ---\")\n",
    "    if X_reduced_umap is None or X_reduced_umap.shape[0] < 2:\n",
    "        print(\"  Error: Reduced feature matrix is empty or has less than 2 samples.\")\n",
    "        return None\n",
    "\n",
    "    inertia_domain = []\n",
    "    silhouette_scores_domain = []\n",
    "    k_values_domain = list(k_range)\n",
    "    actual_sample_size = min(sample_size, X_reduced_umap.shape[0])\n",
    "\n",
    "    # Prepare sample for Silhouette\n",
    "    if actual_sample_size > 1:\n",
    "        # Use the random_state from kmeans_params if available, otherwise global RANDOM_STATE\n",
    "        sampling_random_state = kmeans_params.get('random_state', RANDOM_STATE)\n",
    "        X_sample = resample(X_reduced_umap, n_samples=actual_sample_size, random_state=sampling_random_state, replace=False)\n",
    "        print(f\"  Using sample size {actual_sample_size} for Silhouette score.\")\n",
    "    else:\n",
    "        X_sample = X_reduced_umap # Use all data if too small\n",
    "        print(\"  Warning: Dataset too small for sampling, using all data for Silhouette.\")\n",
    "\n",
    "    print(\"  Calculating Inertia and Silhouette scores...\")\n",
    "    for k in k_values_domain:\n",
    "        # Create KMeans instance with combined default and specific k\n",
    "        current_kmeans_params = {**kmeans_params, 'n_clusters': k}\n",
    "        kmeans_eval = KMeans(**current_kmeans_params)\n",
    "\n",
    "        # Elbow (fit on full reduced data)\n",
    "        kmeans_eval.fit(X_reduced_umap)\n",
    "        inertia_domain.append(kmeans_eval.inertia_)\n",
    "\n",
    "        # Silhouette (predict on sample using model fitted on full data)\n",
    "        if actual_sample_size > 1:\n",
    "            cluster_labels_sample = kmeans_eval.predict(X_sample)\n",
    "            if len(np.unique(cluster_labels_sample)) > 1: # Score requires at least 2 labels\n",
    "                try:\n",
    "                    score = silhouette_score(X_sample, cluster_labels_sample)\n",
    "                    silhouette_scores_domain.append(score)\n",
    "                except Exception as sil_e:\n",
    "                    print(f\"    Warning: Error calculating Silhouette for k={k}: {sil_e}\")\n",
    "                    silhouette_scores_domain.append(-1) # Invalid score\n",
    "            else:\n",
    "                # Only 1 cluster label found in the sample prediction\n",
    "                silhouette_scores_domain.append(-1) # Invalid score\n",
    "        else:\n",
    "             # Cannot calculate Silhouette if sample size <= 1\n",
    "             silhouette_scores_domain.append(-1)\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax1 = plt.subplots(figsize=(25, 8))\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Number of clusters (k)')\n",
    "    ax1.set_ylabel('Inertia', color=color)\n",
    "    ax1.plot(k_values_domain, inertia_domain, marker='o', color=color, label='Inertia')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_xticks(k_values_domain)\n",
    "    ax1.grid(True)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Check if silhouette_scores_domain has valid scores\n",
    "    valid_indices = [i for i, s in enumerate(silhouette_scores_domain) if s > -1]\n",
    "    suggested_k = k_values_domain[0]\n",
    "\n",
    "    if valid_indices:\n",
    "        valid_k_domain = [k_values_domain[i] for i in valid_indices]\n",
    "        valid_scores_domain = [silhouette_scores_domain[i] for i in valid_indices]\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:red'\n",
    "        ax2.set_ylabel('Avg Silhouette Score (Sample)', color=color)\n",
    "        ax2.plot(valid_k_domain, valid_scores_domain, marker='x', linestyle='--', color=color, label='Silhouette (Sample)')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # Suggest best k based on Silhouette\n",
    "        best_k_silhouette_idx_in_valid = np.argmax(valid_scores_domain)\n",
    "        suggested_k = valid_k_domain[best_k_silhouette_idx_in_valid]\n",
    "        print(f\"  Suggested k = {suggested_k} (Max Silhouette Score: {valid_scores_domain[best_k_silhouette_idx_in_valid]:.4f})\")\n",
    "\n",
    "        # Combine legends\n",
    "        lines, labels = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(lines + lines2, labels + labels2, loc='center right')\n",
    "    else:\n",
    "        print(\"  Could not calculate valid Silhouette scores.\")\n",
    "        ax1.legend(loc='upper right')\n",
    "\n",
    "\n",
    "    plt.title(f'Elbow & Silhouette Analysis for Domain: {domain_name}')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return suggested_k\n",
    "\n",
    "def apply_clustering(\n",
    "    X_reduced_umap: np.ndarray,\n",
    "    chosen_k: int,\n",
    "    df: pd.DataFrame,\n",
    "    df_domain_index: pd.Index,\n",
    "    domain_name: str,\n",
    "    kmeans_params: Dict = DEFAULT_KMEANS_PARAMS\n",
    ") -> Optional[Tuple[pd.DataFrame, KMeans]]:\n",
    "    \"\"\"\n",
    "    Applies K-Means with the chosen k and adds sub-cluster labels to the main DataFrame.\n",
    "\n",
    "    Args:\n",
    "        X_reduced_umap: The UMAP-reduced feature matrix for the domain.\n",
    "        chosen_k: The selected number of clusters.\n",
    "        df: The main DataFrame (will be modified).\n",
    "        df_domain_index: The index of the rows belonging to the current domain in the main df.\n",
    "        domain_name: Name of the domain for context.\n",
    "        kmeans_params: Dictionary of base parameters for KMeans.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - df: The modified main DataFrame with 'sub_cluster' labels added/updated.\n",
    "        - kmeans_model: The fitted KMeans object.\n",
    "        Returns None on error.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Applying K-Means with k={chosen_k} for Domain: {domain_name} ---\")\n",
    "    if X_reduced_umap is None or chosen_k < 2:\n",
    "        print(\"  Error: Invalid input data or chosen_k < 2.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Create KMeans instance\n",
    "        current_kmeans_params = {**kmeans_params, 'n_clusters': chosen_k}\n",
    "        kmeans_model = KMeans(**current_kmeans_params)\n",
    "\n",
    "        # Fit and predict\n",
    "        cluster_labels_domain = kmeans_model.fit_predict(X_reduced_umap)\n",
    "\n",
    "        # Add labels back to the main DataFrame using the correct index\n",
    "        # Ensure 'sub_cluster' column exists\n",
    "        if 'sub_cluster' not in df.columns:\n",
    "             # Initialize with a type that can hold NaNs and integers if needed\n",
    "             df['sub_cluster'] = pd.Series(index=df.index, dtype='Int64') # Use nullable integer type\n",
    "\n",
    "        # Assign labels using .loc - this modifies the main df\n",
    "        df.loc[df_domain_index, 'sub_cluster'] = cluster_labels_domain\n",
    "\n",
    "        print(f\"  Added/Updated sub-cluster labels for '{domain_name}' in main DataFrame.\")\n",
    "        print(f\"  Sub-cluster counts for {domain_name}:\")\n",
    "        # Count labels just assigned\n",
    "        print(pd.Series(cluster_labels_domain).value_counts().sort_index())\n",
    "\n",
    "        return df, kmeans_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during K-Means clustering or labeling: {e}\")\n",
    "        return None\n",
    "# HDBSCAN\n",
    "def apply_hdbscan_clustering(\n",
    "    X_reduced_umap: np.ndarray,\n",
    "    df: pd.DataFrame,\n",
    "    df_domain_index: pd.Index,\n",
    "    domain_name: str,\n",
    "    min_cluster_size: int = 15,\n",
    "    min_samples: Optional[int] = None, # If None, defaults to min_cluster_size\n",
    "    metric: str = 'euclidean',\n",
    "    cluster_selection_method: str = 'eom', # Excess of Mass\n",
    "    allow_single_cluster: bool = False # HDBSCAN parameter\n",
    ") -> Optional[Tuple[pd.DataFrame, hdbscan.HDBSCAN, int]]:\n",
    "    \"\"\"\n",
    "    Applies HDBSCAN clustering and adds sub-cluster labels to the main DataFrame.\n",
    "    Noise points are labeled as -1.\n",
    "\n",
    "    Args:\n",
    "        X_reduced_umap: The UMAP-reduced feature matrix for the domain.\n",
    "        df: The main DataFrame (will be modified).\n",
    "        df_domain_index: The index of the rows belonging to the current domain in the main df.\n",
    "        domain_name: Name of the domain for context.\n",
    "        min_cluster_size: The minimum size of clusters.\n",
    "        min_samples: The number of samples in a neighborhood for a point\n",
    "                     to be considered as a core point.\n",
    "        metric: The metric to use when calculating distance between instances in a feature array.\n",
    "        cluster_selection_method: The method used to select clusters from the condensed tree.\n",
    "        allow_single_cluster: Whether to allow HDBSCAN to return a single cluster.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - df: The modified main DataFrame with 'sub_cluster' labels added/updated.\n",
    "        - hdbscan_model: The fitted HDBSCAN object.\n",
    "        - n_clusters_found: The number of clusters found (excluding noise).\n",
    "        Returns None on error.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Applying HDBSCAN for Domain: {domain_name} ---\")\n",
    "    if X_reduced_umap is None or X_reduced_umap.shape[0] < 2 : # HDBSCAN needs at least 2 samples\n",
    "        print(\"  Error: Invalid input data for HDBSCAN (must have at least 2 samples).\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        hdbscan_model = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric=metric,\n",
    "            cluster_selection_method=cluster_selection_method,\n",
    "            allow_single_cluster=allow_single_cluster,\n",
    "            gen_min_span_tree=True # Useful for some diagnostics or alternative cluster extraction\n",
    "        )\n",
    "\n",
    "        cluster_labels_domain = hdbscan_model.fit_predict(X_reduced_umap)\n",
    "\n",
    "        # Calculate number of clusters found (excluding noise label -1)\n",
    "        unique_labels = set(cluster_labels_domain)\n",
    "        n_clusters_found = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "        \n",
    "        print(f\"  HDBSCAN found {n_clusters_found} cluster(s) and {np.sum(cluster_labels_domain == -1)} noise points.\")\n",
    "\n",
    "        if 'sub_cluster' not in df.columns:\n",
    "             df['sub_cluster'] = pd.Series(index=df.index, dtype='Int64')\n",
    "\n",
    "        df.loc[df_domain_index, 'sub_cluster'] = cluster_labels_domain\n",
    "\n",
    "        print(f\"  Added/Updated sub-cluster labels for '{domain_name}' in main DataFrame.\")\n",
    "        print(f\"  Sub-cluster counts for {domain_name} (HDBSCAN):\")\n",
    "        print(pd.Series(cluster_labels_domain).value_counts().sort_index())\n",
    "\n",
    "        return df, hdbscan_model, n_clusters_found\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during HDBSCAN clustering or labeling: {e}\")\n",
    "        return None\n",
    "   \n",
    "\n",
    "# --- Function to Identify Emerging Clusters ---\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "def identify_emerging_cluster_ids(\n",
    "    df: pd.DataFrame,\n",
    "    domain_name: str,\n",
    "    cluster_column: str = 'sub_cluster',\n",
    "    date_column: str = 'first_date',\n",
    "    recent_months_window: int = 12,\n",
    "    min_papers_recent_period: int = 1, # Min papers for a cluster in recent period to be considered\n",
    "    emerging_ratio_threshold: float = 1.5, # Ratio of recent_prop/baseline_prop\n",
    "    emerging_diff_threshold: float = 0.01, # Absolute increase in proportion (e.g., 0.01 = 1% increase)\n",
    "    newly_active_min_recent_prop: float = 0.001 # Min proportion in recent period to be \"newly active\"\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Identifies emerging cluster IDs within a specific domain by comparing their\n",
    "    proportion of publications in a recent period versus a baseline period.\n",
    "    The computation is done directly on the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The main DataFrame containing paper data, including\n",
    "                           domain boolean columns, cluster labels, and dates.\n",
    "        domain_name (str): The name of the boolean column representing the domain.\n",
    "        cluster_column (str): The name of the column containing cluster labels (defaults to 'sub_cluster').\n",
    "        date_column (str): The name of the column containing publication dates.\n",
    "        recent_months_window (int): The number of months to consider for the \"recent\" period.\n",
    "        min_papers_recent_period (int): Minimum number of papers a cluster must have\n",
    "                                        in the recent period to be considered potentially emerging.\n",
    "        emerging_ratio_threshold (float): Minimum ratio of (recent_prop / baseline_prop)\n",
    "                                          for a cluster to be considered emerging (if baseline > 0).\n",
    "        emerging_diff_threshold (float): Minimum absolute increase in proportion\n",
    "                                         (recent_prop - baseline_prop) for a cluster to be\n",
    "                                         considered emerging.\n",
    "        newly_active_min_recent_prop (float): Minimum proportion in the recent period for a\n",
    "                                              cluster with no baseline presence to be considered \"newly active\".\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of cluster IDs identified as emerging or newly active.\n",
    "                   Returns an empty list if data is insufficient or errors occur.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Identifying Emerging Cluster IDs for Domain: {domain_name} ---\")\n",
    "    emerging_cluster_ids: List[int] = []\n",
    "\n",
    "    # --- 1. Input Validation and Data Preparation ---\n",
    "    if domain_name not in df.columns:\n",
    "        print(f\"  Error: Domain column '{domain_name}' not found.\")\n",
    "        return emerging_cluster_ids\n",
    "    if cluster_column not in df.columns:\n",
    "        print(f\"  Error: Cluster column '{cluster_column}' not found.\")\n",
    "        return emerging_cluster_ids\n",
    "    if date_column not in df.columns:\n",
    "        print(f\"  Error: Date column '{date_column}' not found.\")\n",
    "        return emerging_cluster_ids\n",
    "\n",
    "    # Filter for the domain and operate on a copy\n",
    "    df_domain_analysis = df[df[domain_name] == True].copy()\n",
    "\n",
    "    if df_domain_analysis.empty:\n",
    "        print(f\"  No data found for domain '{domain_name}'.\")\n",
    "        return emerging_cluster_ids\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_domain_analysis[date_column]):\n",
    "        try:\n",
    "            df_domain_analysis.loc[:, date_column] = pd.to_datetime(df_domain_analysis[date_column])\n",
    "        except Exception as e:\n",
    "            print(f\"  Error converting date column '{date_column}' to datetime: {e}\")\n",
    "            return emerging_cluster_ids\n",
    "\n",
    "    # Drop rows where cluster label is NA and ensure integer type\n",
    "    df_domain_analysis.dropna(subset=[cluster_column], inplace=True)\n",
    "    try:\n",
    "        df_domain_analysis.loc[:, cluster_column] = df_domain_analysis[cluster_column].astype(int)\n",
    "        # Filter out noise cluster if it's labeled as -1 (common for HDBSCAN)\n",
    "        # For K-Means, this usually isn't an issue unless -1 is a valid label.\n",
    "        df_domain_analysis = df_domain_analysis[df_domain_analysis[cluster_column] != -1]\n",
    "    except ValueError:\n",
    "        print(f\"  Warning: Could not convert cluster column '{cluster_column}' to int. Proceeding with original type.\")\n",
    "\n",
    "    if df_domain_analysis.empty:\n",
    "        print(f\"  No valid clustered data found for domain '{domain_name}' after filtering noise/NA.\")\n",
    "        return emerging_cluster_ids\n",
    "\n",
    "    # --- 2. Define Time Windows ---\n",
    "    max_date_in_domain = df_domain_analysis[date_column].max()\n",
    "    if pd.isna(max_date_in_domain):\n",
    "        print(\"  Error: No valid maximum date found in the domain's data.\")\n",
    "        return emerging_cluster_ids\n",
    "\n",
    "    recent_period_start_date = (max_date_in_domain - DateOffset(months=recent_months_window - 1)).replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "    print(f\"  Most recent date in domain: {max_date_in_domain.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Recent period starts: {recent_period_start_date.strftime('%Y-%m-%d')} (approx. last {recent_months_window} months)\")\n",
    "\n",
    "    df_recent = df_domain_analysis[df_domain_analysis[date_column] >= recent_period_start_date]\n",
    "    df_baseline = df_domain_analysis[df_domain_analysis[date_column] < recent_period_start_date]\n",
    "\n",
    "    if df_recent.empty:\n",
    "        print(f\"  No data found in the recent {recent_months_window}-month period for domain '{domain_name}'. Cannot assess emergence.\")\n",
    "        return emerging_cluster_ids\n",
    "\n",
    "    # --- 3. Calculate Proportions and Identify Emerging Clusters ---\n",
    "    total_papers_domain_recent = len(df_recent)\n",
    "    total_papers_domain_baseline = len(df_baseline)\n",
    "\n",
    "    print(f\"  Total papers in domain - Recent: {total_papers_domain_recent}, Baseline: {total_papers_domain_baseline}\")\n",
    "\n",
    "    unique_clusters = sorted(df_domain_analysis[cluster_column].unique())\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        papers_cluster_recent = len(df_recent[df_recent[cluster_column] == cluster_id])\n",
    "        \n",
    "        # Skip if cluster has too few papers in the recent period\n",
    "        if papers_cluster_recent < min_papers_recent_period:\n",
    "            continue\n",
    "\n",
    "        papers_cluster_baseline = len(df_baseline[df_baseline[cluster_column] == cluster_id])\n",
    "\n",
    "        prop_recent = (papers_cluster_recent / total_papers_domain_recent) if total_papers_domain_recent > 0 else 0\n",
    "        prop_baseline = (papers_cluster_baseline / total_papers_domain_baseline) if total_papers_domain_baseline > 0 else 0\n",
    "        \n",
    "        is_emerging = False\n",
    "\n",
    "        if prop_baseline == 0: # Cluster is new or was not present in baseline\n",
    "            if prop_recent >= newly_active_min_recent_prop:\n",
    "                print(f\"    Cluster {cluster_id}: Newly Active (Recent Prop: {prop_recent:.4f})\")\n",
    "                is_emerging = True\n",
    "        else: # Cluster existed in baseline\n",
    "            emergence_ratio = prop_recent / prop_baseline\n",
    "            emergence_difference = prop_recent - prop_baseline\n",
    "\n",
    "            if emergence_ratio >= emerging_ratio_threshold and emergence_difference >= emerging_diff_threshold:\n",
    "                print(f\"    Cluster {cluster_id}: Emerging (Ratio: {emergence_ratio:.2f}, Diff: {emergence_difference:.4f}, Recent Prop: {prop_recent:.4f}, Baseline Prop: {prop_baseline:.4f})\")\n",
    "                is_emerging = True\n",
    "        \n",
    "        if is_emerging:\n",
    "            emerging_cluster_ids.append(cluster_id)\n",
    "\n",
    "    if not emerging_cluster_ids:\n",
    "        print(\"  No clusters met the criteria for being 'emerging' or 'newly active'.\")\n",
    "    else:\n",
    "        print(f\"  Identified emerging/newly active cluster IDs: {emerging_cluster_ids}\")\n",
    "        \n",
    "    return sorted(list(set(emerging_cluster_ids))) # Return unique sorted list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49888e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Functions to Analyze Clusters and Plot Trends ---\n",
    "def analyze_sub_clusters(\n",
    "    df: pd.DataFrame,\n",
    "    domain_name: str,\n",
    "    chosen_k: int,\n",
    "    relevant_features_used_for_clustering: List[str],\n",
    "    metadata_features: List[str],\n",
    "    rolling_window_param = ROLLING_WINDOW\n",
    ") -> Optional[Tuple[pd.DataFrame, Dict[int, List[str]]]]:\n",
    "    \"\"\"\n",
    "    Analyzes sub-clusters. For each sub-cluster, plots:\n",
    "    1. Faceted bar plot of its Top 5 keywords (by mean proportion within the cluster).\n",
    "       (Top 10 keywords are generated and stored for other uses).\n",
    "    2. Faceted bar plot of its Top 5 unique areas (by mean proportion within the cluster).\n",
    "    3. Faceted line plot of its relative temporal trend (proportion of monthly papers).\n",
    "    Returns smoothed proportions DataFrame for temporal trends and a dictionary of top N keywords per sub-cluster.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing Sub-Clusters for {domain_name} (k={chosen_k} actual clusters) ---\")\n",
    "    smoothed_proportions_df_to_return = None\n",
    "    top_keywords_map: Dict[int, List[str]] = {} # Stores top N (e.g., 10) keywords\n",
    "\n",
    "    N_TOP_KEYWORDS_TO_GENERATE = 10 # How many keywords to identify and store in top_keywords_map\n",
    "    N_KEYWORDS_TO_PLOT = 5        # How many keywords to display in the bar plot\n",
    "\n",
    "    # --- Filtering and Grouping ---\n",
    "    try:\n",
    "        # Filter for the current domain and ensure sub_cluster labels are present\n",
    "        # This mask will include papers belonging to any cluster (0, 1, ..., and -1 if HDBSCAN noise)\n",
    "        analysis_mask = (df.loc[:, domain_name] == True) & (df.loc[:, 'sub_cluster'].notna())\n",
    "        df_analysis = df.loc[analysis_mask].copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"  Error accessing columns for filtering: {e}.\")\n",
    "        return None, None # Return tuple of Nones\n",
    "\n",
    "    if not df_analysis.empty:\n",
    "        try:\n",
    "            # Convert sub_cluster to int. If HDBSCAN was used, -1 for noise is already int.\n",
    "            # If K-Means was used, labels are already int.\n",
    "            # This ensures consistency if the column was, e.g., float.\n",
    "            df_analysis['sub_cluster'] = df_analysis['sub_cluster'].astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not convert 'sub_cluster' to int: {e}\")\n",
    "            # Proceeding, but downstream functions might expect int.\n",
    "\n",
    "    if df_analysis.empty:\n",
    "        print(\"  No data with sub-cluster labels found for analysis in this domain.\")\n",
    "        return None, None # Return tuple of Nones\n",
    "\n",
    "    try:\n",
    "        grouped_sub_clusters = df_analysis.groupby('sub_cluster')\n",
    "    except Exception as e:\n",
    "         print(f\"  Error grouping by 'sub_cluster': {e}\")\n",
    "         return None, None # Return tuple of Nones\n",
    "\n",
    "    # --- Analyze and Plot Top Keywords per Sub-cluster ---\n",
    "    print(f\"\\n  Generating Top {N_KEYWORDS_TO_PLOT} Keywords per Sub-cluster Plot\")\n",
    "    actual_keyword_cols_for_summary = [\n",
    "        col for col in relevant_features_used_for_clustering if col.startswith('kw_') and col in df_analysis.columns\n",
    "    ]\n",
    "    keyword_plot_data_list = [] # Data for the bar plot (top 5)\n",
    "\n",
    "    if not actual_keyword_cols_for_summary:\n",
    "        print(\"    No 'kw_' prefixed keyword columns found for summary.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Mean proportion of each keyword *within each cluster*\n",
    "            sub_cluster_keyword_proportions = grouped_sub_clusters[actual_keyword_cols_for_summary].mean()\n",
    "\n",
    "            # Iterate through each cluster ID found in the data (includes 0, 1,... and -1 if present)\n",
    "            for sub_cluster_id in sub_cluster_keyword_proportions.index:\n",
    "                cluster_keyword_means = sub_cluster_keyword_proportions.loc[sub_cluster_id]\n",
    "                if isinstance(cluster_keyword_means.dtype, pd.SparseDtype):\n",
    "                    cluster_keyword_means = cluster_keyword_means.sparse.to_dense()\n",
    "\n",
    "                # Identify Top N (e.g., 10) keywords for this cluster\n",
    "                top_n_generated_kws = cluster_keyword_means.sort_values(ascending=False).head(N_TOP_KEYWORDS_TO_GENERATE)\n",
    "                # Store these Top N (e.g., 10) keywords in the map for external use\n",
    "                top_keywords_map[sub_cluster_id] = [kw.replace('kw_', '') for kw in top_n_generated_kws.index.tolist()]\n",
    "\n",
    "                # For plotting, select only the Top X (e.g., 5) from the generated top N\n",
    "                top_x_kws_for_plot = top_n_generated_kws.head(N_KEYWORDS_TO_PLOT)\n",
    "\n",
    "                for keyword, proportion in top_x_kws_for_plot.items():\n",
    "                    keyword_plot_data_list.append({\n",
    "                        'sub_cluster': sub_cluster_id,\n",
    "                        'item_name': keyword.replace('kw_', ''),\n",
    "                        'percentage': proportion * 100\n",
    "                    })\n",
    "\n",
    "            if keyword_plot_data_list:\n",
    "                keyword_plot_df = pd.DataFrame(keyword_plot_data_list)\n",
    "                # Sort facets by sub_cluster_id for consistent order if not already\n",
    "                cluster_order_keywords = sorted(keyword_plot_df['sub_cluster'].unique())\n",
    "\n",
    "                g_keywords = sns.FacetGrid(keyword_plot_df, row=\"sub_cluster\", row_order=cluster_order_keywords,\n",
    "                                           aspect=4, height=1.2,\n",
    "                                           sharex=True, sharey=False)\n",
    "                g_keywords.map_dataframe(sns.barplot, x=\"percentage\", y=\"item_name\", orient=\"h\", palette=\"crest_r\")\n",
    "\n",
    "                for ax_idx, ax_row in enumerate(g_keywords.axes):\n",
    "                    for c_idx, c in enumerate(ax_row[0].containers):\n",
    "                        labels = [f'{w:.1f}%' for w in c.datavalues]\n",
    "                        ax_row[0].bar_label(c, labels=labels, label_type='edge', padding=3, fontsize=8)\n",
    "\n",
    "                g_keywords.set_titles(row_template=f\"Sub-cluster {{row_name}} - Top {N_KEYWORDS_TO_PLOT} Keywords\")\n",
    "                g_keywords.set_axis_labels(\"Avg. Presence in Cluster Papers (%)\", \"Keyword\")\n",
    "                g_keywords.figure.subplots_adjust(top=0.93)\n",
    "                g_keywords.figure.suptitle(f'Top {N_KEYWORDS_TO_PLOT} Keywords by Sub-cluster ({domain_name})', fontsize=16, y=1.0)\n",
    "                plt.tight_layout(rect=[0,0,1,0.96])\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"    No keyword data to plot.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error calculating or plotting top keywords: {e}\")\n",
    "            # Fall through to try other plots\n",
    "\n",
    "    # --- Analyze and Plot Top 5 Unique Areas per Sub-cluster ---\n",
    "    print(f\"\\n  Generating Top 5 Unique Areas per Sub-cluster Plot:\")\n",
    "    unique_area_cols = [col for col in metadata_features if col != 'number_of_authors' and col in df_analysis.columns]\n",
    "    area_plot_data_list = []\n",
    "\n",
    "    if not unique_area_cols:\n",
    "        print(\"    No unique area columns identified for plotting.\")\n",
    "    else:\n",
    "        try:\n",
    "            area_proportions_per_cluster = grouped_sub_clusters[unique_area_cols].mean()\n",
    "            for sub_cluster_id in area_proportions_per_cluster.index:\n",
    "                cluster_area_means = area_proportions_per_cluster.loc[sub_cluster_id]\n",
    "                top_5_areas_for_this_cluster = cluster_area_means.sort_values(ascending=False).head(5)\n",
    "                for area, proportion in top_5_areas_for_this_cluster.items():\n",
    "                    if proportion > 0.001: # Only plot if proportion is somewhat significant\n",
    "                        area_plot_data_list.append({\n",
    "                            'sub_cluster': sub_cluster_id,\n",
    "                            'item_name': area,\n",
    "                            'percentage': proportion * 100\n",
    "                        })\n",
    "            if area_plot_data_list:\n",
    "                area_plot_df = pd.DataFrame(area_plot_data_list)\n",
    "                cluster_order_areas = sorted(area_plot_df['sub_cluster'].unique())\n",
    "                g_areas = sns.FacetGrid(area_plot_df, row=\"sub_cluster\", row_order=cluster_order_areas,\n",
    "                                        aspect=4, height=1.2,\n",
    "                                        sharex=True, sharey=False)\n",
    "                g_areas.map_dataframe(sns.barplot, x=\"percentage\", y=\"item_name\", orient=\"h\", palette=\"flare_r\")\n",
    "                for ax_idx, ax_row in enumerate(g_areas.axes):\n",
    "                    for c_idx, c in enumerate(ax_row[0].containers):\n",
    "                        labels = [f'{w:.1f}%' for w in c.datavalues]\n",
    "                        ax_row[0].bar_label(c, labels=labels, label_type='edge', padding=3, fontsize=8)\n",
    "                g_areas.set_titles(row_template=\"Sub-cluster {row_name} - Top 5 Areas\")\n",
    "                g_areas.set_axis_labels(\"Avg. Presence in Cluster Papers (%)\", \"Unique Area\")\n",
    "                g_areas.figure.subplots_adjust(top=0.93)\n",
    "                g_areas.figure.suptitle(f'Top 5 Unique Areas by Sub-cluster ({domain_name})', fontsize=16, y=1.0)\n",
    "                plt.tight_layout(rect=[0,0,1,0.96])\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"    No area data to plot (or proportions too low).\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error plotting unique area distributions: {e}\")\n",
    "            # Fall through\n",
    "\n",
    "    # --- Analyze Temporal Trends (Proportions Plot - Faceted) ---\n",
    "    if 'first_date' in df_analysis.columns and pd.api.types.is_datetime64_any_dtype(df_analysis['first_date']):\n",
    "        print(\"\\n  Relative Temporal Trends (Proportion of Monthly Publications):\")\n",
    "        try:\n",
    "            # Use .loc for assignment to avoid SettingWithCopyWarning\n",
    "            df_analysis.loc[:, 'YearMonth'] = df_analysis['first_date'].dt.to_period('M')\n",
    "            # Count papers per YearMonth and sub_cluster\n",
    "            cluster_monthly_counts = df_analysis.groupby(['YearMonth', 'sub_cluster']).size().unstack(fill_value=0)\n",
    "            # Total papers per YearMonth across all clusters (including -1 if present)\n",
    "            total_monthly_counts = cluster_monthly_counts.sum(axis=1)\n",
    "            # Calculate proportion\n",
    "            proportions_df = cluster_monthly_counts.divide(total_monthly_counts, axis=0).fillna(0)\n",
    "\n",
    "            # Smooth proportions\n",
    "            smoothed_proportions_df_to_return = proportions_df.rolling(window=rolling_window_param, center=True, min_periods=1).mean()\n",
    "            # Convert PeriodIndex to DatetimeIndex for plotting\n",
    "            smoothed_proportions_df_to_return.index = smoothed_proportions_df_to_return.index.to_timestamp()\n",
    "\n",
    "            # Melt for FacetGrid\n",
    "            proportions_melted = smoothed_proportions_df_to_return.reset_index().melt(\n",
    "                id_vars='YearMonth', var_name='sub_cluster', value_name='proportion'\n",
    "            )\n",
    "            # Ensure consistent cluster order in facets\n",
    "            cluster_order_temporal = sorted(df_analysis['sub_cluster'].unique())\n",
    "\n",
    "            g_temporal = sns.FacetGrid(\n",
    "                proportions_melted, row=\"sub_cluster\", row_order=cluster_order_temporal, hue=\"sub_cluster\",\n",
    "                hue_order=cluster_order_temporal, aspect=5, height=1.5,\n",
    "                palette=sns.color_palette(\"hsv\", n_colors=len(cluster_order_temporal)), # Use a palette with enough colors\n",
    "                sharex=True, sharey=False # Allow different y-scales if proportions vary a lot\n",
    "            )\n",
    "            g_temporal.map(sns.lineplot, \"YearMonth\", \"proportion\", lw=2)\n",
    "            g_temporal.set_titles(row_template=\"Sub-cluster {row_name}\")\n",
    "            g_temporal.set_axis_labels(\"Time (Month)\", \"Proportion of Monthly Papers\")\n",
    "            g_temporal.map(plt.grid, axis='y', linestyle='--', alpha=0.7)\n",
    "            g_temporal.figure.subplots_adjust(top=0.93)\n",
    "            g_temporal.figure.suptitle(f'Relative Trend within {domain_name} ({rolling_window_param}-Month Rolling Avg)', fontsize=16, y=1.0)\n",
    "            plt.tight_layout(rect=[0,0,1,0.96])\n",
    "            plt.show()\n",
    "\n",
    "            # Clean up temporary column\n",
    "            if 'YearMonth' in df_analysis.columns:\n",
    "                df_analysis.drop(columns=['YearMonth'], inplace=True, errors='ignore')\n",
    "\n",
    "        except Exception as plot_e:\n",
    "            print(f\"    Error during proportion plot generation: {plot_e}\")\n",
    "            if 'YearMonth' in df_analysis.columns:\n",
    "                df_analysis.drop(columns=['YearMonth'], inplace=True, errors='ignore')\n",
    "            # Still return what we have, top_keywords_map should be populated by now\n",
    "            return smoothed_proportions_df_to_return, top_keywords_map\n",
    "\n",
    "        print(\"\\n  Median Publication Date per Sub-cluster:\")\n",
    "        try:\n",
    "            median_dates = grouped_sub_clusters['first_date'].median()\n",
    "            print(median_dates)\n",
    "        except Exception as e:\n",
    "            print(f\"    Error calculating median dates: {e}\")\n",
    "\n",
    "        # top_keywords_map will contain the top N_TOP_KEYWORDS_TO_GENERATE keywords\n",
    "        return smoothed_proportions_df_to_return, top_keywords_map\n",
    "    else:\n",
    "        print(\"\\n  Skipping temporal analysis (missing/invalid 'first_date').\")\n",
    "        # top_keywords_map will contain the top N_TOP_KEYWORDS_TO_GENERATE keywords\n",
    "        return smoothed_proportions_df_to_return, top_keywords_map\n",
    "\n",
    "def plot_combined_trends_legend(\n",
    "    proportions_df: pd.DataFrame,\n",
    "    clusters_to_plot: List[int],\n",
    "    cluster_colors: Dict[int, str],\n",
    "    top_keywords_data: Dict[int, List[str]],\n",
    "    domain_name: str,\n",
    "    rolling_window: int = 6,\n",
    "    keyword_exclusion_color: str = 'grey',\n",
    "    annotation_fontsize: int = 9,\n",
    "    annotation_start_y: float = 0.97,\n",
    "    annotation_y_step: float = 0.18,\n",
    "    y_axis_scale: str = 'linear',  # 'linear' or 'log'\n",
    "    display_excluded_clusters: bool = True  # Toggle for displaying grey lines\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots smoothed proportion trends (as percentages) for selected sub-clusters on a single graph\n",
    "    with custom colors and keyword annotations on the right.\n",
    "\n",
    "    Args:\n",
    "        proportions_df: DataFrame of smoothed proportions (values between 0 and 1).\n",
    "        clusters_to_plot: List of cluster IDs to plot.\n",
    "        cluster_colors: Dictionary mapping cluster IDs to their respective colors.\n",
    "        top_keywords_data: Dictionary mapping cluster IDs to their top keywords.\n",
    "        domain_name: Name of the domain for the plot title.\n",
    "        rolling_window: Rolling window size for smoothing.\n",
    "        keyword_exclusion_color: Color used for excluded clusters (default: 'grey').\n",
    "        annotation_fontsize: Font size for keyword annotations.\n",
    "        annotation_start_y: Starting y-position for keyword annotations.\n",
    "        annotation_y_step: Step size for y-position of annotations.\n",
    "        y_axis_scale: Scale of the y-axis. Can be 'linear' or 'log'. Defaults to 'linear'.\n",
    "        display_excluded_clusters: Whether to display excluded clusters (grey lines). Defaults to True.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Combined Trends for Selected Sub-clusters in {domain_name} (Y-axis: {y_axis_scale}) ---\")\n",
    "\n",
    "    if proportions_df is None or proportions_df.empty:\n",
    "        print(\"  Error: Input proportions_df is None or empty.\")\n",
    "        return\n",
    "\n",
    "    # Identify clusters to include and exclude\n",
    "    clusters_present = [c for c in clusters_to_plot if c in proportions_df.columns]\n",
    "    missing_clusters = [c for c in clusters_to_plot if c not in proportions_df.columns]\n",
    "    if missing_clusters:\n",
    "        print(f\"  Warning: Requested clusters not in proportions_df: {missing_clusters}\")\n",
    "    if not clusters_present:\n",
    "        print(\"  Error: None of the requested clusters found in proportions_df.\")\n",
    "        return\n",
    "\n",
    "    df_to_plot = proportions_df[clusters_present].copy()\n",
    "\n",
    "    # Separate clusters by color\n",
    "    included_clusters = [c for c in clusters_present if cluster_colors.get(c) != keyword_exclusion_color]\n",
    "    excluded_clusters = [c for c in clusters_present if cluster_colors.get(c) == keyword_exclusion_color]\n",
    "\n",
    "    # Create a color palette for the clusters\n",
    "    plot_palette = {}\n",
    "    default_color_palette = sns.color_palette(\"tab10\", n_colors=len(clusters_present))\n",
    "    for i, cluster_id in enumerate(clusters_present):\n",
    "        plot_palette[cluster_id] = cluster_colors.get(cluster_id, default_color_palette[i % len(default_color_palette)])\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    # Plot excluded clusters (grey lines) first, if enabled\n",
    "    if display_excluded_clusters:\n",
    "        for cluster_id in excluded_clusters:\n",
    "            color = plot_palette.get(cluster_id)\n",
    "            plot_data = df_to_plot[cluster_id] * 100  # Convert proportion to percentage\n",
    "            ax.plot(df_to_plot.index, plot_data, color=color, lw=1.5, alpha=0.5, zorder=1)\n",
    "\n",
    "    # Plot included clusters (colored lines)\n",
    "    for cluster_id in included_clusters:\n",
    "        color = plot_palette.get(cluster_id)\n",
    "        plot_data = df_to_plot[cluster_id] * 100  # Convert proportion to percentage\n",
    "        ax.plot(df_to_plot.index, plot_data, color=color, lw=2.5, zorder=2, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "    # --- Axis Labels and Title ---\n",
    "    ax.set_title(f'Relative Trend for Clusters within {domain_name}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Percentage of Monthly Publications (%)')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Set y-axis scale and formatter\n",
    "    if y_axis_scale.lower() == 'log':\n",
    "        ax.set_yscale('log')\n",
    "        try:\n",
    "            ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=100.0, decimals=None))\n",
    "        except Exception:\n",
    "            ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: '{:.1f}%'.format(y)))\n",
    "    else:  # Linear scale\n",
    "        ax.set_yscale('linear')\n",
    "        ax.set_ylim(bottom=0, top=max(1.0, df_to_plot.mul(100).max().max() * 1.1) if not df_to_plot.empty else 1.0)\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=100.0, decimals=0))\n",
    "\n",
    "    # Adjust layout and show plot\n",
    "    plt.subplots_adjust(left=0.07, right=0.70, top=0.9, bottom=0.1)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), title=\"Clusters\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_trends(\n",
    "    proportions_df: pd.DataFrame,\n",
    "    clusters_to_plot: List[int],\n",
    "    cluster_colors: Dict[int, str],\n",
    "    top_keywords_data: Dict[int, List[str]],\n",
    "    domain_name: str,\n",
    "    rolling_window: int = 6,\n",
    "    keyword_exclusion_color: str = 'grey',\n",
    "    annotation_fontsize: int = 9,\n",
    "    annotation_start_y: float = 0.97,\n",
    "    annotation_y_step: float = 0.18,\n",
    "    y_axis_scale: str = 'linear',  # 'linear' or 'log'\n",
    "    max_keywords_to_display: int = 5  # New parameter to limit the number of keywords displayed\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots smoothed proportion trends (as percentages) for selected sub-clusters on a single graph\n",
    "    with custom colors and keyword annotations on the right.\n",
    "\n",
    "    Args:\n",
    "        proportions_df: DataFrame of smoothed proportions (values between 0 and 1).\n",
    "        clusters_to_plot: List of cluster IDs to plot.\n",
    "        cluster_colors: Dictionary mapping cluster IDs to their respective colors.\n",
    "        top_keywords_data: Dictionary mapping cluster IDs to their top keywords.\n",
    "        domain_name: Name of the domain for the plot title.\n",
    "        rolling_window: Rolling window size for smoothing.\n",
    "        keyword_exclusion_color: Color used for excluded clusters (default: 'grey').\n",
    "        annotation_fontsize: Font size for keyword annotations.\n",
    "        annotation_start_y: Starting y-position for keyword annotations.\n",
    "        annotation_y_step: Step size for y-position of annotations.\n",
    "        y_axis_scale: Scale of the y-axis. Can be 'linear' or 'log'. Defaults to 'linear'.\n",
    "        max_keywords_to_display: Maximum number of keywords to display for each cluster. Defaults to 5.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Combined Trends for Selected Sub-clusters in {domain_name} (Y-axis: {y_axis_scale}) ---\")\n",
    "\n",
    "    if proportions_df is None or proportions_df.empty:\n",
    "        print(\"  Error: Input proportions_df is None or empty.\")\n",
    "        return\n",
    "\n",
    "    # Identify clusters to include and exclude\n",
    "    clusters_present = [c for c in clusters_to_plot if c in proportions_df.columns]\n",
    "    missing_clusters = [c for c in clusters_to_plot if c not in proportions_df.columns]\n",
    "    if missing_clusters:\n",
    "        print(f\"  Warning: Requested clusters not in proportions_df: {missing_clusters}\")\n",
    "    if not clusters_present:\n",
    "        print(\"  Error: None of the requested clusters found in proportions_df.\")\n",
    "        return\n",
    "\n",
    "    df_to_plot = proportions_df[clusters_present].copy()\n",
    "\n",
    "    # Separate clusters by color\n",
    "    included_clusters = [c for c in clusters_present if cluster_colors.get(c) != keyword_exclusion_color]\n",
    "    excluded_clusters = [c for c in clusters_present if cluster_colors.get(c) == keyword_exclusion_color]\n",
    "\n",
    "    # Create a color palette for the clusters\n",
    "    plot_palette = {}\n",
    "    default_color_palette = sns.color_palette(\"tab10\", n_colors=len(clusters_present))\n",
    "    for i, cluster_id in enumerate(clusters_present):\n",
    "        plot_palette[cluster_id] = cluster_colors.get(cluster_id, default_color_palette[i % len(default_color_palette)])\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    # Plot excluded clusters (grey lines) first, if enabled\n",
    "    for cluster_id in excluded_clusters:\n",
    "        color = plot_palette.get(cluster_id)\n",
    "        plot_data = df_to_plot[cluster_id] * 100  # Convert proportion to percentage\n",
    "        ax.plot(df_to_plot.index, plot_data, color=color, lw=1.5, alpha=0.5, zorder=1)\n",
    "\n",
    "    # Plot included clusters (colored lines)\n",
    "    for cluster_id in included_clusters:\n",
    "        color = plot_palette.get(cluster_id)\n",
    "        plot_data = df_to_plot[cluster_id] * 100  # Convert proportion to percentage\n",
    "        ax.plot(df_to_plot.index, plot_data, color=color, lw=2.5, zorder=2)\n",
    "\n",
    "    # --- Keyword Annotations ---\n",
    "    last_proportions = {\n",
    "        cid: df_to_plot[cid].iloc[-1]\n",
    "        for cid in clusters_present\n",
    "        if not df_to_plot[cid].empty and pd.notna(df_to_plot[cid].iloc[-1])\n",
    "    }\n",
    "    sorted_clusters_for_text = sorted(last_proportions, key=last_proportions.get, reverse=True)\n",
    "    clusters_not_in_sorted = [c for c in clusters_present if c not in sorted_clusters_for_text]\n",
    "    sorted_clusters_for_text.extend(clusters_not_in_sorted)\n",
    "    current_y_text_position = annotation_start_y\n",
    "\n",
    "    for cluster_id in sorted_clusters_for_text:\n",
    "        color = plot_palette.get(cluster_id)\n",
    "        skip_keywords = False\n",
    "        if color:\n",
    "            if isinstance(color, str) and color.lower() == keyword_exclusion_color.lower():\n",
    "                skip_keywords = True\n",
    "        if skip_keywords:\n",
    "            continue\n",
    "        keywords = top_keywords_data.get(cluster_id, [])\n",
    "        if keywords:\n",
    "            # Limit the number of keywords displayed\n",
    "            keywords_to_display = keywords[:max_keywords_to_display]\n",
    "            keyword_string = f\"Cluster {cluster_id}:\\n\" + \"\\n\".join(keywords_to_display)\n",
    "            ax.text(1.02, current_y_text_position, keyword_string,\n",
    "                    transform=ax.transAxes, fontsize=annotation_fontsize, color=color,\n",
    "                    verticalalignment='top', horizontalalignment='left',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=color, alpha=0.75))\n",
    "            current_y_text_position -= annotation_y_step\n",
    "\n",
    "    # --- Axis Labels and Title ---\n",
    "    ax.set_title(f'Relative Trend for Clusters within {domain_name}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Percentage of Monthly Publications (%)')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Set y-axis scale and formatter\n",
    "    if y_axis_scale.lower() == 'log':\n",
    "        ax.set_yscale('log')\n",
    "        try:\n",
    "            ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=100.0, decimals=None))\n",
    "        except Exception:\n",
    "            ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: '{:.1f}%'.format(y)))\n",
    "    else:  # Linear scale\n",
    "        ax.set_yscale('linear')\n",
    "        ax.set_ylim(bottom=0, top=max(1.0, df_to_plot.mul(100).max().max() * 1.1) if not df_to_plot.empty else 1.0)\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=100.0, decimals=0))\n",
    "\n",
    "    # Adjust layout and show plot\n",
    "    plt.subplots_adjust(left=0.07, right=0.70, top=0.9, bottom=0.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cluster_keyword_trends(\n",
    "    df: pd.DataFrame,\n",
    "    domain_name: str,\n",
    "    cluster_id: int,\n",
    "    all_cluster_keywords: List[str],\n",
    "    num_keywords_to_plot: int = 10,\n",
    "    date_column: str = 'first_date',\n",
    "    text_column: str = 'abstract',\n",
    "    cluster_column: str = 'sub_cluster',\n",
    "    rolling_window: int = ROLLING_WINDOW,\n",
    "    palette_name: str = 'tab10'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the temporal trend of the top N keywords for a specific cluster.\n",
    "\n",
    "    For each of the selected keywords, it calculates the proportion of papers\n",
    "    within the given cluster, per month, that contain the keyword in their abstract.\n",
    "    These proportions are then smoothed using a rolling average and plotted.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The main DataFrame containing paper data.\n",
    "        domain_name (str): The domain to filter for (e.g., 'Quantitative Biology').\n",
    "        cluster_id (int): The specific sub-cluster ID to analyze.\n",
    "        all_cluster_keywords (List[str]): A list of all top keywords identified for this cluster.\n",
    "                                          The function will select the top 'num_keywords_to_plot' from this list.\n",
    "        num_keywords_to_plot (int): The number of top keywords to plot from 'all_cluster_keywords'.\n",
    "        date_column (str): Name of the column containing publication dates (datetime objects).\n",
    "        text_column (str): Name of the column containing text for keyword search (e.g., 'abstract').\n",
    "        cluster_column (str): Name of the column containing sub-cluster labels.\n",
    "        rolling_window (int): The window size for the rolling average.\n",
    "        palette_name (str): Name of the seaborn color palette to use for plotting lines.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Keyword Trends for Domain: {domain_name}, Cluster: {cluster_id} ---\")\n",
    "\n",
    "    # --- 1. Filter Data for the specific domain and cluster ---\n",
    "    try:\n",
    "        df_filtered = df[(df[domain_name] == True) & (df[cluster_column] == cluster_id)].copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"  Error: Column not found during filtering: {e}. Make sure domain and cluster columns exist.\")\n",
    "        return\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"  No data found for domain '{domain_name}' and cluster '{cluster_id}'. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Prepare Dates ---\n",
    "    if date_column not in df_filtered.columns:\n",
    "        print(f\"  Error: Date column '{date_column}' not found.\")\n",
    "        return\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_filtered[date_column]):\n",
    "        try:\n",
    "            df_filtered.loc[:, date_column] = pd.to_datetime(df_filtered[date_column])\n",
    "        except Exception as e:\n",
    "            print(f\"  Error converting date column '{date_column}' to datetime: {e}. Skipping.\")\n",
    "            return\n",
    "    \n",
    "    df_filtered.loc[:, 'YearMonth'] = df_filtered[date_column].dt.to_period('M')\n",
    "\n",
    "    # --- 3. Select Keywords to Plot ---\n",
    "    if not all_cluster_keywords:\n",
    "        print(f\"  No keywords provided for cluster {cluster_id}. Skipping plot.\")\n",
    "        return\n",
    "    \n",
    "    keywords_to_plot = all_cluster_keywords[:num_keywords_to_plot]\n",
    "    if len(keywords_to_plot) < num_keywords_to_plot:\n",
    "        print(f\"  Warning: Fewer than {num_keywords_to_plot} keywords available. Plotting {len(keywords_to_plot)} keywords.\")\n",
    "    if not keywords_to_plot:\n",
    "        print(f\"  No keywords selected to plot for cluster {cluster_id}. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Plotting top {len(keywords_to_plot)} keywords: {', '.join(keywords_to_plot)}\")\n",
    "\n",
    "    # --- 4. Calculate Monthly Proportions for each Keyword ---\n",
    "    keyword_monthly_proportions_dict = {}\n",
    "    \n",
    "    # Group once by YearMonth to optimize\n",
    "    grouped_by_month = df_filtered.groupby('YearMonth')\n",
    "\n",
    "    for keyword in keywords_to_plot:\n",
    "        monthly_proportions = []\n",
    "        # Ensure the keyword string is valid for regex\n",
    "        safe_keyword_pattern = re.escape(keyword.lower())\n",
    "\n",
    "        for ym, month_group in grouped_by_month:\n",
    "            if text_column not in month_group.columns:\n",
    "                print(f\"  Error: Text column '{text_column}' not found in month_group for {ym}.\")\n",
    "                continue\n",
    "\n",
    "            # Count papers containing the keyword (case-insensitive)\n",
    "            papers_with_keyword = month_group[text_column].str.lower().str.contains(safe_keyword_pattern, na=False).sum()\n",
    "            total_papers_this_month_in_cluster = len(month_group)\n",
    "            \n",
    "            proportion = papers_with_keyword / total_papers_this_month_in_cluster if total_papers_this_month_in_cluster > 0 else 0\n",
    "            monthly_proportions.append({'YearMonth': ym, 'proportion': proportion})\n",
    "        \n",
    "        if monthly_proportions:\n",
    "            keyword_ts = pd.DataFrame(monthly_proportions).set_index('YearMonth')['proportion']\n",
    "            keyword_monthly_proportions_dict[keyword] = keyword_ts\n",
    "\n",
    "    if not keyword_monthly_proportions_dict:\n",
    "        print(\"  No keyword data could be processed for plotting.\")\n",
    "        return\n",
    "\n",
    "    proportions_df = pd.DataFrame(keyword_monthly_proportions_dict)\n",
    "\n",
    "    # --- 5. Handle Missing Months & Smooth Data ---\n",
    "    # Create a full date range from min to max observed YearMonth\n",
    "    if not proportions_df.empty:\n",
    "        min_date = proportions_df.index.min().to_timestamp()\n",
    "        max_date = proportions_df.index.max().to_timestamp()\n",
    "        full_date_range = pd.period_range(start=min_date, end=max_date, freq='M')\n",
    "        proportions_df = proportions_df.reindex(full_date_range, fill_value=0)\n",
    "        \n",
    "        # Apply rolling mean\n",
    "        smoothed_proportions_df = proportions_df.rolling(window=rolling_window, center=True, min_periods=1).mean()\n",
    "        # Convert PeriodIndex to DatetimeIndex for plotting\n",
    "        smoothed_proportions_df.index = smoothed_proportions_df.index.to_timestamp()\n",
    "    else:\n",
    "        print(\"  Proportions DataFrame is empty before smoothing. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # --- 6. Plot ---\n",
    "    if smoothed_proportions_df.empty:\n",
    "        print(\"  Smoothed proportions DataFrame is empty. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid') # Using a seaborn style\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    \n",
    "    colors = sns.color_palette(palette_name, n_colors=len(smoothed_proportions_df.columns))\n",
    "\n",
    "    for i, keyword in enumerate(smoothed_proportions_df.columns):\n",
    "        ax.plot(smoothed_proportions_df.index, smoothed_proportions_df[keyword] * 100, # Plot as percentage\n",
    "                label=keyword, color=colors[i % len(colors)], linewidth=2)\n",
    "\n",
    "    ax.set_title(f'Temporal Trend of Top {len(keywords_to_plot)} Keywords in {domain_name} - Cluster {cluster_id}\\n({rolling_window}-Month Rolling Average)', fontsize=16)\n",
    "    ax.set_xlabel('Time (Month)', fontsize=12)\n",
    "    ax.set_ylabel('Proportion of Cluster Papers Containing Keyword (%)', fontsize=12)\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=100.0, decimals=1))\n",
    "    ax.legend(title='Keywords', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_keyword_trends_plotly(\n",
    "    df: pd.DataFrame,\n",
    "    domain_name: str,\n",
    "    cluster_id: int,\n",
    "    all_cluster_keywords: List[str],\n",
    "    num_keywords_to_plot: int = 10,\n",
    "    custom_colors: Optional[List[str]] = None, # New argument for custom hex colors\n",
    "    date_column: str = 'first_date',\n",
    "    text_column: str = 'abstract',\n",
    "    cluster_column: str = 'sub_cluster',\n",
    "    rolling_window: int = ROLLING_WINDOW\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the temporal trend of the top N keywords for a specific cluster using Plotly.\n",
    "\n",
    "    For each of the selected keywords, it calculates the proportion of papers\n",
    "    within the given cluster, per month, that contain the keyword in their abstract.\n",
    "    These proportions are then smoothed using a rolling average and plotted.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The main DataFrame containing paper data.\n",
    "        domain_name (str): The domain to filter for (e.g., 'Quantitative Biology').\n",
    "        cluster_id (int): The specific sub-cluster ID to analyze.\n",
    "        all_cluster_keywords (List[str]): A list of all top keywords identified for this cluster.\n",
    "                                          The function will select the top 'num_keywords_to_plot' from this list.\n",
    "        num_keywords_to_plot (int): The number of top keywords to plot from 'all_cluster_keywords'.\n",
    "        custom_colors (Optional[List[str]]): A list of hex color codes to use for plotting lines.\n",
    "                                             If None, Plotly's default colors will be used.\n",
    "                                             The list should have at least num_keywords_to_plot elements.\n",
    "        date_column (str): Name of the column containing publication dates (datetime objects).\n",
    "        text_column (str): Name of the column containing text for keyword search (e.g., 'abstract').\n",
    "        cluster_column (str): Name of the column containing sub-cluster labels.\n",
    "        rolling_window (int): The window size for the rolling average.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Keyword Trends (Plotly) for Domain: {domain_name}, Cluster: {cluster_id} ---\")\n",
    "\n",
    "    # --- 1. Filter Data for the specific domain and cluster ---\n",
    "    try:\n",
    "        df_filtered = df[(df[domain_name] == True) & (df[cluster_column] == cluster_id)].copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"  Error: Column not found during filtering: {e}. Make sure domain and cluster columns exist.\")\n",
    "        return\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"  No data found for domain '{domain_name}' and cluster '{cluster_id}'. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Prepare Dates ---\n",
    "    if date_column not in df_filtered.columns:\n",
    "        print(f\"  Error: Date column '{date_column}' not found.\")\n",
    "        return\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_filtered[date_column]):\n",
    "        try:\n",
    "            df_filtered.loc[:, date_column] = pd.to_datetime(df_filtered[date_column])\n",
    "        except Exception as e:\n",
    "            print(f\"  Error converting date column '{date_column}' to datetime: {e}. Skipping.\")\n",
    "            return\n",
    "\n",
    "    df_filtered.loc[:, 'YearMonth'] = df_filtered[date_column].dt.to_period('M')\n",
    "\n",
    "    # --- 3. Select Keywords to Plot ---\n",
    "    if not all_cluster_keywords:\n",
    "        print(f\"  No keywords provided for cluster {cluster_id}. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    keywords_to_plot = all_cluster_keywords[:num_keywords_to_plot]\n",
    "    if len(keywords_to_plot) < num_keywords_to_plot:\n",
    "        print(f\"  Warning: Fewer than {num_keywords_to_plot} keywords available. Plotting {len(keywords_to_plot)} keywords.\")\n",
    "    if not keywords_to_plot:\n",
    "        print(f\"  No keywords selected to plot for cluster {cluster_id}. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Plotting top {len(keywords_to_plot)} keywords: {', '.join(keywords_to_plot)}\")\n",
    "\n",
    "    if custom_colors and len(custom_colors) < len(keywords_to_plot):\n",
    "        print(f\"  Warning: Not enough custom colors provided ({len(custom_colors)}) for the number of keywords to plot ({len(keywords_to_plot)}). Plotly defaults will be used for some lines.\")\n",
    "        # Allow Plotly to cycle if not enough colors are provided, or extend with defaults.\n",
    "        # For simplicity, we'll let Plotly handle it if the list is too short.\n",
    "\n",
    "    # --- 4. Calculate Monthly Proportions for each Keyword ---\n",
    "    keyword_monthly_proportions_dict = {}\n",
    "    grouped_by_month = df_filtered.groupby('YearMonth')\n",
    "\n",
    "    for keyword in keywords_to_plot:\n",
    "        monthly_proportions = []\n",
    "        safe_keyword_pattern = re.escape(keyword.lower())\n",
    "\n",
    "        for ym, month_group in grouped_by_month:\n",
    "            if text_column not in month_group.columns:\n",
    "                print(f\"  Error: Text column '{text_column}' not found in month_group for {ym}.\")\n",
    "                continue\n",
    "\n",
    "            papers_with_keyword = month_group[text_column].str.lower().str.contains(safe_keyword_pattern, na=False).sum()\n",
    "            total_papers_this_month_in_cluster = len(month_group)\n",
    "\n",
    "            proportion = papers_with_keyword / total_papers_this_month_in_cluster if total_papers_this_month_in_cluster > 0 else 0\n",
    "            monthly_proportions.append({'YearMonth': ym, 'proportion': proportion})\n",
    "\n",
    "        if monthly_proportions:\n",
    "            keyword_ts = pd.DataFrame(monthly_proportions).set_index('YearMonth')['proportion']\n",
    "            keyword_monthly_proportions_dict[keyword] = keyword_ts\n",
    "\n",
    "    if not keyword_monthly_proportions_dict:\n",
    "        print(\"  No keyword data could be processed for plotting.\")\n",
    "        return\n",
    "\n",
    "    proportions_df = pd.DataFrame(keyword_monthly_proportions_dict)\n",
    "\n",
    "    # --- 5. Handle Missing Months & Smooth Data ---\n",
    "    if proportions_df.empty:\n",
    "        print(\"  Proportions DataFrame is empty before smoothing. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    min_date_period = proportions_df.index.min()\n",
    "    max_date_period = proportions_df.index.max()\n",
    "\n",
    "    if pd.isna(min_date_period) or pd.isna(max_date_period):\n",
    "        print(\"  Could not determine date range from proportions_df index. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    full_date_range = pd.period_range(start=min_date_period.to_timestamp(),\n",
    "                                      end=max_date_period.to_timestamp(), freq='M')\n",
    "    proportions_df = proportions_df.reindex(full_date_range, fill_value=0)\n",
    "\n",
    "    smoothed_proportions_df = proportions_df.rolling(window=rolling_window, center=True, min_periods=1).mean()\n",
    "    smoothed_proportions_df.index = smoothed_proportions_df.index.to_timestamp() # Convert PeriodIndex to DatetimeIndex for Plotly\n",
    "\n",
    "    # --- 6. Plot with Plotly ---\n",
    "    if smoothed_proportions_df.empty:\n",
    "        print(\"  Smoothed proportions DataFrame is empty. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for i, keyword in enumerate(smoothed_proportions_df.columns):\n",
    "        color = custom_colors[i % len(custom_colors)] if custom_colors else None\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=smoothed_proportions_df.index,\n",
    "            y=smoothed_proportions_df[keyword] * 100, # Plot as percentage\n",
    "            mode='lines',\n",
    "            name=keyword,\n",
    "            line=dict(width=2, color=color) # Apply custom color if provided\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Temporal Trend of Top {len(keywords_to_plot)} Keywords in {domain_name} - Cluster {cluster_id}<br>({rolling_window}-Month Rolling Average)',\n",
    "        title_x=0.5,\n",
    "        xaxis_title='Time (Month)',\n",
    "        yaxis_title='Proportion of Cluster Papers Containing Keyword (%)',\n",
    "        yaxis_tickformat='.1f', # Format y-axis ticks to one decimal place\n",
    "        yaxis_ticksuffix='%',\n",
    "        legend_title_text='Keywords',\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        margin=dict(l=50, r=50, b=100, t=100, pad=4), # Adjust margins\n",
    "        hovermode='x unified' # Improved hover experience\n",
    "    )\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_cluster_record_counts(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_column: str,\n",
    "    domain_name: Optional[str] = None,\n",
    "    ax: Optional[plt.Axes] = None,\n",
    "    top_n: Optional[int] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates and displays a bar plot showing the number of records in each actual cluster\n",
    "    (excluding noise cluster -1) for a specified domain.\n",
    "    Optionally shows only the top N clusters.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        cluster_column (str): The name of the column containing cluster labels.\n",
    "        domain_name (Optional[str]): The name of the domain to filter by.\n",
    "                                     If None, data is not filtered by domain.\n",
    "                                     The domain_name column in df must be boolean.\n",
    "        ax (Optional[plt.Axes]): A matplotlib axes object to plot on.\n",
    "                                 If None, a new figure and axes will be created.\n",
    "        top_n (Optional[int]): If set, displays only the top N largest clusters\n",
    "                               and groups the rest into an 'Other' category.\n",
    "    \"\"\"\n",
    "    if cluster_column not in df.columns:\n",
    "        print(f\"Error: Cluster column '{cluster_column}' not found in DataFrame.\")\n",
    "        return\n",
    "\n",
    "    df_to_plot = df.copy()\n",
    "    plot_title = \"Number of Records per Actual Cluster\"\n",
    "\n",
    "    if domain_name:\n",
    "        if domain_name not in df.columns:\n",
    "            print(f\"Warning: Domain column '{domain_name}' not found in DataFrame. Plotting for all data in df.\")\n",
    "        elif not pd.api.types.is_bool_dtype(df_to_plot[domain_name]):\n",
    "            print(f\"Warning: Domain column '{domain_name}' is not boolean. Plotting for all data in df.\")\n",
    "        else:\n",
    "            df_to_plot = df_to_plot[df_to_plot[domain_name] == True]\n",
    "            plot_title = f\"Number of Records per Actual Cluster in {domain_name}\"\n",
    "    \n",
    "    if df_to_plot.empty:\n",
    "        if domain_name:\n",
    "            print(f\"No data to plot for domain '{domain_name}'.\")\n",
    "        else:\n",
    "            print(\"No data to plot (DataFrame is empty).\")\n",
    "        return\n",
    "\n",
    "    # Exclude noise cluster (-1)\n",
    "    actual_clusters_df = df_to_plot[df_to_plot[cluster_column] != -1]\n",
    "    \n",
    "    if actual_clusters_df.empty:\n",
    "        print(f\"No actual (non-noise) cluster data found in '{cluster_column}' for the specified selection.\")\n",
    "        return\n",
    "        \n",
    "    cluster_counts = actual_clusters_df[cluster_column].value_counts()\n",
    "\n",
    "    if top_n and len(cluster_counts) > top_n:\n",
    "        top_clusters = cluster_counts.nlargest(top_n)\n",
    "        other_count = cluster_counts.nsmallest(len(cluster_counts) - top_n).sum()\n",
    "        if other_count > 0:\n",
    "            top_clusters['Other (Remaining Clusters)'] = other_count\n",
    "        cluster_counts = top_clusters\n",
    "        plot_title += f\" (Top {top_n})\"\n",
    "    \n",
    "    cluster_counts = cluster_counts.sort_index(ascending=True, key=lambda x: pd.Series(x).astype(str) if 'Other' not in x.name else 'ZZZ') # Sort numerically, 'Other' last\n",
    "\n",
    "\n",
    "    if cluster_counts.empty:\n",
    "        print(f\"No actual cluster data to plot after filtering and top_n in '{cluster_column}'.\")\n",
    "        return\n",
    "\n",
    "    current_ax = ax\n",
    "    if current_ax is None:\n",
    "        # Adjust figsize based on number of clusters to plot\n",
    "        num_items_to_plot = len(cluster_counts)\n",
    "        fig_width = max(12, num_items_to_plot * 0.5) # Dynamic width\n",
    "        fig_height = 7\n",
    "        fig, current_ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "    sns.barplot(x=cluster_counts.index.astype(str), y=cluster_counts.values, ax=current_ax, palette=\"viridis\")\n",
    "    current_ax.set_title(plot_title, fontsize=15)\n",
    "    current_ax.set_xlabel(\"Cluster ID\", fontsize=12)\n",
    "    current_ax.set_ylabel(\"Number of Records\", fontsize=12)\n",
    "    \n",
    "    if len(cluster_counts.index) > 15 :\n",
    "        current_ax.tick_params(axis='x', rotation=90, labelsize=8)\n",
    "    elif len(cluster_counts.index) > 5:\n",
    "        current_ax.tick_params(axis='x', rotation=45, ha='right', labelsize=10)\n",
    "    else:\n",
    "        current_ax.tick_params(axis='x', rotation=0, labelsize=10)\n",
    "\n",
    "    for i, v in enumerate(cluster_counts.values):\n",
    "        current_ax.text(i, v + (cluster_counts.values.max() * 0.015), str(v), color='black', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    current_ax.set_ylim(top=cluster_counts.values.max() * 1.1)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_noise_vs_clustered_summary(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_column: str,\n",
    "    domain_name: Optional[str] = None,\n",
    "    ax: Optional[plt.Axes] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a bar plot showing the count of noise records vs. actual clustered records\n",
    "    for a specified domain.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        cluster_column (str): The name of the column containing cluster labels.\n",
    "        domain_name (Optional[str]): The name of the domain to filter by.\n",
    "                                     If None, data is not filtered by domain.\n",
    "                                     The domain_name column in df must be boolean.\n",
    "        ax (Optional[plt.Axes]): A matplotlib axes object to plot on.\n",
    "                                 If None, a new figure and axes will be created.\n",
    "    \"\"\"\n",
    "    if cluster_column not in df.columns:\n",
    "        print(f\"Error: Cluster column '{cluster_column}' not found in DataFrame.\")\n",
    "        return\n",
    "\n",
    "    df_to_plot = df.copy()\n",
    "    plot_title = \"Noise vs. Clustered Records\"\n",
    "    original_total_records = len(df_to_plot)\n",
    "\n",
    "    if domain_name:\n",
    "        if domain_name not in df.columns:\n",
    "            print(f\"Warning: Domain column '{domain_name}' not found in DataFrame. Plotting for all data in df.\")\n",
    "        elif not pd.api.types.is_bool_dtype(df_to_plot[domain_name]):\n",
    "            print(f\"Warning: Domain column '{domain_name}' is not boolean. Plotting for all data in df.\")\n",
    "        else:\n",
    "            df_to_plot = df_to_plot[df_to_plot[domain_name] == True]\n",
    "            plot_title = f\"Noise vs. Clustered Records in {domain_name}\"\n",
    "            original_total_records = len(df_to_plot) # Update total for the specific domain\n",
    "            \n",
    "    if df_to_plot.empty:\n",
    "        if domain_name:\n",
    "            print(f\"No data to plot for domain '{domain_name}'.\")\n",
    "        else:\n",
    "            print(\"No data to plot (DataFrame is empty).\")\n",
    "        return\n",
    "\n",
    "    noise_count = (df_to_plot[cluster_column] == -1).sum()\n",
    "    clustered_count = (df_to_plot[cluster_column] != -1).sum()\n",
    "    \n",
    "    total_processed_records = noise_count + clustered_count\n",
    "\n",
    "    if total_processed_records == 0:\n",
    "        print(f\"No records (noise or clustered) found for the specified selection.\")\n",
    "        return\n",
    "\n",
    "    labels = ['Noise Records (-1)', 'Clustered Records (>=0)']\n",
    "    counts = [noise_count, clustered_count]\n",
    "    \n",
    "    # Calculate percentages\n",
    "    percentages = [(count / total_processed_records * 100) if total_processed_records > 0 else 0 for count in counts]\n",
    "\n",
    "    current_ax = ax\n",
    "    if current_ax is None:\n",
    "        fig, current_ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    bars = sns.barplot(x=labels, y=counts, ax=current_ax, palette=[\"#FF6347\", \"#4682B4\"]) # Tomato for noise, SteelBlue for clustered\n",
    "    current_ax.set_title(plot_title, fontsize=15)\n",
    "    current_ax.set_ylabel(\"Number of Records\", fontsize=12)\n",
    "\n",
    "    for i, bar in enumerate(bars.patches):\n",
    "        count_val = counts[i]\n",
    "        percentage_val = percentages[i]\n",
    "        current_ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + (max(counts) * 0.015), # Position text above bar\n",
    "            f\"{count_val}\\n({percentage_val:.1f}%)\", # Show count and percentage\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    current_ax.set_ylim(top=max(counts) * 1.15) # Adjust ylim to make space for text\n",
    "\n",
    "    print(f\"Analysis for {domain_name if domain_name else 'all data'}:\")\n",
    "    print(f\"  Total records processed: {total_processed_records}\")\n",
    "    print(f\"  Noise records (-1): {noise_count} ({percentages[0]:.1f}%)\")\n",
    "    print(f\"  Clustered records (>=0): {clustered_count} ({percentages[1]:.1f}%)\")\n",
    "\n",
    "\n",
    "    if ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_size_distribution(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_column: str,\n",
    "    domain_name: Optional[str] = None,\n",
    "    bin_step: int = 20,\n",
    "    max_bins: Optional[int] = 10,\n",
    "    ax: Optional[plt.Axes] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a bar plot showing the distribution of cluster sizes.\n",
    "    It counts how many clusters fall into predefined size bins.\n",
    "    Includes a text box with min, max, and median actual cluster sizes.\n",
    "    Bars are colored blue.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        cluster_column (str): The name of the column containing cluster labels.\n",
    "        domain_name (Optional[str]): The name of the domain to filter by.\n",
    "                                     If None, data is not filtered by domain.\n",
    "                                     The domain_name column in df must be boolean.\n",
    "        bin_step (int): The step size for creating bins (e.g., 20 means bins like 0-19, 20-39).\n",
    "        max_bins (Optional[int]): Approximate maximum number of bins to create.\n",
    "                                  The last bin will be an \"X+\" bin if sizes exceed this.\n",
    "                                  If None, bins will go up to the max cluster size.\n",
    "        ax (Optional[plt.Axes]): A matplotlib axes object to plot on.\n",
    "                                 If None, a new figure and axes will be created.\n",
    "    \"\"\"\n",
    "    if cluster_column not in df.columns:\n",
    "        print(f\"Error: Cluster column '{cluster_column}' not found in DataFrame.\")\n",
    "        return\n",
    "\n",
    "    df_to_analyze = df.copy()\n",
    "    plot_title = \"Distribution of Cluster Sizes\"\n",
    "\n",
    "    if domain_name:\n",
    "        if domain_name not in df.columns:\n",
    "            print(f\"Warning: Domain column '{domain_name}' not found in DataFrame. Analyzing all data in df.\")\n",
    "        elif not pd.api.types.is_bool_dtype(df_to_analyze[domain_name]):\n",
    "            print(f\"Warning: Domain column '{domain_name}' is not boolean. Analyzing for all data in df.\")\n",
    "        else:\n",
    "            df_to_analyze = df_to_analyze[df_to_analyze[domain_name] == True]\n",
    "            plot_title = f\"Distribution of Cluster Sizes in {domain_name}\"\n",
    "\n",
    "    if df_to_analyze.empty:\n",
    "        if domain_name:\n",
    "            print(f\"No data to analyze for domain '{domain_name}'.\")\n",
    "        else:\n",
    "            print(\"No data to analyze (DataFrame is empty).\")\n",
    "        return\n",
    "\n",
    "    # Get sizes of actual clusters (excluding noise -1)\n",
    "    cluster_sizes = df_to_analyze[df_to_analyze[cluster_column] != -1][cluster_column].value_counts()\n",
    "\n",
    "    if cluster_sizes.empty:\n",
    "        print(f\"No actual (non-noise) clusters found for '{domain_name if domain_name else 'all data'}'.\")\n",
    "        return\n",
    "\n",
    "    min_actual_cluster_size = cluster_sizes.min()\n",
    "    max_actual_cluster_size = cluster_sizes.max()\n",
    "    median_actual_cluster_size = cluster_sizes.median() # Calculate median\n",
    "\n",
    "    # Define bins\n",
    "    max_size_for_bins = cluster_sizes.max()\n",
    "    \n",
    "    bins = []\n",
    "    if max_bins is not None and (max_size_for_bins / bin_step) > (max_bins - 1):\n",
    "        upper_limit_regular_bins = (max_bins - 1) * bin_step\n",
    "        bins = list(np.arange(0, upper_limit_regular_bins + bin_step, bin_step))\n",
    "        \n",
    "        if bins[-1] <= max_size_for_bins:\n",
    "            bins.append(np.inf)\n",
    "        else: \n",
    "            bins = [b for b in bins if b < max_size_for_bins] \n",
    "            if not bins or bins[-1] < max_size_for_bins : \n",
    "                 bins.append(max_size_for_bins +1) \n",
    "            bins.append(np.inf) \n",
    "    else:\n",
    "        bins = list(np.arange(0, max_size_for_bins + bin_step, bin_step))\n",
    "\n",
    "    final_bins = [0]\n",
    "    for b_val in bins:\n",
    "        if b_val > final_bins[-1]: \n",
    "            final_bins.append(b_val)\n",
    "    \n",
    "    if final_bins[-1] != np.inf:\n",
    "        if max_bins is not None and len(final_bins) >= max_bins: \n",
    "            final_bins[-1] = np.inf\n",
    "        elif final_bins[-1] <= max_size_for_bins : \n",
    "            final_bins.append(np.inf)\n",
    "\n",
    "    bins = sorted(list(set(final_bins)))\n",
    "    if not bins: bins = [0, np.inf] \n",
    "    if bins[0] != 0 and 0 not in bins: bins.insert(0,0)\n",
    "    bins = sorted(list(set(bins))) \n",
    "\n",
    "    if max_size_for_bins < bin_step and len(bins) > 2 and bins[1] != np.inf :\n",
    "        bins = [0, max_size_for_bins + 1, np.inf] \n",
    "        bins = sorted(list(set(bins)))\n",
    "\n",
    "\n",
    "    bin_labels = []\n",
    "    if len(bins) < 2: \n",
    "        print(\"Warning: Not enough bins to create labels for cluster size distribution.\")\n",
    "        if cluster_sizes.shape[0] > 0:\n",
    "             print(f\"All {cluster_sizes.shape[0]} clusters have sizes: {cluster_sizes.values}\")\n",
    "        return\n",
    "\n",
    "    for i in range(len(bins) - 1):\n",
    "        lower_bound = int(bins[i])\n",
    "        upper_bound = bins[i+1]\n",
    "\n",
    "        if lower_bound == 0: \n",
    "            actual_lower_bound_display = 1 \n",
    "        else:\n",
    "            actual_lower_bound_display = lower_bound\n",
    "        \n",
    "        if upper_bound == np.inf:\n",
    "            bin_labels.append(f\"{actual_lower_bound_display}+ records\")\n",
    "        else:\n",
    "            bin_labels.append(f\"{actual_lower_bound_display}-{int(upper_bound-1)} records\")\n",
    "    \n",
    "    binned_cluster_sizes = pd.cut(cluster_sizes, bins=bins, labels=bin_labels, right=False, include_lowest=True)\n",
    "    \n",
    "    distribution = binned_cluster_sizes.value_counts().sort_index()\n",
    "\n",
    "    if distribution.empty:\n",
    "        print(\"Could not generate cluster size distribution (no data fell into bins).\")\n",
    "        print(f\"Cluster sizes (min: {min_actual_cluster_size}, median: {median_actual_cluster_size}, max: {max_actual_cluster_size}): {cluster_sizes.values if not cluster_sizes.empty else 'empty'}\")\n",
    "        print(f\"Bins used: {bins}\")\n",
    "        print(f\"Bin labels: {bin_labels}\")\n",
    "        return\n",
    "\n",
    "    current_ax = ax\n",
    "    if current_ax is None:\n",
    "        fig_width = max(10, len(distribution) * 0.9) \n",
    "        fig, current_ax = plt.subplots(figsize=(fig_width, 7)) \n",
    "\n",
    "    sns.barplot(x=distribution.index, y=distribution.values, ax=current_ax, color=\"steelblue\")\n",
    "    \n",
    "    current_ax.set_title(plot_title, fontsize=15)\n",
    "    current_ax.set_xlabel(\"Cluster Size (Number of Records in Cluster)\", fontsize=12)\n",
    "    current_ax.set_ylabel(\"Number of Clusters\", fontsize=12)\n",
    "    \n",
    "    current_ax.tick_params(axis='x', rotation=45, labelsize=10) \n",
    "\n",
    "    for i, v in enumerate(distribution.values):\n",
    "        current_ax.text(i, v + (distribution.values.max() * 0.015), str(v), color='black', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    current_ax.set_ylim(top=distribution.values.max() * 1.15) \n",
    "\n",
    "    # Add text box for min/median/max cluster size\n",
    "    textstr = (f'Min Cluster Size: {min_actual_cluster_size}\\n'\n",
    "               f'Median Cluster Size: {median_actual_cluster_size:.0f}\\n' # Format median as integer\n",
    "               f'Max Cluster Size: {max_actual_cluster_size}')\n",
    "    props = dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.7)\n",
    "    current_ax.text(0.97, 0.97, textstr, transform=current_ax.transAxes, fontsize=10,\n",
    "                    verticalalignment='top', horizontalalignment='right', bbox=props)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf1242",
   "metadata": {},
   "source": [
    "#### Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_publications_per_domain(\n",
    "    df: pd.DataFrame,\n",
    "    domain_column_names: List[str],\n",
    "    date_column: str = 'first_date',\n",
    "    figsize_per_subplot: Tuple[int, int] = (14, 4)\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a series of bar plots, one for each domain, showing the number of\n",
    "    papers published each month.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing publication data.\n",
    "        domain_column_names (List[str]): A list of column names, where each column\n",
    "                                         represents a domain and contains boolean values\n",
    "                                         (True if the paper belongs to the domain).\n",
    "        date_column (str): The name of the column containing publication dates.\n",
    "        figsize_per_subplot (Tuple[int, int]): Tuple specifying (width, height) for each subplot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the date column is in datetime format for proper processing\n",
    "    # Operate on a copy to avoid modifying the original DataFrame passed to the function\n",
    "    df_processed = df.copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_processed[date_column]):\n",
    "        try:\n",
    "            df_processed[date_column] = pd.to_datetime(df_processed[date_column])\n",
    "            print(f\"  Info: Converted '{date_column}' to datetime objects for plotting.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: Could not convert '{date_column}' to datetime: {e}. Aborting plot generation.\")\n",
    "            return\n",
    "\n",
    "    # Create a 'YearMonth' column for grouping\n",
    "    df_processed['YearMonthPlotting'] = df_processed[date_column].dt.to_period('M')\n",
    "\n",
    "    num_domains = len(domain_column_names)\n",
    "    if num_domains == 0:\n",
    "        print(\"  No domain columns provided. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    ncols = 2\n",
    "    nrows = (num_domains + ncols - 1) // ncols\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols,\n",
    "                             figsize=(figsize_per_subplot[0] * ncols, figsize_per_subplot[1] * nrows),\n",
    "                             squeeze=False) # squeeze=False ensures axes is always a 2D array\n",
    "    axes = axes.ravel() # Flatten to 1D array for easy iteration\n",
    "\n",
    "    plot_count = 0\n",
    "    for i, domain_col in enumerate(domain_column_names):\n",
    "        ax = axes[i] # Get the current axis\n",
    "\n",
    "        # Filter data for the current domain\n",
    "        df_domain = df_processed[df_processed[domain_col] == True]\n",
    "\n",
    "        monthly_counts = df_domain.groupby('YearMonthPlotting').size()\n",
    "\n",
    "        # Convert PeriodIndex to Timestamps for better plotting control with matplotlib\n",
    "        plot_index = monthly_counts.index.to_timestamp()\n",
    "\n",
    "        ax.bar(plot_index, monthly_counts.values, width=20) # width in days for monthly data\n",
    "        ax.set_title(f'Monthly Publications: {domain_col}', fontsize=14)\n",
    "        ax.set_ylabel('Number of Papers', fontsize=10)\n",
    "        ax.set_xlabel('Month', fontsize=10)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Improve x-axis tick labels for readability\n",
    "        num_months_on_axis = len(plot_index)\n",
    "        if num_months_on_axis > 12: # If more than 1 year of data\n",
    "            # Show major ticks for years, minor for months, format as YYYY-MM\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "            ax.xaxis.set_minor_locator(mdates.MonthLocator((1,4,7,10))) # Every 3 months for minor if too dense\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        else: # For shorter periods, default formatting might be okay or just rotate\n",
    "            plt.setp(ax.get_xticklabels(), rotation=30, ha='right')\n",
    "\n",
    "\n",
    "        plot_count += 1\n",
    "\n",
    "    # Turn off any unused subplots if the number of domains is not a perfect fit for the grid\n",
    "    for j in range(plot_count, nrows * ncols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout, rect leaves space for suptitle and bottom\n",
    "    fig.suptitle('Monthly Publication Counts per Domain', fontsize=18, y=0.99)\n",
    "    plt.show()\n",
    "\n",
    "domain_cols_for_plot = [\n",
    "    \"Physics\", \"Computer Science\", \"Statistics\", \"Mathematics\",\n",
    "    \"Electrical Engineering and Systems Science\", \"Quantitative Biology\",\n",
    "    \"Economics\", \"Quantitative Finance\"\n",
    "]\n",
    "\n",
    "plot_monthly_publications_per_domain(df, domain_cols_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffa139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_publications_per_domain(\n",
    "    df: pd.DataFrame,\n",
    "    domain_column_names: List[str],\n",
    "    figsize: Tuple[int, int] = (12, 7)\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a single bar plot showing the total number of publications for each domain,\n",
    "    with x-axis tick labels wrapped if they are too long.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing publication data.\n",
    "        domain_column_names (List[str]): A list of column names, where each column\n",
    "                                         represents a domain and contains boolean values\n",
    "                                         (True if the paper belongs to the domain).\n",
    "        figsize (Tuple[int, int]): Tuple specifying (width, height) for the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    if not domain_column_names:\n",
    "        print(\"  No domain columns provided. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    domain_counts = {}\n",
    "    for domain_col in domain_column_names:\n",
    "        if domain_col not in df.columns:\n",
    "            print(f\"  Warning: Domain column '{domain_col}' not found in DataFrame. Skipping.\")\n",
    "            continue\n",
    "        # Assuming boolean True means the paper belongs to the domain\n",
    "        domain_counts[domain_col] = df[df[domain_col] == True].shape[0]\n",
    "\n",
    "    if not domain_counts:\n",
    "        print(\"  No valid domain columns found or no data for specified domains. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    sorted_domain_counts = dict(sorted(domain_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    domains = list(sorted_domain_counts.keys())\n",
    "    counts = list(sorted_domain_counts.values())\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    bars = plt.bar(domains, counts, color='#4169e1')\n",
    "\n",
    "    # Add counts on top of bars\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01 * max(counts),\n",
    "                 int(yval), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "\n",
    "    plt.title('Total Publications per Domain', fontsize=16)\n",
    "    plt.xlabel('', fontsize=12)\n",
    "    plt.ylabel('Total Number of Publications', fontsize=12)\n",
    "\n",
    "    # Wrap x-axis tick labels\n",
    "    # You can adjust the width parameter as needed\n",
    "    wrapped_labels = [textwrap.fill(label, width=15) for label in domains]\n",
    "    plt.xticks(ticks=range(len(domains)), labels=wrapped_labels, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_total_publications_per_domain(df, domain_cols_for_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd26d5",
   "metadata": {},
   "source": [
    "##### Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2eced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_to_process = 'Physics'\n",
    "\n",
    "prep_result = prepare_domain_data(\n",
    "    domain_to_process, df, metadata_features, keyword_cols_created, domain_top_keywords_map\n",
    ")\n",
    "\n",
    "df_domain, X_domain_dense, relevant_keywords = prep_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for PCA pre-processing\n",
    "PCA_ACTIVATION_THRESHOLD = 150  # Apply PCA if number of features exceeds this\n",
    "PCA_TARGET_COMPONENTS = 100     # Target number of components after PCA\n",
    "\n",
    "\n",
    "# Ensure X_domain_dense is valid before proceeding\n",
    "if X_domain_dense is not None and X_domain_dense.shape[0] > 0:\n",
    "    print(f\"\\n--- PCA Pre-processing for domain: {domain_to_process} ---\")\n",
    "    print(f\"Original X_domain_dense shape: {X_domain_dense.shape}\")\n",
    "\n",
    "\n",
    "    data_for_umap = X_domain_dense  # Default to original data\n",
    "\n",
    "\n",
    "    if X_domain_dense.shape[1] > PCA_ACTIVATION_THRESHOLD:\n",
    "        print(f\"Number of features ({X_domain_dense.shape[1]}) > {PCA_ACTIVATION_THRESHOLD}. Applying PCA.\")\n",
    "\n",
    "\n",
    "        # 1. Scale data before PCA\n",
    "        pca_input_scaler = StandardScaler()\n",
    "        X_domain_scaled_for_pca = pca_input_scaler.fit_transform(X_domain_dense)\n",
    "        print(\"  Data scaled for PCA.\")\n",
    "\n",
    "\n",
    "        # 2. Determine the number of PCA components\n",
    "        # n_components must be <= min(n_samples, n_features) and > 0\n",
    "        # It should also be less than the original number of features to ensure reduction.\n",
    "        _n_samples = X_domain_scaled_for_pca.shape[0]\n",
    "        _n_features = X_domain_scaled_for_pca.shape[1]\n",
    "\n",
    "\n",
    "        # Calculate a valid number of components for PCA\n",
    "        # Target PCA_TARGET_COMPONENTS, but ensure it's less than current features and samples.\n",
    "        if _n_features <= 1 or _n_samples <=1 : # PCA not meaningful or possible\n",
    "            n_pca_components = 0\n",
    "        else:\n",
    "            # Ensure reduction: n_components should be < _n_features\n",
    "            # Ensure validity: n_components should be <= _n_samples\n",
    "            # Ensure target: n_components aims for PCA_TARGET_COMPONENTS\n",
    "            n_pca_components = min(PCA_TARGET_COMPONENTS, _n_samples, _n_features -1)\n",
    "\n",
    "\n",
    "        if n_pca_components > 0:\n",
    "            print(f\"  Applying PCA with n_components = {n_pca_components}\")\n",
    "            # Assuming RANDOM_STATE is defined in your notebook (e.g., RANDOM_STATE = 42)\n",
    "            pca_reducer = PCA(n_components=n_pca_components, random_state=RANDOM_STATE)\n",
    "            X_domain_intermediate_pca = pca_reducer.fit_transform(X_domain_scaled_for_pca)\n",
    "            \n",
    "            print(f\"  Shape after PCA: {X_domain_intermediate_pca.shape}\")\n",
    "            if hasattr(pca_reducer, 'explained_variance_ratio_'):\n",
    "                print(f\"  Total explained variance by {pca_reducer.n_components_} PCA components: {np.sum(pca_reducer.explained_variance_ratio_):.4f}\")\n",
    "            data_for_umap = X_domain_intermediate_pca  # Use PCA output for UMAP\n",
    "        else:\n",
    "            print(f\"  PCA not applied: Calculated n_pca_components ({n_pca_components}) is not suitable for reduction. Using original features.\")\n",
    "            # data_for_umap remains X_domain_dense (the original features)\n",
    "    else:\n",
    "        print(f\"Number of features ({X_domain_dense.shape[1]}) is not greater than {PCA_ACTIVATION_THRESHOLD}. PCA not applied.\")\n",
    "        # data_for_umap remains X_domain_dense\n",
    "    \n",
    "    print(f\"--- End of PCA Pre-processing. Data shape for UMAP: {data_for_umap.shape if data_for_umap is not None else 'None'} ---\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"X_domain_dense for domain '{domain_to_process}' is None or empty. Skipping PCA and UMAP.\")\n",
    "    data_for_umap = None # Ensure data_for_umap is defined for the next step, even if None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec806fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_for_umap is not None:\n",
    "    reduction_result = reduce_dimensionality(data_for_umap, umap_params={\n",
    "        'n_components': 10,\n",
    "        'n_neighbors': 50,\n",
    "        'min_dist': 0.0,\n",
    "        'metric': 'euclidean',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': True,\n",
    "        'low_memory': True\n",
    "    })\n",
    "else:\n",
    "    reduction_result = None\n",
    "\n",
    "\n",
    "# Check reduction_result before unpacking\n",
    "if reduction_result:\n",
    "    X_domain_reduced_umap, scaler, reducer = reduction_result\n",
    "else:\n",
    "    print(f\"Dimensionality reduction (UMAP) failed for domain {domain_to_process} or was skipped.\")\n",
    "    # Initialize to prevent errors if these variables are expected later,\n",
    "    # though subsequent clustering steps should also check if X_domain_reduced_umap is valid.\n",
    "    X_domain_reduced_umap = None\n",
    "    scaler = None\n",
    "    reducer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68859689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "if X_domain_reduced_umap is not None:\n",
    "    try:\n",
    "        joblib.dump(X_domain_reduced_umap, 'X_domain_reduced_umap_physics.joblib')\n",
    "        print(\"X_domain_reduced_umap_physics.joblib saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving X_domain_reduced_umap_physics.joblib: {e}\")\n",
    "else:\n",
    "    print(\"X_domain_reduced_umap is None, skipping save.\")\n",
    "\n",
    "\n",
    "if scaler is not None:\n",
    "    try:\n",
    "        joblib.dump(scaler, 'scaler_physics.joblib')\n",
    "        print(\"scaler_physics.joblib saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving scaler_physics.joblib: {e}\")\n",
    "else:\n",
    "    print(\"scaler is None, skipping save.\")\n",
    "\n",
    "\n",
    "if reducer is not None:\n",
    "    try:\n",
    "        joblib.dump(reducer, 'reducer_physics.joblib')\n",
    "        print(\"reducer_physics.joblib saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving reducer_physics.joblib: {e}\")\n",
    "else:\n",
    "    print(\"reducer is None, skipping save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbdc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Attempt to reload the UMAP reduced data\n",
    "try:\n",
    "    X_domain_reduced_umap = joblib.load('X_domain_reduced_umap_physics.joblib')\n",
    "    print(\"X_domain_reduced_umap_physics.joblib reloaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: X_domain_reduced_umap_physics.joblib not found.\")\n",
    "    X_domain_reduced_umap = None\n",
    "except Exception as e:\n",
    "    print(f\"Error reloading X_domain_reduced_umap_physics.joblib: {e}\")\n",
    "    X_domain_reduced_umap = None\n",
    "\n",
    "\n",
    "# Attempt to reload the scaler\n",
    "try:\n",
    "    scaler = joblib.load('scaler_physics.joblib')\n",
    "    print(\"scaler_physics.joblib reloaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: scaler_physics.joblib not found.\")\n",
    "    scaler = None\n",
    "except Exception as e:\n",
    "    print(f\"Error reloading scaler_physics.joblib: {e}\")\n",
    "    scaler = None\n",
    "\n",
    "\n",
    "# Attempt to reload the reducer\n",
    "try:\n",
    "    reducer = joblib.load('reducer_physics.joblib')\n",
    "    print(\"reducer_physics.joblib reloaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: reducer_physics.joblib not found.\")\n",
    "    reducer = None\n",
    "except Exception as e:\n",
    "    print(f\"Error reloading reducer_physics.joblib: {e}\")\n",
    "    reducer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size = 100000\n",
    "# X_domain_dense_sampled = X_domain_dense[np.random.choice(X_domain_dense.shape[0], sample_size, replace=False)]\n",
    "\n",
    "# reduction_result = reduce_dimensionality(X_domain_dense_sampled, umap_params={\n",
    "#     'n_components': 10,\n",
    "#     'n_neighbors': 50,\n",
    "#     'min_dist': 0.0,\n",
    "#     'metric': 'euclidean',\n",
    "#     'random_state': RANDOM_STATE,\n",
    "#     'verbose': True,\n",
    "#     'low_memory': True\n",
    "# })\n",
    "\n",
    "reduction_result = reduce_dimensionality(X_domain_dense, umap_params={\n",
    "    'n_components': 10,\n",
    "    'n_neighbors': 50,\n",
    "    'min_dist': 0.0,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': True,\n",
    "    'low_memory': True\n",
    "})\n",
    "\n",
    "X_domain_reduced_umap, scaler, reducer = reduction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HDBSCAN Clustering  ---\n",
    "# Adjust min_cluster_size and min_samples based on your domain's data size and density\n",
    "# Smaller min_cluster_size allows for smaller, more granular clusters.\n",
    "# Larger min_samples makes clustering more conservative (more points become noise).\n",
    "hdbscan_params = {\n",
    "    \"min_cluster_size\": 20,  # Example: minimum number of papers to form a cluster\n",
    "    \"min_samples\": 10,        # Example: lower values make it easier to form clusters\n",
    "    \"metric\": 'euclidean',    # Metric used for UMAP output\n",
    "    \"cluster_selection_method\": 'eom' # 'eom' or 'leaf'\n",
    "}\n",
    "\n",
    "clustering_result_hdbscan = apply_hdbscan_clustering(\n",
    "    X_domain_reduced_umap,\n",
    "    df, # Main DataFrame\n",
    "    df_domain.index, # Index for the current domain's data in the main df\n",
    "    domain_to_process,\n",
    "    **hdbscan_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is modified in-place by apply_hdbscan_clustering\n",
    "_, hdbscan_model, n_clusters_found_hdbscan = clustering_result_hdbscan\n",
    "\n",
    "chosen_c_for_analysis = n_clusters_found_hdbscan\n",
    "print(f\"HDBSCAN identified {chosen_c_for_analysis} clusters for analysis (excluding noise).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze Sub-Clusters (mostly the same function call) ---\n",
    "# The analyze_sub_clusters function should handle the -1 label for noise if it appears in plots.\n",
    "# It iterates over unique cluster labels found.\n",
    "analysis_output = analyze_sub_clusters(\n",
    "    df,  # df already updated with HDBSCAN labels\n",
    "    domain_to_process,\n",
    "    chosen_c_for_analysis, # Number of actual clusters (for print statements, etc.)\n",
    "    relevant_keywords, # Features used for UMAP\n",
    "    metadata_features,\n",
    "    rolling_window_param=ROLLING_WINDOW\n",
    ")\n",
    "\n",
    "smoothed_proportions_df = None\n",
    "top_keywords_data_for_domain = {}\n",
    "\n",
    "smoothed_proportions_df, top_keywords_data_for_domain = analysis_output\n",
    "\n",
    "# --- Store Results ---\n",
    "domain_cluster_results = {}\n",
    "\n",
    "domain_cluster_results[domain_to_process] = {\n",
    "    'k_found': chosen_c_for_analysis,\n",
    "    'hdbscan_model': hdbscan_model,\n",
    "    'umap_reducer': reducer,\n",
    "    'scaler': scaler,\n",
    "    'features_used': relevant_keywords,\n",
    "    'hdbscan_params': hdbscan_params\n",
    "}\n",
    "\n",
    "domain_proportions_data = {}\n",
    "domain_top_keywords_info = {}\n",
    "\n",
    "domain_proportions_data[domain_to_process] = smoothed_proportions_df\n",
    "domain_top_keywords_info[domain_to_process] = top_keywords_data_for_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noise_vs_clustered_summary(df=df,\n",
    "                                cluster_column='sub_cluster',\n",
    "                                domain_name=domain_to_process)\n",
    "\n",
    "plot_cluster_record_counts(df=df,\n",
    "                           cluster_column='sub_cluster',\n",
    "                           domain_name=domain_to_process,\n",
    "                           top_n=50\n",
    "                           )\n",
    "\n",
    "plot_cluster_size_distribution(df=df,\n",
    "                               cluster_column='sub_cluster',\n",
    "                               domain_name=domain_to_process,\n",
    "                               bin_step=10,      \n",
    "                               max_bins=10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9634fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "emerging_ids_qb = identify_emerging_cluster_ids(\n",
    "    df,\n",
    "    domain_name=domain_to_process,\n",
    "    cluster_column='sub_cluster',\n",
    "    date_column='first_date',\n",
    "    recent_months_window=4,\n",
    "    min_papers_recent_period=2,        # Example: cluster needs at least 5 papers recently\n",
    "    emerging_ratio_threshold=1,      # Example: recent proportion must be 1.5x baseline\n",
    "    emerging_diff_threshold=0.01,      # Example: recent proportion must be at least 1% higher\n",
    "    newly_active_min_recent_prop=0.0001 # Example: new cluster must be at least 0.5% of recent papers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c09826",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_to_combine = list(range(chosen_c_for_analysis))\n",
    "colors_for_combine = COLORS_2.copy()\n",
    "\n",
    "clusters_to_preserve_color = emerging_ids_qb\n",
    "\n",
    "for cluster_id in colors_for_combine:\n",
    "    if cluster_id not in clusters_to_preserve_color:\n",
    "        colors_for_combine[cluster_id] = EXCLUSION_COLOR \n",
    "\n",
    "proportions_df_to_plot = domain_proportions_data[domain_to_process]\n",
    "top_keywords_for_plot = domain_top_keywords_info[domain_to_process]\n",
    "\n",
    "plot_combined_trends(\n",
    "    proportions_df_to_plot,\n",
    "    clusters_to_combine,\n",
    "    colors_for_combine,\n",
    "    top_keywords_for_plot,\n",
    "    domain_to_process,\n",
    "    rolling_window=ROLLING_WINDOW,\n",
    "    keyword_exclusion_color=EXCLUSION_COLOR,\n",
    "    y_axis_scale = 'linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 218\n",
    "\n",
    "keywords_for_this_cluster = domain_top_keywords_info[domain_to_process][cluster]\n",
    "\n",
    "plot_cluster_keyword_trends_plotly(\n",
    "    df=df,\n",
    "    domain_name=domain_to_process,\n",
    "    cluster_id=cluster,\n",
    "    all_cluster_keywords=keywords_for_this_cluster,\n",
    "    num_keywords_to_plot=10,\n",
    "    custom_colors=COLORS,\n",
    "    rolling_window=12 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd2113",
   "metadata": {},
   "source": [
    "##### Computer Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b7dfc",
   "metadata": {},
   "source": [
    "##### Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7063c9",
   "metadata": {},
   "source": [
    "##### Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cbc7f",
   "metadata": {},
   "source": [
    "##### Electrical Engineering and Systems Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b89e28",
   "metadata": {},
   "source": [
    "##### Quantitative Biology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856aff5",
   "metadata": {},
   "source": [
    "###### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_to_process = 'Quantitative Biology'\n",
    "\n",
    "prep_result = prepare_domain_data(domain_to_process, df, metadata_features, keyword_cols_created, domain_top_keywords_map)\n",
    " \n",
    "df_domain, X_domain_dense, relevant_keywords = prep_result\n",
    "\n",
    "reduction_result = reduce_dimensionality(X_domain_dense, umap_params={\n",
    "    'n_components': 50,\n",
    "    'n_neighbors': 20,\n",
    "    'min_dist': 0.1,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': 42,\n",
    "    'verbose': True\n",
    "})\n",
    "\n",
    "X_domain_reduced_umap, scaler, reducer = reduction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_k = evaluate_k(X_domain_reduced_umap, domain_to_process,\n",
    "                         k_range=DEFAULT_K_RANGE, sample_size=DEFAULT_SILHOUETTE_SAMPLE_SIZE,\n",
    "                         kmeans_params=DEFAULT_KMEANS_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_k = 5\n",
    "\n",
    "clustering_result = apply_clustering(X_domain_reduced_umap, chosen_k, df, df_domain.index,\n",
    "                                    domain_to_process, kmeans_params=DEFAULT_KMEANS_PARAMS)\n",
    "\n",
    "if clustering_result:\n",
    "    _, kmeans_model = clustering_result\n",
    "\n",
    "    analysis_output = analyze_sub_clusters(\n",
    "        df, domain_to_process, chosen_k, relevant_keywords, metadata_features\n",
    "    )\n",
    "\n",
    "    smoothed_proportions_df = None\n",
    "    top_keywords_data_for_domain = {} # Initialize as empty dict\n",
    "\n",
    "    if analysis_output is not None:\n",
    "        smoothed_proportions_df, top_keywords_data_for_domain = analysis_output\n",
    "\n",
    "    domain_cluster_results = {}\n",
    "\n",
    "    domain_cluster_results[domain_to_process] = {\n",
    "    'k': chosen_k,\n",
    "    'kmeans_model': kmeans_model,\n",
    "    'umap_reducer': reducer,\n",
    "    'scaler': scaler,\n",
    "    'features_used': metadata_features + relevant_keywords\n",
    "    }\n",
    "\n",
    "    # --- Store Proportions Data and Keywords Data Globally ---\n",
    "    if 'domain_proportions_data' not in locals():\n",
    "         domain_proportions_data = {}\n",
    "    if 'domain_top_keywords_info' not in locals(): # New dictionary for keywords\n",
    "         domain_top_keywords_info = {}\n",
    "\n",
    "    if smoothed_proportions_df is not None:\n",
    "        domain_proportions_data[domain_to_process] = smoothed_proportions_df\n",
    "        print(f\"\\nSmoothed proportions data for '{domain_to_process}' stored.\")\n",
    "    else:\n",
    "        domain_proportions_data[domain_to_process] = None\n",
    "        print(f\"\\nNo proportions data stored for '{domain_to_process}'.\")\n",
    "\n",
    "    if top_keywords_data_for_domain: # Check if it's not empty\n",
    "        domain_top_keywords_info[domain_to_process] = top_keywords_data_for_domain\n",
    "        print(f\"Top keywords data for '{domain_to_process}' stored.\")\n",
    "    else:\n",
    "        domain_top_keywords_info[domain_to_process] = {} # Store empty dict if no keywords\n",
    "        print(f\"No top keywords data stored for '{domain_to_process}'.\")\n",
    "\n",
    "    print(f\"\\n--- Finished analysis block for domain: {domain_to_process} ---\")\n",
    "else:\n",
    "    print(f\"Clustering failed for domain {domain_to_process}, skipping analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500cd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_to_combine = list(range(chosen_k))\n",
    "colors_for_combine = COLORS.copy()\n",
    "\n",
    "clusters_to_preserve_color = [3, 4]\n",
    "\n",
    "for cluster_id in colors_for_combine:\n",
    "    if cluster_id not in clusters_to_preserve_color:\n",
    "        colors_for_combine[cluster_id] = EXCLUSION_COLOR \n",
    "\n",
    "proportions_df_to_plot = domain_proportions_data[domain_to_process]\n",
    "top_keywords_for_plot = domain_top_keywords_info[domain_to_process]\n",
    "\n",
    "plot_combined_trends(\n",
    "    proportions_df_to_plot,\n",
    "    clusters_to_combine,\n",
    "    colors_for_combine,\n",
    "    top_keywords_for_plot,\n",
    "    domain_to_process,\n",
    "    rolling_window=ROLLING_WINDOW,\n",
    "    keyword_exclusion_color=EXCLUSION_COLOR,\n",
    "    y_axis_scale = 'linear'\n",
    ")\n",
    "\n",
    "# plot_combined_trends(\n",
    "#     proportions_df_to_plot,\n",
    "#     clusters_to_combine,\n",
    "#     colors_for_combine,\n",
    "#     top_keywords_for_plot,\n",
    "#     domain_to_process,\n",
    "#     rolling_window=ROLLING_WINDOW,\n",
    "#     keyword_exclusion_color=EXCLUSION_COLOR,\n",
    "#     y_axis_scale = 'log'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 4\n",
    "\n",
    "keywords_for_this_cluster = domain_top_keywords_info[domain_to_process][cluster]\n",
    "\n",
    "plot_cluster_keyword_trends_plotly(\n",
    "    df=df,\n",
    "    domain_name=domain_to_process,\n",
    "    cluster_id=cluster,\n",
    "    all_cluster_keywords=keywords_for_this_cluster,\n",
    "    num_keywords_to_plot=10,\n",
    "    custom_colors=COLORS,\n",
    "    rolling_window=12 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3883b",
   "metadata": {},
   "source": [
    "###### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa700330",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_to_process = 'Quantitative Biology'\n",
    "\n",
    "prep_result = prepare_domain_data(\n",
    "    domain_to_process, df, metadata_features, keyword_cols_created, domain_top_keywords_map\n",
    ")\n",
    "\n",
    "df_domain, X_domain_dense, relevant_keywords = prep_result\n",
    "\n",
    "reduction_result = reduce_dimensionality(X_domain_dense, umap_params={\n",
    "    'n_components': 50,\n",
    "    'n_neighbors': 20,\n",
    "    'min_dist': 0.0,\n",
    "    'metric': 'euclidean',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': True\n",
    "})\n",
    "\n",
    "X_domain_reduced_umap, scaler, reducer = reduction_result\n",
    "\n",
    "# --- HDBSCAN Clustering  ---\n",
    "# Adjust min_cluster_size and min_samples based on your domain's data size and density\n",
    "# Smaller min_cluster_size allows for smaller, more granular clusters.\n",
    "# Larger min_samples makes clustering more conservative (more points become noise).\n",
    "hdbscan_params = {\n",
    "    \"min_cluster_size\": 20,  # Example: minimum number of papers to form a cluster\n",
    "    \"min_samples\": 10,        # Example: lower values make it easier to form clusters\n",
    "    \"metric\": 'euclidean',    # Metric used for UMAP output\n",
    "    \"cluster_selection_method\": 'eom' # 'eom' or 'leaf'\n",
    "}\n",
    "\n",
    "clustering_result_hdbscan = apply_hdbscan_clustering(\n",
    "    X_domain_reduced_umap,\n",
    "    df, # Main DataFrame\n",
    "    df_domain.index, # Index for the current domain's data in the main df\n",
    "    domain_to_process,\n",
    "    **hdbscan_params\n",
    ")\n",
    "\n",
    "# df is modified in-place by apply_hdbscan_clustering\n",
    "_, hdbscan_model, n_clusters_found_hdbscan = clustering_result_hdbscan\n",
    "\n",
    "chosen_c_for_analysis = n_clusters_found_hdbscan\n",
    "print(f\"HDBSCAN identified {chosen_c_for_analysis} clusters for analysis (excluding noise).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze Sub-Clusters (mostly the same function call) ---\n",
    "# The analyze_sub_clusters function should handle the -1 label for noise if it appears in plots.\n",
    "# It iterates over unique cluster labels found.\n",
    "analysis_output = analyze_sub_clusters(\n",
    "    df,  # df already updated with HDBSCAN labels\n",
    "    domain_to_process,\n",
    "    chosen_c_for_analysis, # Number of actual clusters (for print statements, etc.)\n",
    "    relevant_keywords, # Features used for UMAP\n",
    "    metadata_features,\n",
    "    rolling_window_param=ROLLING_WINDOW\n",
    ")\n",
    "\n",
    "smoothed_proportions_df = None\n",
    "top_keywords_data_for_domain = {}\n",
    "\n",
    "smoothed_proportions_df, top_keywords_data_for_domain = analysis_output\n",
    "\n",
    "# --- Store Results ---\n",
    "domain_cluster_results = {}\n",
    "\n",
    "domain_cluster_results[domain_to_process] = {\n",
    "    'k_found': chosen_c_for_analysis,\n",
    "    'hdbscan_model': hdbscan_model,\n",
    "    'umap_reducer': reducer,\n",
    "    'scaler': scaler,\n",
    "    'features_used': relevant_keywords,\n",
    "    'hdbscan_params': hdbscan_params\n",
    "}\n",
    "\n",
    "domain_proportions_data = {}\n",
    "domain_top_keywords_info = {}\n",
    "\n",
    "domain_proportions_data[domain_to_process] = smoothed_proportions_df\n",
    "domain_top_keywords_info[domain_to_process] = top_keywords_data_for_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noise_vs_clustered_summary(df=df,\n",
    "                                cluster_column='sub_cluster',\n",
    "                                domain_name=domain_to_process)\n",
    "\n",
    "plot_cluster_record_counts(df=df,\n",
    "                           cluster_column='sub_cluster',\n",
    "                           domain_name=domain_to_process,\n",
    "                           top_n=50\n",
    "                           )\n",
    "\n",
    "plot_cluster_size_distribution(df=df,\n",
    "                               cluster_column='sub_cluster',\n",
    "                               domain_name=domain_to_process,\n",
    "                               bin_step=10,      \n",
    "                               max_bins=10)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb54464",
   "metadata": {},
   "outputs": [],
   "source": [
    "emerging_ids_qb = identify_emerging_cluster_ids(\n",
    "    df,\n",
    "    domain_name=domain_to_process,\n",
    "    cluster_column='sub_cluster',\n",
    "    date_column='first_date',\n",
    "    recent_months_window=4,\n",
    "    min_papers_recent_period=2,        # Example: cluster needs at least 5 papers recently\n",
    "    emerging_ratio_threshold=1,      # Example: recent proportion must be 1.5x baseline\n",
    "    emerging_diff_threshold=0.01,      # Example: recent proportion must be at least 1% higher\n",
    "    newly_active_min_recent_prop=0.0001 # Example: new cluster must be at least 0.5% of recent papers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98112616",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_to_combine = list(range(chosen_c_for_analysis))\n",
    "colors_for_combine = COLORS_2.copy()\n",
    "\n",
    "clusters_to_preserve_color = emerging_ids_qb\n",
    "\n",
    "for cluster_id in colors_for_combine:\n",
    "    if cluster_id not in clusters_to_preserve_color:\n",
    "        colors_for_combine[cluster_id] = EXCLUSION_COLOR \n",
    "\n",
    "proportions_df_to_plot = domain_proportions_data[domain_to_process]\n",
    "top_keywords_for_plot = domain_top_keywords_info[domain_to_process]\n",
    "\n",
    "plot_combined_trends(\n",
    "    proportions_df_to_plot,\n",
    "    clusters_to_combine,\n",
    "    colors_for_combine,\n",
    "    top_keywords_for_plot,\n",
    "    domain_to_process,\n",
    "    rolling_window=ROLLING_WINDOW,\n",
    "    keyword_exclusion_color=EXCLUSION_COLOR,\n",
    "    y_axis_scale = 'linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 218\n",
    "\n",
    "keywords_for_this_cluster = domain_top_keywords_info[domain_to_process][cluster]\n",
    "\n",
    "plot_cluster_keyword_trends_plotly(\n",
    "    df=df,\n",
    "    domain_name=domain_to_process,\n",
    "    cluster_id=cluster,\n",
    "    all_cluster_keywords=keywords_for_this_cluster,\n",
    "    num_keywords_to_plot=10,\n",
    "    custom_colors=COLORS,\n",
    "    rolling_window=12 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f8f59",
   "metadata": {},
   "source": [
    "##### Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65686a51",
   "metadata": {},
   "source": [
    "##### Quantitative Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2910c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
